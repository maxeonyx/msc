{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Gregor Kobsik\n",
    "\n",
    "Date: 15.12.2020\n",
    "\n",
    "Description: This notebook shows a basic implementation of a transformer (decoder) architecture for image generation in TensorFlow 2.\n",
    "\n",
    "References:\n",
    "\n",
    "[Transformers Tutorial](https://www.tensorflow.org/tutorials/text/transformer) - This notebook is based on this tutorial.\n",
    "\n",
    "[Illustrated Transformers Guide](http://jalammar.github.io/illustrated-transformer/) - Quick visuall explanation.\n",
    "\n",
    "[ImageGPT in PyTorch](https://github.com/teddykoker/image-gpt) - Other reference implementation in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder for Images aka ImageGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import tensorflow as tf\n",
    "# https://stackoverflow.com/a/60699372/7989988\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd1NWMxjfsDd"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4_Qt8W1hJE_"
   },
   "source": [
    "### Download data\n",
    "\n",
    "This dataset contains 60000 training examples. [MNIST](https://www.tensorflow.org/datasets/catalog/mnist)-Reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8q9t4FmN96eN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:28:49.564870: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-29 17:28:50.113583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2021-10-29 17:28:50.183321: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-29 17:28:50.220848: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:28:50.460315: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET = 'mnist'\n",
    "image_width = 28\n",
    "image_height = 28\n",
    "\n",
    "examples, metadata = tfds.load(DATASET, with_info=True, as_supervised=True)\n",
    "\n",
    "train_examples = examples['train']\n",
    "val_examples = examples['test']\n",
    "\n",
    "fig = tfds.show_examples(train_examples.take(3), metadata)\n",
    "fig = tfds.show_examples(val_examples.take(3), metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute centroids\n",
    "Perform K-Means clustering to find `k` cluster centers of the given images/pixels. \n",
    "\n",
    "First map pixel values to a range `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834adb5a39b847e997e0b2517255f27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 8\n",
    "\n",
    "def normalize_image(image, label):\n",
    "    return tf.cast(image, dtype=tf.float16) / 255.0, label\n",
    "\n",
    "def find_centroids(ds_train, num_clusters=16, batch_size=1024):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, batch_size=batch_size, verbose=True)\n",
    "    ds_batched = ds_train.batch(batch_size)\n",
    "    for img, _ in tqdm(iter(ds_batched)):\n",
    "        pixels = img.numpy().reshape(-1, img.shape[-1])\n",
    "        kmeans.partial_fit(pixels)\n",
    "\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "centroids = find_centroids(train_examples.map(normalize_image), NUM_CLUSTERS, 1000)\n",
    "centroids = tf.convert_to_tensor(centroids, dtype=tf.float16)\n",
    "print(centroids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCEKotqosGfq"
   },
   "source": [
    "### Quantize images\n",
    "\n",
    "Quantize all samples to create a training dataset.\n",
    "\n",
    "- First, flatten each input batch.\n",
    "- Next, compute the euclidean distance to the centroid for each pixel.\n",
    "- Last, take the index with the smallest distance as the resulting label.\n",
    "\n",
    "The size of the tensor shape is given in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_euclidean_distance(a, b):\n",
    "    b = tf.transpose(b)        \n",
    "    a2 = tf.math.reduce_sum(tf.math.square(a), axis=1, keepdims=True)\n",
    "    b2 = tf.math.reduce_sum(tf.math.square(b), axis=0, keepdims=True)\n",
    "    ab = tf.linalg.matmul(a, b)\n",
    "    return a2 - 2 * ab + b2\n",
    "\n",
    "def quantize(image, label):\n",
    "    shape = tf.shape(image) # (height, width, color)\n",
    "    x = tf.reshape(image, (-1, shape[2])) # (height * width, color)\n",
    "    d = squared_euclidean_distance(x, centroids) # (height * width, centroids)\n",
    "    sequence = tf.math.argmin(d, axis=1)  # (height * width)\n",
    "    return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = (\n",
    "    train_examples\n",
    "    .map(normalize_image)\n",
    "    .map(quantize)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "val_dataset = (\n",
    "    val_examples\n",
    "    .map(normalize_image)\n",
    "    .map(quantize)\n",
    "    .cache()\n",
    "    .padded_batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a random data sequence sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
      "  5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 4 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 2 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 2\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 5 2 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 2 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 3 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 5 4 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 5 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 3 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 5 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 784), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:29:06.733072: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 1\n",
    "\n",
    "image_batch, _ = next(iter(train_dataset))\n",
    "print(image_batch[:NUM_SAMPLES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now show some quantitized and unquantized images. \n",
    "\n",
    "First, define a function to revert the quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unquantize(x):\n",
    "    x_one_hot = tf.cast(tf.one_hot(x, depth=len(centroids)), dtype=tf.float16)  # (seq, num_centroids)\n",
    "    return tf.linalg.matmul(x_one_hot,centroids)  # (seq, num_features)\n",
    "\n",
    "def np_showSeq(seq, size, max_images=3, cmap=None):\n",
    "    \"\"\" Show one or more images encoded as sequence. (numpy version)\n",
    "\n",
    "        seq: numpy array of sequences which encode the image. Either a single sequence or multiple sequences.\n",
    "        size: the image size. e.g. (28, 28) for `mnist` images.\n",
    "        max_images: the maximum number of images to display.\n",
    "    \"\"\" \n",
    "    batch = seq.shape[0]\n",
    "    num_show_img = min(max_images, seq.shape[0])\n",
    "    img = np.reshape(seq, (batch, *size, -1))\n",
    "    if img.shape[-1] == 1:\n",
    "      img = np.squeeze(img, axis=-1)    \n",
    "    \n",
    "    fig=plt.figure(figsize=(3*num_show_img, 3))\n",
    "    for i in range(num_show_img):\n",
    "        ax = fig.add_subplot(1, num_show_img, i+1)\n",
    "        ax.set_axis_off()\n",
    "        plt.imshow(img[i], cmap=cmap)\n",
    "    plt.show()\n",
    "\n",
    "def showSeq(seq, size, max_images=3, cmap=None):\n",
    "    \"\"\" Show one or more images encoded as sequence. (tensorflow version)\n",
    "\n",
    "        seq: tensor of sequences which encode the image. Either a single sequence or multiple sequences.\n",
    "        size: the image size. e.g. (28, 28) for `mnist` images.\n",
    "        max_images: the maximum number of images to display.\n",
    "    \"\"\"\n",
    "    if cmap:\n",
    "        seq = tf.map_fn(fn=unquantize, elems=seq, fn_output_signature=tf.float16)\n",
    "    seq = tf.cast(seq, float).numpy()\n",
    "\n",
    "    np_showSeq(seq, size, max_images, cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4DYWukNFkGQN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:29:06.865138: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unquantized\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_SAMPLES = 10\n",
    "\n",
    "image_batch, _ = next(iter(train_dataset))\n",
    "image_batch = image_batch[:NUM_SAMPLES]\n",
    "\n",
    "print(\"quantized:\")\n",
    "showSeq(image_batch, (image_width, image_height), NUM_SAMPLES)\n",
    "print(\"unquantized\")\n",
    "showSeq(image_batch, (image_width, image_height), NUM_SAMPLES, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "## Masking\n",
    "\n",
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "e.g. If we predict the third token, we use only the first and second token as reference. \n",
    "All future tokens will be masked in the seöf-attention by subtracting a very big number from the corresponding attention logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size_q, size_k):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size_q, size_k)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]], shape=(3, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(create_look_ahead_mask(3, 6))\n",
    "tf.linalg.band_part(tf.ones((5,5)), 0, -1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "We call the function Self-Attention if we use the same input for Q, K and V.\n",
    "\n",
    "The mask is applied by multiplying it with -1e9 (close to negative infinity) and subtracting it from the attention logits. The following softmax function will render the corresponding positions to zero or almost zero and thus ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Any value to create and use a lookahead mask. \n",
    "          Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    mask = create_look_ahead_mask(tf.shape(q)[-2], tf.shape(k)[-2])\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[1.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [8.433274e-26 1.000000e+00 0.000000e+00]\n",
      " [8.433274e-26 8.433274e-26 1.000000e+00]], shape=(3, 3), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[1.000000e+01 0.000000e+00 0.000000e+00]\n",
      " [8.433274e-25 1.000000e+01 0.000000e+00]\n",
      " [8.433274e-25 8.433274e-25 1.000000e+01]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_x = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (3, 3)\n",
    "temp_mask = create_look_ahead_mask(3, 3)\n",
    "\n",
    "temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      temp_x, temp_x, temp_x, temp_mask)\n",
    "print ('Attention weights are:')\n",
    "print (temp_attn)\n",
    "print ('Output is:')\n",
    "print (temp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer.\n",
    "\n",
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.\n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "TODO: Use directly [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 50, 64]), TensorShape([32, 8, 50, 50]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "y = tf.random.uniform((32, 50, 64))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "## Point wise feed forward network\n",
    "\n",
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "\n",
    "`TODO`: Use gelu activation function instead of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      # tf.keras.activations.gelu(approximate=False), # tf-nightly\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "Each `DecoderLayer` consists of sublayers:\n",
    "\n",
    "-   Multi-head attention (with masking) \n",
    "-   Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it preceeded by a layer normalization and dropout. Residual connections help in avoiding the vanishing gradient problem in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "    # t.shape == (batch_size, input_seq_len, d_model)\n",
    "    # for t in [x, out1, attn_out, res1, out2, ffn_out, res2]\n",
    "    \n",
    "    # masked multihead self-attention with residual connection\n",
    "    out1 = self.layernorm1(self.dropout1(x, training=training)) \n",
    "    attn_out, attn_weights = self.mha(out1, out1, out1, mask) \n",
    "    res1 = x + attn_out\n",
    "\n",
    "    # feed forward neural network with residual connection\n",
    "    out2 = self.layernorm2(self.dropout2(res1, training=training))\n",
    "    ffn_out = self.ffn(out2)\n",
    "    res2 = res1 + ffn_out\n",
    "    \n",
    "    return res2, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "\n",
    "The `Decoder` consists of:\n",
    "-   Input Embedding\n",
    "-   Positional Embedding\n",
    "-   N Decoder Layers\n",
    "\n",
    "The input is put through an token embedding which is summed with the positional embedding. \n",
    "The position of each token is simply encoded by their embedded position number, e.g. (0, 1, 2, 3, ...)\n",
    "\n",
    "The output of this summation is the input to the decoder layers. Afterwards the output is put through a dropout and normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.tok_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(maximum_position_encoding, d_model)                                            \n",
    "        \n",
    "    self.enc_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    batch = tf.shape(x)[0]\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}  \n",
    "\n",
    "    # token and positional embedding\n",
    "    positions = tf.expand_dims(tf.range(seq_len), axis=0)\n",
    "    x = self.tok_embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x = x + tf.tile(self.pos_embedding(positions), [batch, 1, 1])  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    # n * decoder layers\n",
    "    for i in range(self.num_layers):\n",
    "      x, attn_w = self.enc_layers[i](x, training, mask)\n",
    "      attention_weights['decoder_layer{}'.format(i+1)] = attn_w\n",
    "\n",
    "    # dropout and layer normalization\n",
    "    x = self.dropout(x, training=training)\n",
    "    x = self.layer_norm(x)\n",
    "              \n",
    "    return x, attention_weights  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 783, 16)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = DecoderBlock(num_layers=4, d_model=16, num_heads=2, \n",
    "                         dff=64, vocab_size=8,\n",
    "                         maximum_position_encoding=783)\n",
    "temp_input = tf.random.uniform((32, 783), dtype=tf.int64, minval=0, maxval=8)\n",
    "\n",
    "sample_encoder_output, _ = sample_encoder(temp_input, training=False, mask=True)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uERO1y54cOKq"
   },
   "source": [
    "## Create the Image Transformer \n",
    "\n",
    "The Image Transformer consists of the encoder and a final linear layer. The output of the decoder is the input to the linear layer.\n",
    "\n",
    "Return the output of the final layer, as well as the attention weights of each Decoder Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class ImageTransformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               vocab_size, max_pos_encoding, rate=0.1):\n",
    "    super(ImageTransformer, self).__init__()\n",
    "\n",
    "    self.decoder = DecoderBlock(num_layers, d_model, num_heads, dff, \n",
    "                           vocab_size, max_pos_encoding, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, mask):\n",
    "\n",
    "    enc_output, attention_weights = self.decoder(inp, training, mask)  # (batch_size, inp_seq_len, d_model)    \n",
    "    \n",
    "    final_output = self.final_layer(enc_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 783, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = ImageTransformer(\n",
    "    num_layers=8, d_model=64, num_heads=2, dff=256,\n",
    "    vocab_size=8, max_pos_encoding=784)\n",
    "\n",
    "temp_input = tf.random.uniform((32, 783), dtype=tf.int64, minval=0, maxval=8)\n",
    "temp_target = tf.random.uniform((32, 783), dtype=tf.int64, minval=0, maxval=10)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# sample_transformer.summary()\n",
    "# plot_model(sample_transformer, layer_range=[\"decoder_block_8\", \"dense_423\"], expand_nested=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "## Set hyperparameters\n",
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "d_model = 64\n",
    "dff = 256\n",
    "num_heads = 4\n",
    "\n",
    "vocab_size = len(centroids) # num_centroids\n",
    "max_sequence_length = image_width * image_height\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=2000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.95, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='sum_over_batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  loss_ = loss_object(real, pred)  \n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing\n",
    "\n",
    "Create a new Image Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "UiysUa--4tOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers: 8, d_model: 64, num_heads: 4, dff: 256, vocab_size: 8, max_pos_encoding: 784, rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:58:24.795649: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"image_transformer_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_block_12 (DecoderBlo multiple                  450688    \n",
      "_________________________________________________________________\n",
      "dense_615 (Dense)            multiple                  520       \n",
      "=================================================================\n",
      "Total params: 451,208\n",
      "Trainable params: 451,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = ImageTransformer(num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                          max_pos_encoding=max_sequence_length, \n",
    "                          rate=dropout_rate)\n",
    "print(f'num_layers: {num_layers}, d_model: {d_model}, num_heads: {num_heads}, dff: {dff}, vocab_size: {vocab_size}, max_pos_encoding: {max_sequence_length}, rate: {dropout_rate}')\n",
    "transformer(*next(iter(train_dataset)), mask=True)\n",
    "transformer.summary()\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(transformer, expand_nested=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "hNhuYfllndLZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a checkpoint exists, restore the latest checkpoint. Skip if you want to retrain the model from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    " ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    " print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):  \n",
    "  # add start of sequence token\n",
    "  tar_inp = inp[:, :-1]\n",
    "  tar_real = inp[:, 1:]\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(tar_inp, tar, \n",
    "                                 training=True, \n",
    "                                 mask=True)\n",
    "    loss = loss_object(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "bbvmaKNiznHZ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783d87477bdf4b5bab8d3861af9c78b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afdab71b93f4e5eb0230d79af1c1162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1130987/718586293.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_pb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbatch_pb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss: {loss:.4f}, Learning rate: {optimizer._decayed_lr(tf.float32):.5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "epoch_pb = tqdm(range(EPOCHS), desc=f'Training progress')\n",
    "for epoch in epoch_pb:  \n",
    "  \n",
    "  batch_pb = tqdm(enumerate(train_dataset), desc=f'Epoch {epoch+1}')\n",
    "  for (batch, (inp, tar)) in batch_pb:\n",
    "    loss = train_step(inp, tar)\n",
    "    batch_pb.set_postfix_str(f'Loss: {loss:.4f}, Learning rate: {optimizer._decayed_lr(tf.float32):.5f}')\n",
    "      \n",
    "  if (epoch + 1) % 1 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "Remove half of the image values and restore them iteratively by the outputs of the image transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "eval_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=eval_step_signature)\n",
    "def call_transformer(inp, tar):\n",
    "    return transformer(inp, tar, training=False, mask=True)\n",
    "\n",
    "def evaluate(inp_sequence):\n",
    "\n",
    "  # remove half of the input \n",
    "  idx = int(inp_sequence.shape[-1] / 2) \n",
    "  seq = inp_sequence[:, :idx]\n",
    "\n",
    "  # create target and transform to batched shape\n",
    "  target = tf.expand_dims(tf.cast([0], dtype=tf.int64), axis=0)\n",
    "  target = tf.tile(target, [seq.shape[0], 1])\n",
    "\n",
    "  for _ in tqdm(range(idx)):\n",
    "    logits, _ = call_transformer(seq, target)\n",
    "    # apply softmax on the logits and return the result\n",
    "    predictions = tf.random.categorical(logits[:, -1], 1, dtype=tf.int64)\n",
    "    # append prediction\n",
    "    seq = tf.concat([seq, predictions], axis=-1)\n",
    "\n",
    "  return seq\n",
    "\n",
    "def process_batch(inp_sequence):\n",
    "  idx = int(inp_sequence.shape[-1] / 2) \n",
    "  halfseq = inp_sequence[:, :idx]\n",
    "  showSeq(halfseq, (image_width//2, image_height), inp_sequence.shape[0], cmap='gray')\n",
    "  showSeq(inp_sequence, (image_width, image_height), inp_sequence.shape[0], cmap='gray')\n",
    "  result_seq = evaluate(inp_sequence)\n",
    "  showSeq(result_seq, (image_width, image_height), inp_sequence.shape[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "YsxrAlvFG8SZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 11:12:08.195746: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-11-01 11:12:08.197794: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8198114343747cb9054259f730d22f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(iter(val_dataset))\n",
    "process_batch(batch[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "To explore the model further you can examine one of the following points:\n",
    "\n",
    "- Change the hyperparameters: e.g. the number of layers from 8 to 24 - do not forget to adjust the batch size such that the model still fits into your memory.\n",
    "- Use a different dataset: e.g. 'cifar10' - do not forget to adjust the image size variables and recompute the centroids, change the centroids size to a bigger number (128).\n",
    "- Add a classification head: only the generative pretraining is implemented, yet. Add a final dense layer and a classification loss. First run the model with a generative loss only, than run it a second time with a mixed classification and generative loss to classify the inputs.\n",
    "- Use a different learning rate schedule or activation function: the original ImageGPT paper used a cosine schedule with no dropout and a gelu activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "- [Transformers Tutorial](https://www.tensorflow.org/tutorials/text/transformer) - In depth tutorial on transformers in TF2.\n",
    "\n",
    "- [Illustrated Transformers Guide](http://jalammar.github.io/illustrated-transformer/) - Quick and intuitive explanation of transformers.\n",
    "\n",
    "- [Image GPT Blog](https://openai.com/blog/image-gpt/) - original ImageGPT by Chen et al.\n",
    "\n",
    "- [ImageGPT in PyTorch](https://github.com/teddykoker/image-gpt) - an implementation of the code above for PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
