{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dee3ae3-fbbe-42aa-abd2-96ff6bcf38e4",
   "metadata": {},
   "source": [
    "# Conditional generative transformer: MNIST demo\n",
    "\n",
    "Based off https://github.com/GregorKobsik/ImageTransformer\n",
    "\n",
    "- mnist dataset\n",
    "    - quantize to 8 values\n",
    "    - random masking\n",
    "- model\n",
    "    - attention mask so that each pixel doesn't get itself as input?\n",
    "    - predict output distribution for particular pixel\n",
    "- training\n",
    "    - learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c53df5c-78a7-4707-9552-7c9171e32ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = input()\n",
    "print()\n",
    "new_kernel = True\n",
    "\n",
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb_id = wandb.util.generate_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5740aeb7-c7ad-4af0-8f85-525a98cc44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 16,\n",
    "    'n_colors': 8,\n",
    "    'model': {\n",
    "        'n_layers': 8,\n",
    "        'ffl_dim': 256,\n",
    "        'embd_dim': 64,\n",
    "        'n_heads': 4,\n",
    "        'dropout_rate': 0.1,\n",
    "    },\n",
    "    'dataset': {\n",
    "        'buffer_size': 10000,\n",
    "        'min_mask': 100,\n",
    "        'max_mask': 740,\n",
    "        'square_regions': False,\n",
    "    },\n",
    "    'lr_warmup_steps': 2000,\n",
    "    'steps_per_epoch': 1000,\n",
    "    'n_epochs': 10,\n",
    "    'seq_length': 784,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8541efbc-9c69-4d57-89e4-ebdeab69d710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxeonyx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/maxeonyx/cgt-mnist/runs/3legdx44\" target=\"_blank\">shuffle-ogqedrlk</a></strong> to <a href=\"https://wandb.ai/maxeonyx/cgt-mnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb:\n",
    "    resume = not new_kernel\n",
    "    wandb.init(project='cgt-mnist', entity='maxeonyx', name=model_name + '-' + wandb_id, config=config, resume=resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43453531-6931-4e17-bd71-4a32007b6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import tensorflow as tf\n",
    "# https://stackoverflow.com/a/60699372/7989988\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Input, layers\n",
    "from IPython.display import display\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import enlighten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b798f404-f9b6-4145-b04d-e518ef47f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:20:35.471728: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 15:20:35.957988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10421 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1\n",
      "2021-10-20 15:20:36.026247: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-20 15:20:36.062404: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:20:36.140707: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset, metadata = tfds.load('mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "dataset_train_original = dataset['train']\n",
    "dataset_test_original = dataset['test']\n",
    "\n",
    "image_width=28\n",
    "image_height=28\n",
    "\n",
    "fig = tfds.show_examples(dataset_train_original.take(1), metadata)\n",
    "fig = tfds.show_examples(dataset_test_original.take(1), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c833c8c7-cd8a-4d0f-bdf0-a5b11dc4ab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>                          K-Means clustering to make 8-color MNIST Dataset                          </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>cen. 0    cen. 1    cen. 2    cen. 3    cen. 4    cen. 5    cen. 6    cen. 7                        </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>0.000     0.095     0.211     0.347     0.509     0.691     0.857     0.988                         </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Discretize to 8 colors 100%|███████████████████████████████| 60/60 [00:16&lt;00:00, 3.72 minibatches/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def normalize_image(image, label):\n",
    "    return tf.cast(image, dtype=tf.float16) / 255.0, label\n",
    "\n",
    "def find_centroids(ds_train, num_clusters, batch_size):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, batch_size=batch_size, verbose=True)\n",
    "    ds_batched = ds_train.batch(batch_size)\n",
    "    with enlighten.get_manager() as manager:\n",
    "        title = manager.status_bar(f\"K-Means clustering to make {num_clusters}-color MNIST Dataset\", justify=enlighten.Justify.CENTER)\n",
    "        clusters_names = manager.status_bar(''.join('{:<10}'.format(f\"cen. {i}\") for i in range(num_clusters)))\n",
    "        clusters_status = manager.status_bar(''.join('{:<10}'.format('??????') for _ in range(num_clusters)))\n",
    "        pbar = manager.counter(total=60000//batch_size, desc='Discretize to 8 colors', unit='minibatches')\n",
    "        for img, _ in pbar(iter(ds_batched)):\n",
    "            pixels = img.numpy().reshape(-1, 1)\n",
    "            kmeans.partial_fit(pixels)\n",
    "            clusters_status.update(''.join('{:<10.3f}'.format(x[0]) for x in np.sort(kmeans.cluster_centers_, axis=0)))\n",
    "\n",
    "        return kmeans.cluster_centers_\n",
    "\n",
    "centroids = find_centroids(dataset_train_original.map(normalize_image), num_clusters=config['n_colors'], batch_size=1000)\n",
    "centroids = tf.convert_to_tensor(np.sort(centroids, axis=0), dtype=tf.float16)\n",
    "print(centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6d2879-99a8-4ecd-b0d2-fef0a0155c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_euclidean_distance(a, b):\n",
    "    b = tf.transpose(b)        \n",
    "    a2 = tf.math.reduce_sum(tf.math.square(a), axis=1, keepdims=True)\n",
    "    b2 = tf.math.reduce_sum(tf.math.square(b), axis=0, keepdims=True)\n",
    "    ab = tf.linalg.matmul(a, b)\n",
    "    return a2 - 2 * ab + b2\n",
    "\n",
    "def flatten(image, label):\n",
    "    shape = tf.shape(image) # (height, width, color)\n",
    "    sequence = tf.reshape(image, (-1, shape[2])) # (height * width, color)\n",
    "    return sequence, label\n",
    "\n",
    "def quantize(sequence, label):\n",
    "    d = squared_euclidean_distance(sequence, centroids) # (height * width, centroids)\n",
    "    sequence = tf.math.argmin(d, axis=1)  # (height * width)\n",
    "    return sequence, label\n",
    "\n",
    "def unquantize(x):\n",
    "    x_one_hot = tf.cast(tf.one_hot(x, depth=len(centroids)), dtype=tf.float16)  # (seq, num_centroids)\n",
    "    return tf.linalg.matmul(x_one_hot,centroids)  # (seq, num_features)\n",
    "\n",
    "def shuffled_indices(sequence, label):\n",
    "    \n",
    "    idxs = tf.range(config['seq_length'], dtype=tf.int32)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    sequence = tf.gather(sequence, idxs)\n",
    "    \n",
    "    return sequence, idxs, label\n",
    "\n",
    "def indices(sequence, label):\n",
    "    \n",
    "    idxs = tf.range(config['seq_length'], dtype=tf.int32)\n",
    "    \n",
    "    return sequence, idxs, label\n",
    "\n",
    "dataset_train = (\n",
    "    dataset_train_original\n",
    "    .map(normalize_image)\n",
    "    .map(flatten)\n",
    "    .map(quantize)\n",
    "    .map(shuffled_indices)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .shuffle(config['dataset']['buffer_size'])\n",
    "    .batch(config['batch_size'], drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "dataset_test = (\n",
    "    dataset_test_original\n",
    "    .map(normalize_image)\n",
    "    .map(flatten)\n",
    "    .map(quantize)\n",
    "    .map(indices)\n",
    "    .cache()\n",
    "    .batch(config['batch_size'], drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5468d963-4285-4eed-97fc-40178e4f78a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unquantized:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:20:53.066896: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:20:53.592417: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def np_showSeq(seq, size, max_images=3, cmap=None):\n",
    "    \"\"\" Show one or more images encoded as sequence. (numpy version)\n",
    "\n",
    "        seq: numpy array of sequences which encode the image. Either a single sequence or multiple sequences.\n",
    "        size: the image size. e.g. (28, 28) for `mnist` images.\n",
    "        max_images: the maximum number of images to display.\n",
    "    \"\"\" \n",
    "    batch = seq.shape[0]\n",
    "    num_show_img = min(max_images, seq.shape[0])\n",
    "    img = np.reshape(seq, (batch, *size, -1))\n",
    "    if img.shape[-1] == 1:\n",
    "        img = np.squeeze(img, axis=-1)    \n",
    "    \n",
    "    fig=plt.figure(figsize=(3*num_show_img, 3))\n",
    "    for i in range(num_show_img):\n",
    "        ax = fig.add_subplot(1, num_show_img, i+1)\n",
    "        ax.set_axis_off()\n",
    "        plt.imshow(img[i], cmap=cmap)\n",
    "    plt.show()\n",
    "\n",
    "def showSeq(seq, size, max_images=3, cmap='gray', do_unquantize=True):\n",
    "    \"\"\" Show one or more images encoded as sequence. (tensorflow version)\n",
    "\n",
    "        seq: tensor of sequences which encode the image. Either a single sequence or multiple sequences.\n",
    "        size: the image size. e.g. (28, 28) for `mnist` images.\n",
    "        max_images: the maximum number of images to display.\n",
    "    \"\"\"\n",
    "    if do_unquantize:\n",
    "        seq = tf.map_fn(fn=unquantize, elems=seq, fn_output_signature=tf.float16)\n",
    "    seq = tf.cast(seq, float).numpy()\n",
    "\n",
    "    np_showSeq(seq, size, max_images, cmap)\n",
    "    \n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "ds_test_unquantized = (\n",
    "    dataset_test_original\n",
    "    .map(normalize_image)\n",
    "    .map(flatten)\n",
    "    .batch(NUM_SAMPLES)\n",
    ")\n",
    "ds_test_quantized = (\n",
    "    dataset_test_original\n",
    "    .map(normalize_image)\n",
    "    .map(flatten)\n",
    "    .map(quantize)\n",
    "    .batch(NUM_SAMPLES)\n",
    ")\n",
    "print(\"unquantized:\")\n",
    "examples_unquantized, _ = next(iter(ds_test_unquantized))\n",
    "showSeq(examples_unquantized, (image_width, image_height), NUM_SAMPLES, do_unquantize=False)\n",
    "print(\"quantized:\")\n",
    "examples_quantized, _ = next(iter(ds_test_quantized))\n",
    "showSeq(examples_quantized, (image_width, image_height), NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b8ccd56-8f1c-4773-abc3-6c39f478d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size_q, size_k):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size_q, size_k)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def scaled_dot_product_attention(k, q, v, mask):\n",
    "    batch_size = tf.shape(k)[0]\n",
    "    seq_len_kv = tf.shape(k)[-2]\n",
    "    kq_dim = tf.shape(k)[-1]\n",
    "    seq_len_q = tf.shape(q)[-2]\n",
    "    v_dim = tf.shape(v)[-1]\n",
    "    \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    # shape: (batch_size, n_heads, seq_len_q, seq_len_kv)\n",
    "    \n",
    "    dk = tf.cast(kq_dim, tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = create_look_ahead_mask(seq_len_q, seq_len_kv)\n",
    "        scaled_attention_logits += mask * -1e9 # batch dim broadcast\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # sums to 1 along last axis\n",
    "    # shape: (batch_size, seq_len_q, seq_len_kv)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    # shape: (batch_size, seq_len_q, v_dim)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "def multi_head_attention(embd_dim, n_heads):\n",
    "    \n",
    "    wk = layers.Dense(embd_dim)\n",
    "    wq = layers.Dense(embd_dim)\n",
    "    wv = layers.Dense(embd_dim)\n",
    "    dense = layers.Dense(embd_dim)\n",
    "    \n",
    "    assert embd_dim % n_heads == 0, \"embd_dim must divide evenly into n_heads\"\n",
    "    head_width = embd_dim//n_heads\n",
    "    \n",
    "    def split_heads(x, batch_size):\n",
    "        # reshape from (batch_size, seq_length, embd_dim) to (batch_size, num_heads, seq_len, head_width)\n",
    "        x = tf.reshape(x, (batch_size, -1, n_heads, head_width))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(k, q, v, mask):\n",
    "        batch_size = tf.shape(k)[0]\n",
    "        \n",
    "        k = wk(k)\n",
    "        q = wk(q)\n",
    "        v = wk(v)\n",
    "        # shape: (batch_size, seq_len_*, embd_dim)\n",
    "        \n",
    "        k = split_heads(k, batch_size)\n",
    "        q = split_heads(q, batch_size)\n",
    "        v = split_heads(v, batch_size)\n",
    "        # shape: (batch_size, num_heads, seq_len_*, head_width)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(k, q, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, embd_dim))\n",
    "        output = dense(concat_attention)\n",
    "        return output, attention_weights\n",
    "    return call\n",
    "    \n",
    "def pointwise_feedforward_layer(hidden_dim, out_dim):\n",
    "    dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "    dense2 = layers.Dense(out_dim)\n",
    "    def call(x):\n",
    "        x = dense1(x)\n",
    "        x = dense2(x)\n",
    "        return x\n",
    "    return call\n",
    "        \n",
    "\n",
    "def transformer(n_colors, seq_length, mask, embd_dim, ffl_dim, n_heads, n_layers, dropout_rate):\n",
    "    print(\"n_colors, seq_length, mask, embd_dim, ffl_dim, n_heads, n_layers, dropout_rate\")\n",
    "    print(n_colors, seq_length, mask, embd_dim, ffl_dim, n_heads, n_layers, dropout_rate)\n",
    "    def decoder_layer():\n",
    "        mha = multi_head_attention(embd_dim, n_heads)\n",
    "        ffl = pointwise_feedforward_layer(ffl_dim, embd_dim)\n",
    "        layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        dropout1 = layers.Dropout(dropout_rate)\n",
    "        dropout2 = layers.Dropout(dropout_rate)\n",
    "        def call(x, mask):\n",
    "            out1 = layernorm1(dropout1(x))\n",
    "            attn_out, attn_weights = mha(out1, out1, out1, mask)\n",
    "            x += attn_out\n",
    "            \n",
    "            out2 = layernorm2(dropout2(x))\n",
    "            ffl_out = ffl(out2)\n",
    "            x += ffl_out\n",
    "            \n",
    "            return x\n",
    "        return call\n",
    "\n",
    "    def decoder_block():\n",
    "        color_embedding = layers.Embedding(n_colors, embd_dim)\n",
    "        position_embedding = layers.Embedding(seq_length, embd_dim)\n",
    "        dec_layers = [decoder_layer() for _ in range(n_layers)]\n",
    "        dropout = layers.Dropout(dropout_rate)\n",
    "        layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        def call(colors, idxs, mask):\n",
    "            batch = tf.shape(colors)[0]\n",
    "            seq_len = tf.shape(colors)[1]\n",
    "            \n",
    "            col_embd = color_embedding(colors)\n",
    "            pos_embd = tf.tile(position_embedding(idxs), [batch, 1, 1])\n",
    "            x = col_embd + pos_embd\n",
    "            for i in range(n_layers):\n",
    "                x = dec_layers[i](x, mask)\n",
    "            x = layer_norm(dropout(x))\n",
    "            return x\n",
    "        return call\n",
    "    \n",
    "    colors = keras.Input([seq_length])    \n",
    "    idxs = keras.Input([seq_length])\n",
    "    x = decoder_block()(colors, idxs, mask)\n",
    "    final_layer = layers.Dense(n_colors)(x)\n",
    "    \n",
    "    return Model(inputs=[colors, idxs], outputs=[final_layer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23adc812-dee5-4015-b446-23cb2e23257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_colors, seq_length, mask, embd_dim, ffl_dim, n_heads, n_layers, dropout_rate\n",
      "8 784 True 64 256 4 8 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = transformer(\n",
    "    config['n_colors'],\n",
    "    config['seq_length'],\n",
    "    mask=True,\n",
    "    **config['model'],\n",
    ")\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file=\"model.png\", expand_nested=True, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c804721a-eb63-4b93-82b4-49ee7b0e5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:51:21.174045: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-10-20 15:51:21.175013: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:51:22.077868: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".enlighten-fg-green {\n",
       "  color: #00cd00;\n",
       "}\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>                                      Training model 'shuffle'                                      </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Loss: &#63;&#63;&#63;&#63;&#63;&#63;, Learning Rate: &#63;&#63;&#63;&#63;&#63;&#63;&#63;                                                                </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Epochs   0%|<span class=\"enlighten-fg-green\">                                                        </span>|  0/10 [00:00&lt;&#63;, 0.00 epochs/s]</pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre></pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_1625192/1729363644.py:30 train_step  *\n        x_out = model(x_inp, i_inp)\n    /am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/keras/engine/base_layer.py:1020 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/keras/engine/input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'strided_slice:0' shape=(None, None) dtype=int64>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1625192/1729363644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mstep_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_1625192/1729363644.py:30 train_step  *\n        x_out = model(x_inp, i_inp)\n    /am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/keras/engine/base_layer.py:1020 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /am/monterey/home1/clarkemaxw/.cache/pypoetry/virtualenvs/msc-4QRxScII-py3.8/lib/python3.8/site-packages/keras/engine/input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'strided_slice:0' shape=(None, None) dtype=int64>]\n"
     ]
    }
   ],
   "source": [
    "class TransformerLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embd_dim, warmup_steps):\n",
    "        super(TransformerLearningRateSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = embd_dim\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "lr_schedule = TransformerLearningRateSchedule(config['model']['embd_dim'], config['lr_warmup_steps'])\n",
    "# params taken from the linked notebook\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.95, epsilon=1e-9)\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum_over_batch_size')\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
    "]\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(colors, idxs):\n",
    "    x_inp = colors[:, :-1]\n",
    "    x_tar = colors[:, 1:]\n",
    "    i_inp = idxs[:, :-1]\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_out = model(x_inp, i_inp)\n",
    "        loss = loss_function(x_tar, x_out)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "eval_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "@tf.function(input_signature=eval_step_signature)\n",
    "def eval_step(inp):\n",
    "    return model(inp, training=False)\n",
    "\n",
    "def evaluate(inp_colors, inp_idxs, manager=None):\n",
    "    \n",
    "    out_colors = inp_colors\n",
    "\n",
    "    # create target and transform to batched shape\n",
    "    target = tf.expand_dims(tf.cast([0], dtype=tf.int64), axis=0)\n",
    "    target = tf.tile(target, [seq.shape[0], 1])\n",
    "    if manager is None:\n",
    "        manager = enlighten.get_manager()\n",
    "    evaluate_counter = manager.counter(total=idx, desc=\"Evaluating\", unit='pixels', leave=False)\n",
    "    for _ in evaluate_counter(range(seq_length)):\n",
    "        logits = eval_step(out_colors, inp_idxs)\n",
    "        # apply softmax on the logits and return the result\n",
    "        predictions = tf.random.categorical(logits[:, -1], 1, dtype=tf.int64)\n",
    "        # append prediction\n",
    "        out_colors = tf.concat([out_colors, predictions], axis=-1)\n",
    "    evaluate_counter.close()\n",
    "    return out_colors\n",
    "\n",
    "def process_batch(inp_colors, inp_idxs, show_input=True, show_output=True, manager=None):\n",
    "    batch_size = inp_colors.shape[0]\n",
    "    n = int(inp_colors.shape[-1] / 2) \n",
    "    half_colors = inp_colors[:, :n]\n",
    "    half_idxs = inp_idxs[:, :n]\n",
    "    if show_input:\n",
    "        showSeq(half_colors, (image_width//2, image_height), batch_size)\n",
    "        showSeq(inp_colors, (image_width, image_height), batch_size)\n",
    "    if show_output:\n",
    "        result_seq = evaluate(inp_sequence, manager=manager)\n",
    "        showSeq(result_seq, (image_width, image_height), batch_size)\n",
    "\n",
    "batch_colors, batch_idxs, batch_labels = next(iter(dataset_test))\n",
    "process_batch(batch_colors[0:10], batch_idxs[0:10], show_output=False)\n",
    "\n",
    "n_epochs = config['n_epochs']\n",
    "steps_per_epoch = config['steps_per_epoch']\n",
    "loss_history = np.zeros([n_epochs*steps_per_epoch])\n",
    "window_size = 10\n",
    "\n",
    "dataset_iterator = iter(dataset_train)\n",
    "\n",
    "with enlighten.get_manager() as manager:\n",
    "    status = manager.status_bar(f\"Training model '{model_name}'\", justify=enlighten.Justify.CENTER)\n",
    "    info = manager.status_bar('Loss: ??????, Learning Rate: ???????')\n",
    "    def update_infobar(loss, learning_rate):\n",
    "        info.update(f'Loss (10 step avg.): {loss:.5f}, Learning Rate: {learning_rate:.6f}')\n",
    "    epochs = manager.counter(total=n_epochs, desc='Epochs', color='green', unit='epochs')\n",
    "    last_loss = None\n",
    "    last_step = 0\n",
    "    for epoch in epochs(range(n_epochs)):\n",
    "        steps = manager.counter(total=steps_per_epoch, desc=f'Epoch {epoch:<3}', color='blue', unit='steps')\n",
    "        for step in steps(range(steps_per_epoch)):\n",
    "            colors, idxs, label = next(dataset_iterator)\n",
    "            loss = train_step(colors, idxs)\n",
    "            step_index = epoch*steps_per_epoch + step\n",
    "            loss_history[step_index] = loss\n",
    "            running_mean = np.mean(loss_history[max(0, step_index-window_size) : step_index+1])\n",
    "            update_infobar(running_mean, optimizer._decayed_lr(tf.float32))\n",
    "            if last_loss is None or (running_mean <= last_loss * 0.9 and step_index >= last_step + 10):\n",
    "                last_loss = running_mean\n",
    "                last_step = step_index\n",
    "                print(f\"Step {step_index}, Loss (10 step avg.): {last_loss.numpy()}\")\n",
    "                process_batch(batch[0][0:10], show_input=False, manager=manager)\n",
    "\n",
    "        # todo eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8fd21-7a5b-4d76-9946-8476c45c0a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1117cd7-179c-4c0b-b800-4f7e799efe88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
