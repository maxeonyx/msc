{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxeonyx\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">polished-grass-24</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist/runs/1ex250ma\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist/runs/1ex250ma</a><br/>\n",
       "                Run data is saved locally in <code>/am/monterey/home1/clarkemaxw/conditional-mnist/wandb/run-20210923_115715-1ex250ma</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 11:57:20.885006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-23 11:57:21.437538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "tf.constant([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(masks):\n",
    "    image_masks = tf.reshape(masks, [-1, 28, 28])\n",
    "    return image_masks\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=28):\n",
    "    \n",
    "    i = tf.range(dims)\n",
    "    i = i.expand_dims(-2)\n",
    "    \n",
    "    vals = vals.expand_dims(-1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = \n",
    "\n",
    "def img_to_tuples(img, pos_encoding_dims):\n",
    "    assert(pos_encoding_dims % 4 == 0)\n",
    "    height, width, chan = img.shape\n",
    "    vals = tf.reshape(img, [width*height, chan])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1, 1])\n",
    "    cols = tf.reshape(cols, [-1, 1])\n",
    "    tups = tf.concat([vals, rows, cols], axis=-1)\n",
    "    \n",
    "    pos_x_dims = pos_encoding_dims // 2\n",
    "    pos_y_dims = pos_encoding_dims // 2\n",
    "    \n",
    "    pos_x_sin_dims = pos_x_dims // 2\n",
    "    pos_x_cos_dims = pos_x_dims // 2\n",
    "    pos_x_rates = row / tf.pow(28, tf.range(pos_x_sin_dims)/pos_x_dims)\n",
    "    \n",
    "    pos_y_sin_dims = pos_y_dims // 2\n",
    "    pos_y_cos_dims = pos_y_dims // 2\n",
    "    \n",
    "    return tups\n",
    "\n",
    "def random_mask():\n",
    "    idxs = tf.range(784)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "    idxs = idxs[:n]\n",
    "    return idxs_to_multihot(idxs)\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    print(idxs)\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask[np.newaxis, :]), np.uint8)[0]\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: <_OptionsDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: {image: (8, 28, 28, 1), label: (8,), tuples: (8, 784, 3), mask: (8, 784)}, types: {image: tf.uint8, label: tf.int64, tuples: tf.float32, mask: tf.bool}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def make_dataset_generator(split='train', batch_size=None):\n",
    "    \n",
    "    dataset = tfds.load('mnist', split=split, shuffle_files=True)\n",
    "    \n",
    "    print(\"Start:\", dataset)\n",
    "    \n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(60000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    def map_just(component, func):\n",
    "        def do(data):\n",
    "            data[component] = func(data[component])\n",
    "            return data\n",
    "        return do\n",
    "    \n",
    "    def map_add(component, func):\n",
    "        def do(data):\n",
    "            data[component] = func()\n",
    "            return data\n",
    "        return do\n",
    "    \n",
    "    def add_tuples(data):\n",
    "        data['tuples'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    \n",
    "    def offset_tuples(data):\n",
    "        offset = tf.cast(data['offset'], tf.float32)\n",
    "        data['tuples'] = tf.stack([\n",
    "            data['tuples'][:, 0],\n",
    "            data['tuples'][:, 1] + offset[0],\n",
    "            data['tuples'][:, 1] + offset[1],\n",
    "        ], axis=-1)\n",
    "        data['tuples'] = tf.random.shuffle(data['tuples'])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def add_target_px_val(data):\n",
    "        offset = data['offset']\n",
    "        data['target_px'] = tf.cast(data['image'][offset[0],offset[1]], tf.float32)\n",
    "        return data\n",
    "    \n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask\n",
    "        mask = tf.logical_and(mask, square_mask)\n",
    "        data['mask'] = mask\n",
    "        return data\n",
    "    \n",
    "    dataset = dataset.map(add_tuples)\n",
    "    # data['image'][:, 0] are vals\n",
    "    # data['image'][:, 1] are rows\n",
    "    # data['image'][:, 2] are cols\n",
    "    \n",
    "    dataset = dataset.map(map_add('mask', random_mask))\n",
    "#     dataset = dataset.map(map_add('offset', random_offset))\n",
    "#     dataset = dataset.map(offset_tuples)\n",
    "#     dataset = dataset.map(add_target_px_val)\n",
    "    \n",
    "    if batch_size is not None:\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "make_dataset_generator(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qr9uxXbhzo9g",
    "outputId": "158a591e-46a3-4307-e49a-12b0052586f3"
   },
   "outputs": [],
   "source": [
    "# reformat MNIST data into (x, y, val) tuples\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# [row, col, value] tuples\n",
    "def mnist_to_rcv_tuples(dataset):\n",
    "    # positional encoding is just indices as floats\n",
    "    x_idx = np.arange(dataset.shape[1]).astype(np.float32)\n",
    "    y_idx = np.arange(dataset.shape[2]).astype(np.float32)\n",
    "\n",
    "    # make 28x28 cartesian product and repeat N times\n",
    "    idxs = np.transpose([np.repeat(y_idx, len(x_idx)),np.tile(x_idx, len(y_idx))])\n",
    "    \n",
    "    all_idxs = np.tile(idxs.reshape(-1), dataset.shape[0]).reshape(-1,784,2)\n",
    "    # pixel data as float between 0 and 255\n",
    "    all = dataset.astype(np.float32).reshape(-1, 784, 1)\n",
    "    \n",
    "    big = np.dstack((all_idxs, all))\n",
    "    \n",
    "    return big\n",
    "\n",
    "def random_offset_tuples(dataset, dataset_y, min_masked_px=0, max_masked_px=784, random_squares=True):\n",
    "    tuples = mnist_to_rcv_tuples(dataset)\n",
    "    offsets = np.random.randint(0, 28, [dataset.shape[0], 2])\n",
    "    mask_sizes = np.random.randint(min_masked_px, max_masked_px, [dataset.shape[0]])\n",
    "    tuples[:, :, 0] -= offsets[:, 0, np.newaxis] # row (y) offset\n",
    "    tuples[:, :, 1] -= offsets[:, 1, np.newaxis] # col (x) offset\n",
    "    target_idxs = offsets[:, 0] * 28 + offsets[:, 1]\n",
    "    \n",
    "    # permutations \n",
    "    masks = np.ones([dataset.shape[0], 784], dtype=np.bool_)\n",
    "    for i in range(dataset.shape[0]):\n",
    "        # shuffle tuples\n",
    "        tup_shuffle_idxs = np.random.permutation(784)\n",
    "        tuples[i] = tuples[i, tup_shuffle_idxs]\n",
    "        \n",
    "        idxs = np.random.permutation(784)\n",
    "                           \n",
    "        n_mask_px = np.random.randint(min_masked_px, max_masked_px)\n",
    "        # n mask plus always mask the target idx\n",
    "        mask_idxs = np.concatenate([idxs[:n_mask_px], target_idxs[i:i+1]])\n",
    "        \n",
    "        if random_squares:\n",
    "            # random square cut out of the masks\n",
    "            start_row, start_col = np.random.randint(0, 28, [2])\n",
    "            end_row = np.random.randint(start_row, 28)\n",
    "            end_col = np.random.randint(start_col, 28)\n",
    "            height, width = abs(end_row - start_row), abs(end_col - start_col)\n",
    "            square_row_idxs, square_col_idxs = np.indices([height, width])\n",
    "            square_row_idxs += start_row\n",
    "            square_col_idxs += start_col\n",
    "            square_mask_idxs = square_row_idxs * 28 + square_col_idxs\n",
    "            square_mask_idxs = square_mask_idxs.reshape(-1)\n",
    "            \n",
    "            mask_idxs = np.concatenate([mask_idxs, square_mask_idxs])\n",
    "            \n",
    "        masks[i:i+1] = idxs_to_multihot(mask_idxs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    target_pxs = dataset[np.arange(dataset.shape[0]), offsets[:, 0], offsets[:, 1]].astype(np.float32)\n",
    "    return {\n",
    "        \"digits\": tf.cast(dataset_y, tf.uint8),\n",
    "        \"images\": tf.cast(dataset, tf.uint8),\n",
    "        \"tuples\": tf.cast(tuples, tf.float32),\n",
    "        \"offsets\": offsets,\n",
    "        \"target_pxs\": target_pxs,\n",
    "        \"target_idxs\": target_idxs,\n",
    "        \"masks\": tf.cast(masks, tf.bool)\n",
    "    }\n",
    "\n",
    "def load_to_tensorflow(file):\n",
    "    dataset = np.load(file)\n",
    "    return { k: tf.constant(v) for k, v in dataset.items() }\n",
    "    \n",
    "load_saved_dataset = True\n",
    "if not load_saved_dataset:\n",
    "    train_dataset = random_offset_tuples(x_train, y_train)\n",
    "    test_dataset = random_offset_tuples(x_test, y_test)\n",
    "    # give very few pixels\n",
    "    full_masks_dataset = random_offset_tuples(x_test, y_test, min_masked_px=770, max_masked_px=784, random_squares=True)\n",
    "    np.savez(\"train_dataset.npz\", **train_dataset)\n",
    "    np.savez(\"test_dataset.npz\", **test_dataset)\n",
    "    np.savez(\"full_masks_dataset.npz\", **full_masks_dataset)\n",
    "else:\n",
    "    train_dataset = load_to_tensorflow(\"train_dataset.npz\")\n",
    "    test_dataset = load_to_tensorflow(\"test_dataset.npz\")\n",
    "    full_masks_dataset = load_to_tensorflow(\"full_masks_dataset.npz\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits shape (60000,)\n",
      "images shape (60000, 28, 28)\n",
      "tuples shape (60000, 784, 3)\n",
      "offsets shape (60000, 2)\n",
      "target_pxs shape (60000,)\n",
      "target_idxs shape (60000,)\n",
      "masks shape (60000, 784)\n",
      "index 0 which is a tf.Tensor(5, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C019AB0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgElEQVR4nIWSwQKAIAxCqf//59fF6YbTOlghMDaVBJJQLMeHss1YGfgwmV5ZR1IokTeQgrqPvTukhiMSdYqEv9FaNKPHZbM9kkET0KPcR5oYf0QzX0NveiSL9pTYf1/tNNTytbhtvnsZSzddqXqr2xw/5Vy2GmUg2GYfC9JlCN4HVFBykeU+PswAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C0195DEE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAZElEQVR4nGNgGMLgL5Q+jF3i72+GIAYGdWw6jzEwfMQiHMCQ/ZeBgRuHdXwIG/E6hyywgIC8JE6ZXQwM0/DpJOAsBxziTbi1TGFgYGBg2IbfXJIBLndCxHFFCgMDgwCVHYILAAAoIxHY+K0FogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01912FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1 which is a tf.Tensor(0, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDB80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAi0lEQVR4nH2SSQLAIAgDM/z/z/TgAiiWU2ViQAquCPJBkkkS83CwAc9kgTXYH3Zmwgc3IdE6u6wtCNOuwvUaXMKcArPUTT6aqZqRzTZX2FY+IR1asJ8fzfhytz9xQXpIlOdxMxQF+mWL99NhdfvaEjWbcEPvNEe35e/G+FgOJGXqhnNj5saT6u6L+gDWdiYyZrRBqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C0195DEE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/klEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjiUWKy+bBH4Y/p0L+/K2CCDAhSc48IMPwl8GI5yCDLoaksTcj46FSxhfnJzExYnPLZh7vSlEGhr+f0RystvTvywshEPbfP0tR5Ng3/fngLiwDkzyMImn55489nIMueezvPgTn/98jyK71Mfi/CSH57/8FZI2hf55Jwq1v/7uLB1XyPlyu+c9DVI+E/pkIZRks/bMWVY4h7C9UdRHD30Vocgyhf35OMpAN3fTw7/3lFpiSf/48vf7nz58jTehSDAwyDH/+/P3z5+VETCkGBgbJhj9///SqYpWjKwAAyvhsQhtZgPUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01912FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2 which is a tf.Tensor(4, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEVj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDC70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAMUlEQVR4nGP8z4AbMOGRwyHJiE/yP9nGDkVJRnyS/7FKMuINPkodRCtJRiqnocEqCQDf7wQ36smtDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C000B6940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0klEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNQYMBWVKIgYFBn8lZhi2a6fvJnywwlYwMDAzT0j88YmDQY/zz7drJMwdfPhFkQ9KZ9dCKgYHh0cZrJxgYGBjSRO8x4AQr/3XCmExYpDfgk2TAI8moikfyPxMeSQZLfMbisXP7f5wOpBIAADilKi/JV5KoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C0195DEE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 3 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGPyg5u9/e1xyCV9+/7WDMJkwJOXZcRvq8ub3ZXkO7HI2T37/jsOlcfbfv3txyYn8/f3aCYecwtm/v+twacz4/XcHPw65gA+/D4rjMvTv37/zcRk6/ffv3+o45Azu/v69BpfGV79/H+HBJfn39+9IXHLz///9K4/Lxid/v/fgCHAGh99/76CLYcYnNskbx/ApoyoAAGeYO0QsY6cRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDC70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzklEQVR4nGWSUY7EIAzFbDT3v7L3I0BnuqiiVUL68hyIZwUVxWyLnxWAYIKf36RAGGC+Kwu5Ur2yQLPH+p9BqP91E5yu6d3tzU9Tvz63Q47Zl+iozf5q6BiRiOWLQfsRNH9K3a8ulC7zg2CEndOd5pEntp6/pUhYTtMTbSqyS11oRYY+fgXxGD76h8imXp+vivIqQLLQM0S/nAawNvvcsg5DN9vG4kwZxDCE5Q3GaG22HSzPp9t1SAtEZ0he5iOzzl3I140SljqmuOO68/sD0byq29tzhXMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01912FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAfUlEQVR4nGNgGBLAHo/cXyjNhCFT/xu3LheG35dxSj7Bo5GB4e9eXDIiDL9fO+GQUziLz0wGBgZ+HOIBH34fxOMYhvk45X4zoPoDKYQMGBgYNuLS9+o3wxHcNv7GHTjz/8NjAxv43oPbUIY76EII17JiGgqXlDjIjMdGqgIA0yofg3kX8HUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C019AB0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits shape (10000,)\n",
      "images shape (10000, 28, 28)\n",
      "tuples shape (10000, 784, 3)\n",
      "offsets shape (10000, 2)\n",
      "target_pxs shape (10000,)\n",
      "target_idxs shape (10000,)\n",
      "masks shape (10000, 784)\n",
      "index 0 which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6319A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqklEQVR4nGWSWQ4DIQxDnar3v/LrR1YXNIIJCbZjENrBDSSLQNQeVkp/HWbKsPJsL8/+pYqKw4UElTIuDsoiMTI7L30lInGJZmqgryRCUWeSxwXQDlR/JPeUong0z8orrsTPL+cM+sTmQoqiS/qFpsG5LZdYBNg19QSX/Jo3CjFXr+d/Hk9h2+R3bPSrsv285o62d7j5T0m2yV8nS7+idm637F3aLe1e0fMDc9DBXCZMphUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiUlEQVR4nGNgGH5gPgODCg6pe/8YGP4xMDAwHGNgYGBYZYIuX7SY4d9Dhn8MUFXoQJCBwZnB2ZqBgSGLLJeJMTAE45F+h1PG+uc/O5ySrQy7WXHJcRLlLmzgN4MlLilhBoZluOSYsYccDOCWlMfrmn9oGpmQ2J8YGExwSvIxMX3Bbex5Iby2ogIA1cQZm/BGhbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1 which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6314F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlklEQVR4nHWSQRYDIQhDY+9/599FlQSxsxgQQl6CShKSJH7BMf6OdaKqjPbEUyAIDK3zjwNSByEJDzlhtAHn6oFi5QzXqXym4a2ZGsdFNUndLrLV5pEGjcugKZ5TZ58Wu1NOl2tJhXoUVhQW0hJL7Xs5tH5UazOx9DmldbF13HhfD9Bd3f5JFd5cTMTT7jwPxngY11XqC5EUo2X60PwRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAb0lEQVR4nGNgoAf4Py0Wq/hKhr9//zIwMMhhlf2L31AeBoZzDD2Y4r4QqoqBwRGLLnnCZn9nOMaFQ8qHgeE5flfhBH8ZeMjViA/gNPQEebYxMPz4i99GbQYRBGcaqk5kx2KagiTyHU2qjGjnDRwAAMP+GixKciVeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6319D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuklEQVR4nG2SSRYEMQhCP76+/5XpRZwqVe4SFQGVQeYKGRkCqJy08gZCoGcXwoAgGlXd4kQmXKhm1ZzmuCAT8VRGw4nKdETDVcIa+nj+uQSHu2bqCzqu+kPpiCm2BrDBQs1Ow0eLysBurg/Vs5Fy0YmD04Qy4ExCyWlMqKIjSrgBviOOAs2il9c1/ysUkMpvIeBIpmWtNj29VrEJ2XMaCCH1HS4uep1w3VDOeFwZv/WarnQmjgZ766+PP3l4TzrhxXknAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631C40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhklEQVR4nGNgGAzgXy6cyYQuF/7vKZzNgi5pgCGCALpfpiE46Maqc63ErfPUfW6ccgoMN3BrjP+HzEOzU5fBF7fOt2c5cOqsEbrxA6ek/v81OA2VYLiOwkfRmfDvBG5JeYb3OE1lePLPCadOW3Hc+hh6/51lxqWTy4thzV+cOo9twGMsvQAA1vwdZ3JPR08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631D30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 3 which is a tf.Tensor(0, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA70lEQVR4nMXQsWoCQRQF0JvBNNrGdjcgIR8gJJVrqRZCfkMkgn+QlEIIAcHaHzClVWySJkmXSgxqIbKCbcrLxRSbNe7M2uqr7syZefAecOTK9fTp78MLUs2ds9nJ+b71OPMfWzdXAALz9ZrSVCQpclp0bbiRpPVckmPlmUh268Ed2bDsfEVx2skCfsif9qkzxcsZAOCWYsHGDy+K/nuM2zmNuV5E6cQYc5/4+UDG0W07iTFfXlGhl45PJGelKGeQrOElgPFb8vJbqtWW0kYpG2qT8W7ZtdEP/zAcFbI2IniMsOkIAKD6zEGl6qXjweoXXfV/5XmKZEMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxElEQVR4nG2SWw4EIQgEi4n3v3LtBw8x2ZloFEw33YCiAgLK/qOXIYSEEDC7CNYCtQ5IgcowrPAlrGRHPVC5ok+iCLKQJDe477K+mML2Z+Vvucw2NX0TTDwCQhrMJfJBsRxjgoPgruHKHcl8E6RkUjLH25Vvvja1QB6aqqFDTq7fWj4ExmoqLbQVbH+yZXs2GuwSHQrHHBaieifB57hpz1PyhZyUG7ff2UP7VoGeq21K29GGlz/v9Z8pro48sEvlSN3G/gCItPNY3NI3twAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C631220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArElEQVR4nMWRsQ3CMBBFHzZU7JAUKBNQkxpmYRUKOhaghZKOBbIBRRqEyBBfJ1GQECc4QqKAK+yTn/7d/zL8uaZYkQ7B7JN6/c1K1cf8HZ2wYV1pIMifA7pVNcJIlkyhQDMAxsFTcq0b8z1p4KaAfYdtIjbCPW1nJHG2FSoXXXZ5uQ3mu/oeTdyKm4G51lsTZSeP8HIe+kFI7xiS4Bwxm2MIbOA/l0cd4uSX9QATMTTKJhxnxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6313A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits shape (10000,)\n",
      "images shape (10000, 28, 28)\n",
      "tuples shape (10000, 784, 3)\n",
      "offsets shape (10000, 2)\n",
      "target_pxs shape (10000,)\n",
      "target_idxs shape (10000,)\n",
      "masks shape (10000, 784)\n",
      "index 0 which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C5CA6D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAL0lEQVR4nGNgwAn+45YiGZBkFimKqelIsgHRjviPRtMQEGMF5c4Y0OCnJDTJ0QMAn6gM9JnMmeoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C61B4F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAHElEQVR4nGNgGAX0A2ID7QBUUDfQDqAO+EQnewAhAQGHd9Kh9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDB80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1 which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C61B4F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAKUlEQVR4nGNgIAH8J0UxBWZQw57BYyARaqnuPlqB/yjUAIP/WFi0tAsAuXIL9QBgyjYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDC70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGUlEQVR4nGNgGAUDD/4OtANIBCID7YABAQC3gQES47jhPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDC70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C5CAC40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAIElEQVR4nGNgoBj8HxRGjCzwn5pBhs2oYRYjOLxDmi8BHqsH+UxHWqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C5F2EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAF0lEQVR4nGNgGAzg30A7YBSggdEYwQQAteUB/eaVSqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C019AB0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 3 which is a tf.Tensor(0, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA70lEQVR4nMXQsWoCQRQF0JvBNNrGdjcgIR8gJJVrqRZCfkMkgn+QlEIIAcHaHzClVWySJkmXSgxqIbKCbcrLxRSbNe7M2uqr7syZefAecOTK9fTp78MLUs2ds9nJ+b71OPMfWzdXAALz9ZrSVCQpclp0bbiRpPVckmPlmUh268Ed2bDsfEVx2skCfsif9qkzxcsZAOCWYsHGDy+K/nuM2zmNuV5E6cQYc5/4+UDG0W07iTFfXlGhl45PJGelKGeQrOElgPFb8vJbqtWW0kYpG2qT8W7ZtdEP/zAcFbI2IniMsOkIAKD6zEGl6qXjweoXXfV/5XmKZEMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C61B4F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFUlEQVR4nGNgGAWDGfynuYZRQCwAAOdjAf9hLkpzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C019AB0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFUlEQVR4nGNgGAWDGfwdaAeMAjgAAIqXAP4wHYCUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D207BDB80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataset_summary(d):\n",
    "    for name, v in d.items():\n",
    "        print(name, \"shape\", v.shape)\n",
    "\n",
    "    for i in range(4):\n",
    "        print(\"index\", i, \"which is a\", d[\"digits\"][i])\n",
    "        display_uint8_image(d[\"images\"][i])\n",
    "        display_mask(d[\"masks\"][i])\n",
    "        display_uint8_image(tf.constant(d[\"images\"][i]) * tf.cast(mask_to_image_mask(d[\"masks\"][np.newaxis, i])[0], tf.uint8))\n",
    "\n",
    "dataset_summary(train_dataset)    \n",
    "dataset_summary(test_dataset)\n",
    "dataset_summary(full_masks_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f_masks = np.array([29, 31, 754, 752])\n",
    "# print(f_masks)\n",
    "# f_masks = idxs_to_multihot(f_masks)\n",
    "# print(f_masks)\n",
    "# display_mask(f_masks)\n",
    "\n",
    "# t_masks = np.indices([784])\n",
    "# print(t_masks)\n",
    "# t_masks = idxs_to_multihot(t_masks)\n",
    "# print(t_masks)\n",
    "# display_mask(t_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNNZdCnizvLU",
    "outputId": "8472bdb9-66be-4f7a-8b8a-66ac282bb160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784, 3) (10000, 2) (10000,)\n",
      "tf.Tensor(\n",
      "[[ 11.   1.   0.]\n",
      " [ -5. -19.   0.]\n",
      " [ -8. -17.   0.]\n",
      " ...\n",
      " [  1. -26.   0.]\n",
      " [  2. -25.   0.]\n",
      " [  5. -14.   0.]], shape=(784, 3), dtype=float32)\n",
      "tf.Tensor([10 26], shape=(2,), dtype=int64)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_dataset[\"tuples\"].shape, test_dataset[\"offsets\"].shape, test_dataset[\"target_pxs\"].shape)\n",
    "print(test_dataset[\"tuples\"][0])\n",
    "print(test_dataset[\"offsets\"][0])\n",
    "print(test_dataset[\"target_pxs\"][0])\n",
    "\n",
    "# actual training dataset is 784 x,y,v tuples, but with the spatial location changed so the the pixel to predict is in the center\n",
    "# and the pixel value to predict removed (via masking)\n",
    "\n",
    "# then, can mask more and more stuff and compare performance\n",
    "\n",
    "# we can also use the model in an \"iterative\" mode to generate images, in an arbitrary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "W3nKMxnPe6cc"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5. 3.]\n",
      " [1. 4.]\n",
      " [1. 6.]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.11920292 0.         0.880797  ]\n",
      " [0.18242551 0.         0.81757444]\n",
      " [0.5        0.         0.5       ]], shape=(3, 3), dtype=float32)\n",
      "(1, 3, 2)\n",
      "(3, 3, 1)\n",
      "(3, 3, 2)\n",
      "tf.Tensor(\n",
      "[[[0.5960146  0.35760877]\n",
      "  [0.         0.        ]\n",
      "  [0.880797   5.2847824 ]]\n",
      "\n",
      " [[0.91212755 0.54727656]\n",
      "  [0.         0.        ]\n",
      "  [0.81757444 4.9054465 ]]\n",
      "\n",
      " [[2.5        1.5       ]\n",
      "  [0.         0.        ]\n",
      "  [0.5        3.        ]]], shape=(3, 3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.5960146  0.35760877]\n",
      "  [0.         0.        ]\n",
      "  [0.880797   5.2847824 ]]\n",
      "\n",
      " [[0.91212755 0.54727656]\n",
      "  [0.         0.        ]\n",
      "  [0.81757444 4.9054465 ]]\n",
      "\n",
      " [[2.5        1.5       ]\n",
      "  [0.         0.        ]\n",
      "  [0.5        3.        ]]], shape=(3, 3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.4768116 5.642391 ]\n",
      " [1.729702  5.452723 ]\n",
      " [3.        4.5      ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# attention manual calculation example\n",
    "\n",
    "m = tf.constant([[5, 3], [1, 2]], dtype=tf.float32)\n",
    "val = tf.constant([[5, 3], [1, 4], [1, 6]], dtype=tf.float32)\n",
    "print(val)\n",
    "mask = tf.constant([1, 0, 1], dtype=tf.float32)\n",
    "wei = tf.constant([[1, 2, 3], [0.5, 2, 2], [1, 3, 1]], dtype=tf.float32)\n",
    "wei = keras.layers.Softmax()(wei, mask)\n",
    "print(wei)\n",
    "val = tf.expand_dims(val, -3)\n",
    "print(val.shape)\n",
    "wei = tf.expand_dims(wei, -1)\n",
    "print(wei.shape)\n",
    "x = val * wei\n",
    "print(x.shape)\n",
    "print(x)\n",
    "mask = tf.expand_dims(mask, -1)\n",
    "# x = x * mask\n",
    "print(x)\n",
    "x = tf.reduce_sum(x, axis=-2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = layers.Dropout(dropout_rate)\n",
    "    dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        attn_output = attn(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = dropout1(attn_output)\n",
    "        if include_residual:\n",
    "            attn_output = inputs + attn_output\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "        attn_output = attn_output * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        attn_output = layernorm1(attn_output)\n",
    "        dense_output = dense_net_1(attn_output)\n",
    "        dense_output = dense_net_2(dense_output)\n",
    "        dense_output = dropout2(dense_output)\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 20\n",
    "\n",
    "    data_input = keras.Input(shape=[784, 3], batch_size=None)\n",
    "    mask_input = keras.Input(shape=[784], batch_size=None, dtype=tf.bool)\n",
    "    m = data_input\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    m = layers.Dense(200, activation='relu')(m)\n",
    "    m = layers.Dense(n_embd, activation='relu')(m)\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=12, n_dense_dim=200, dropout_rate=0.1)(m, masks=mask_input, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=12, n_dense_dim=200, dropout_rate=0.1)(m, masks=mask_input, include_residual=True)\n",
    "    \n",
    "    m = layers.Reshape((28, 28, n_embd))(m)\n",
    "    m = layers.Dense(1, activation=None)(m)\n",
    "\n",
    "    model = keras.Model(inputs=[data_input, mask_input], outputs=[m])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 784, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 784, 200)     800         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (Sli (None, 784, 1)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (Sli (None, 1, 784)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 784, 20)      4020        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_2 (TFOpLamb (None, 784, 784)     0           tf.__operators__.getitem_4[0][0] \n",
      "                                                                 tf.__operators__.getitem_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 784, 20)      19940       dense_8[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 tf.math.logical_and_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 784, 20)      0           multi_head_attention_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_2 (TFOpLambda)          (None, 784)          0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 784, 20)      0           dense_8[0][0]                    \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 784, 1)       0           tf.cast_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 784, 20)      0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 tf.expand_dims_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 784, 20)      40          tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 784, 200)     4200        layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 784, 20)      4020        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 784, 20)      0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 784, 20)      0           layer_normalization_4[0][0]      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (Sli (None, 784, 1)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (Sli (None, 1, 784)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 784, 20)      40          tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_3 (TFOpLamb (None, 784, 784)     0           tf.__operators__.getitem_6[0][0] \n",
      "                                                                 tf.__operators__.getitem_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 784, 20)      19940       layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "                                                                 tf.math.logical_and_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 784, 20)      0           multi_head_attention_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_3 (TFOpLambda)          (None, 784)          0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 784, 20)      0           layer_normalization_5[0][0]      \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 784, 1)       0           tf.cast_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 784, 20)      0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 784, 20)      40          tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 784, 200)     4200        layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 784, 20)      4020        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 784, 20)      0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 784, 20)      0           layer_normalization_6[0][0]      \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 784, 20)      40          tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 20)   0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 28, 28, 1)    21          reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 61,321\n",
      "Trainable params: 61,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss='mse', metrics=['MeanAbsoluteError', lr_metric])\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    txformer.fit(x=[dataset[\"tuples\"], dataset[\"masks\"]], y=dataset[\"images\"], epochs=1, batch_size=batch_size, callbacks=[WandbCallback()])\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, idx):\n",
    "    img = np.zeros([28, 28])\n",
    "    mask = dataset[\"masks\"][idx:idx+1] # slice to keep batch dim\n",
    "    erow, ecol = dataset[\"offsets\"][idx]\n",
    "\n",
    "    image_mask = np.array(mask_to_image_mask(mask[np.newaxis, :]), np.uint8)[0]\n",
    "    masked_image = np.copy(dataset[\"images\"][idx])\n",
    "    \n",
    "    print(\"MNIST idx\", idx, \"which is a\", dataset['digits'][idx])\n",
    "    \n",
    "    display_uint8_image(masked_image) # before mask\n",
    "    display_uint8_image(image_mask * 255) # mask\n",
    "    masked_image = masked_image * image_mask\n",
    "    display_uint8_image(masked_image) # after mask\n",
    "    \n",
    "    img_tups = dataset[\"tuples\"][idx:idx+1] # slice to keep batch dim\n",
    "    img = txformer([img_tups, mask])\n",
    "    img = tf.reshape(img, [28, 28])\n",
    "    display_float32_image(img)\n",
    "\n",
    "def image_performance_test():\n",
    "    gen_image(test_dataset, 0)\n",
    "    gen_image(test_dataset, 1)\n",
    "    gen_image(test_dataset, 2)\n",
    "    \n",
    "    # full masks dataset as sanity test\n",
    "    gen_image(full_masks_dataset, 0)\n",
    "    gen_image(full_masks_dataset, 1)\n",
    "    gen_image(full_masks_dataset, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 0 which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C304340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqklEQVR4nGWSWQ4DIQxDnar3v/LrR1YXNIIJCbZjENrBDSSLQNQeVkp/HWbKsPJsL8/+pYqKw4UElTIuDsoiMTI7L30lInGJZmqgryRCUWeSxwXQDlR/JPeUong0z8orrsTPL+cM+sTmQoqiS/qFpsG5LZdYBNg19QSX/Jo3CjFXr+d/Hk9h2+R3bPSrsv285o62d7j5T0m2yV8nS7+idm637F3aLe1e0fMDc9DBXCZMphUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C2F9700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiUlEQVR4nGNgGH5gPgODCg6pe/8YGP4xMDAwHGNgYGBYZYIuX7SY4d9Dhn8MUFXoQJCBwZnB2ZqBgSGLLJeJMTAE45F+h1PG+uc/O5ySrQy7WXHJcRLlLmzgN4MlLilhBoZluOSYsYccDOCWlMfrmn9oGpmQ2J8YGExwSvIxMX3Bbex5Iby2ogIA1cQZm/BGhbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C304460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbklEQVR4nI1SywLAIAgS/v+jd9gqX9g8JZISaTQZEFiHlwh9Ie5sNKtg5QMtM+v5Jc5NbQYpOnzVsVghMwzGHjZDNpC5jc9ewoIpX5EvzIhjndnIz9puUUnVT5E1/aeR7oliJ+VuLEZ37/ZDSYE9i4QAV/DzDn0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C304460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 1 which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C2F91C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlklEQVR4nHWSQRYDIQhDY+9/599FlQSxsxgQQl6CShKSJH7BMf6OdaKqjPbEUyAIDK3zjwNSByEJDzlhtAHn6oFi5QzXqXym4a2ZGsdFNUndLrLV5pEGjcugKZ5TZ58Wu1NOl2tJhXoUVhQW0hJL7Xs5tH5UazOx9DmldbF13HhfD9Bd3f5JFd5cTMTT7jwPxngY11XqC5EUo2X60PwRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C304340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAb0lEQVR4nGNgoAf4Py0Wq/hKhr9//zIwMMhhlf2L31AeBoZzDD2Y4r4QqoqBwRGLLnnCZn9nOMaFQ8qHgeE5flfhBH8ZeMjViA/gNPQEebYxMPz4i99GbQYRBGcaqk5kx2KagiTyHU2qjGjnDRwAAMP+GixKciVeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C304340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAcElEQVR4nJWSSxLAIAhD3+v9D91FxwIO2sqGjxMNiQACkUa46U7DwGua1n6J3lBZo03HusAlYlwtXErx5Kgd9IJkfSePe7Ltdr/11gzJQkdxZt4r1gzrNqxu/rSmueLrt3V8gu0M1dmnkfZSLDcG4AaHHABUShMzzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C2F9E20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 2 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C2F9520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuklEQVR4nG2SSRYEMQhCP76+/5XpRZwqVe4SFQGVQeYKGRkCqJy08gZCoGcXwoAgGlXd4kQmXKhm1ZzmuCAT8VRGw4nKdETDVcIa+nj+uQSHu2bqCzqu+kPpiCm2BrDBQs1Ow0eLysBurg/Vs5Fy0YmD04Qy4ExCyWlMqKIjSrgBviOOAs2il9c1/ysUkMpvIeBIpmWtNj29VrEJ2XMaCCH1HS4uep1w3VDOeFwZv/WarnQmjgZ766+PP3l4TzrhxXknAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C3042B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhklEQVR4nGNgGAzgXy6cyYQuF/7vKZzNgi5pgCGCALpfpiE46Maqc63ErfPUfW6ccgoMN3BrjP+HzEOzU5fBF7fOt2c5cOqsEbrxA6ek/v81OA2VYLiOwkfRmfDvBG5JeYb3OE1lePLPCadOW3Hc+hh6/51lxqWTy4thzV+cOo9twGMsvQAA1vwdZ3JPR08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C3042B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0klEQVR4nF2SWw7EIAwDx3P/M8N+hEC6VaUUEowfRUEI5wkRlADJaeWNiFUwb8OzSKrdYFbpRcFixDHXoOPga+XC9n3nuw4FCivC3gDZgOtij9rA84LWCCGFIwRT4H/cSukDsHklings9XIdw3mlcXPIRcuiuz0sErxMHyXRITAGEkp6Ni4ErhXNyXajcNM3Zcx9zDuMISR2sFYsSfleBi6E5XkDLtkBluug7I0r+2mJPnrT8t4eIWR6YHcyIv2crEwO5fptetYO6cnO09VJSlT4AaT2F2Ol9ILUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C06A23790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 0 which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01949760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAL0lEQVR4nGNgwAn+45YiGZBkFimKqelIsgHRjviPRtMQEGMF5c4Y0OCnJDTJ0QMAn6gM9JnMmeoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01949760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAHElEQVR4nGNgGAX0A2ID7QBUUDfQDqAO+EQnewAhAQGHd9Kh9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01949760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAIklEQVR4nGNgwAkYcUsNHjAkHAkHjGj0kAcD6hFKLCdHLwANrgAI3geAYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C06256BE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 1 which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C609AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAKUlEQVR4nGNgIAH8J0UxBWZQw57BYyARaqnuPlqB/yjUAIP/WFi0tAsAuXIL9QBgyjYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C609AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGUlEQVR4nGNgGAUDD/4OtANIBCID7YABAQC3gQES47jhPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C609AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAH0lEQVR4nGNgoDNgpLeFNLF0QHwxCAAjCjXAgN6uAAAMhQAH75GXJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0C01949760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 2 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6031C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAIElEQVR4nGNgoBj8HxRGjCzwn5pBhs2oYRYjOLxDmi8BHqsH+UxHWqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6031C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAF0lEQVR4nGNgGAzg30A7YBSggdEYwQQAteUB/eaVSqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C6031C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAF0lEQVR4nGNgoBgwDgojRsEoIBmQlu4ACOkABOKISf8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D20784550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/3750 [..............................] - ETA: 4:58 - loss: 7448.3726 - mean_absolute_error: 38.9690 - lr: 0.0100WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0732s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0732s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 288s 77ms/step - loss: 6177.4385 - mean_absolute_error: 54.4570 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 0 which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C3043A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqklEQVR4nGWSWQ4DIQxDnar3v/LrR1YXNIIJCbZjENrBDSSLQNQeVkp/HWbKsPJsL8/+pYqKw4UElTIuDsoiMTI7L30lInGJZmqgryRCUWeSxwXQDlR/JPeUong0z8orrsTPL+cM+sTmQoqiS/qFpsG5LZdYBNg19QSX/Jo3CjFXr+d/Hk9h2+R3bPSrsv285o62d7j5T0m2yV8nS7+idm637F3aLe1e0fMDc9DBXCZMphUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C3043A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiUlEQVR4nGNgGH5gPgODCg6pe/8YGP4xMDAwHGNgYGBYZYIuX7SY4d9Dhn8MUFXoQJCBwZnB2ZqBgSGLLJeJMTAE45F+h1PG+uc/O5ySrQy7WXHJcRLlLmzgN4MlLilhBoZluOSYsYccDOCWlMfrmn9oGpmQ2J8YGExwSvIxMX3Bbex5Iby2ogIA1cQZm/BGhbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C3043A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5ElEQVR4nFWSUZIFIQgDO5QH2ftfsvcD0HlTY1mKJCGQv3A/+RzA+o0JvHU2kPmZbAmF0jeBiP3UBIpkQYWQ3WBgJXsBcRbV5CtLobESrCeg8WaLkAOmcV3YFjWchkxqpqwbjOtAMJIswQGyT81IHuq65nXdq0ySlLeqOD7YdkkNwugI6qZbpqtWuEqaNzWmClExpBHavmYLZsUyOqbZiYnenrX/9e1mxgmCEm+dYvvTkhJMvaFYT70Sz3Xa15FM/mEPLd9gj2F+vMUdwh2ZjyCSsKLooK9ou+3DKzUyx6I7Bwr+A+H4iXXlra4+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C2F9460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 1 which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C4F32E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlklEQVR4nHWSQRYDIQhDY+9/599FlQSxsxgQQl6CShKSJH7BMf6OdaKqjPbEUyAIDK3zjwNSByEJDzlhtAHn6oFi5QzXqXym4a2ZGsdFNUndLrLV5pEGjcugKZ5TZ58Wu1NOl2tJhXoUVhQW0hJL7Xs5tH5UazOx9DmldbF13HhfD9Bd3f5JFd5cTMTT7jwPxngY11XqC5EUo2X60PwRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C4F32E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAb0lEQVR4nGNgoAf4Py0Wq/hKhr9//zIwMMhhlf2L31AeBoZzDD2Y4r4QqoqBwRGLLnnCZn9nOMaFQ8qHgeE5flfhBH8ZeMjViA/gNPQEebYxMPz4i99GbQYRBGcaqk5kx2KagiTyHU2qjGjnDRwAAMP+GixKciVeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C4F32E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nFWSSY4gMQgEI1B/ZP7/yJxDgqvbUi02hlzAfxABgvz5ylxsj8AXy4C5hPAtwcnmCEgS6KWEn60WY+9F7IufV8XDzoMfQrLxQIgWJAyiG7fMs5IyIYDmq+kWcHoHLE/THUBGNBKU0N+zafIwkjLx7PJ0WoLFXFFM3dx80aSMgHl+3YnPTaYiSqNUtw+QCRaqaeeCgLNWZ0l6jQ013lrTZxtSnds61PxKJJA5asmvUalDzs2GL/Ct2eZ/U3fdOcyT/ebnTLh9cloXNbXPZ9s3ncEwp2mrHi/FSunVmD9jjf8BPWGAbciKwpgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C4F32E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 2 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C36CFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuklEQVR4nG2SSRYEMQhCP76+/5XpRZwqVe4SFQGVQeYKGRkCqJy08gZCoGcXwoAgGlXd4kQmXKhm1ZzmuCAT8VRGw4nKdETDVcIa+nj+uQSHu2bqCzqu+kPpiCm2BrDBQs1Ow0eLysBurg/Vs5Fy0YmD04Qy4ExCyWlMqKIjSrgBviOOAs2il9c1/ysUkMpvIeBIpmWtNj29VrEJ2XMaCCH1HS4uep1w3VDOeFwZv/WarnQmjgZ766+PP3l4TzrhxXknAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C36CFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhklEQVR4nGNgGAzgXy6cyYQuF/7vKZzNgi5pgCGCALpfpiE46Maqc63ErfPUfW6ccgoMN3BrjP+HzEOzU5fBF7fOt2c5cOqsEbrxA6ek/v81OA2VYLiOwkfRmfDvBG5JeYb3OE1lePLPCadOW3Hc+hh6/51lxqWTy4thzV+cOo9twGMsvQAA1vwdZ3JPR08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C4F32E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAx0lEQVR4nF2SSRLEMAgDW5QfMv//pObAYpwckkoA0VKsH8h8LhkZAuiatOoGQqB3CmFAEKOqGXEpE25Vs3pyOD6SpZidMXKiK3PFyHXBGvxLq9Lkuo5G9bU70vG0NlKaadrUM1ho6GImrFrrEYsP6+P6jGyn6LwJO9aKahMqphtCN+VWYTigxHz9u4Bs0P3RK+uTECsHz1MB5fxrBIrWE6023vkM7GPhsO/RQGSGhXceB9aOdgUPmjNQu8562xEIQ6QHe/vvD38FjVdmRHbxjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C5854C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 0 which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C585460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAL0lEQVR4nGNgwAn+45YiGZBkFimKqelIsgHRjviPRtMQEGMF5c4Y0OCnJDTJ0QMAn6gM9JnMmeoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C585460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAHElEQVR4nGNgGAX0A2ID7QBUUDfQDqAO+EQnewAhAQGHd9Kh9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B1C2F9460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAXElEQVR4nK2RSQ7AMAgDbX6S/z+S3iLC4qZSfR3LGgEXpjhtZCAERIGuIBN0TLFUPrpViAoKoa9wELYssYuubPmLbee0IZvO21fauIJUUM7ewXDNCsM1u9nrl4U8d1kNWgzm96MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C4F32E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 1 which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D29895220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAKUlEQVR4nGNgIAH8J0UxBWZQw57BYyARaqnuPlqB/yjUAIP/WFi0tAsAuXIL9QBgyjYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D29895220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGUlEQVR4nGNgGAUDD/4OtANIBCID7YABAQC3gQES47jhPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D29895220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAXElEQVR4nLXPwQ3AMAgDQMwm2X9I99FHYqegRmrzQdEhMBhRv7Q/O0SHMqNG3MjK0/e82xnsEB0WY9khFJ+vybXXW2SsH3ye9jukFENI+T/QTMEdZwrseL6TEXEBjGEMWH6XJXoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C59FDC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST idx 2 which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C5854C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAIElEQVR4nGNgoBj8HxRGjCzwn5pBhs2oYRYjOLxDmi8BHqsH+UxHWqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C5854C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAF0lEQVR4nGNgGAzg30A7YBSggdEYwQQAteUB/eaVSqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0D29895220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASUlEQVR4nGNUZMANmPDIIST/45NkJMrY//gkGfFJ4jV2+Er+xxrqUElGrKGO29j/WCThxjNikUQzflCGECrAkcAgwjgSGEYgAgDqRAhZ87HW7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B3C59F970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/3750 [..............................] - ETA: 4:54 - loss: 6429.0796 - mean_absolute_error: 55.3272 - lr: 0.0100WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0706s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0706s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345/3750 [=========>....................] - ETA: 3:04 - loss: 6175.4961 - mean_absolute_error: 54.6272 - lr: 0.0100"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(train_dataset)\n",
    "fit_one_epoch(train_dataset)\n",
    "fit_one_epoch(train_dataset)\n",
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
