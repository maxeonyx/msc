{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxeonyx\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">polished-night-35</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist/runs/2dvvxr5n\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist/runs/2dvvxr5n</a><br/>\n",
       "                Run data is saved locally in <code>/am/monterey/home1/clarkemaxw/conditional-mnist/wandb/run-20210930_165119-2dvvxr5n</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 16:51:25.481564: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 16:51:26.032921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "tf.constant([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00]\n",
      " [ 7.0710683e-01  1.3921213e-01  2.4833918e-02  4.4166050e-03\n",
      "   7.0710677e-01  9.9026257e-01  9.9969161e-01  9.9999022e-01]\n",
      " [ 1.0000000e+00  2.7571312e-01  4.9652517e-02  8.8331243e-03\n",
      "  -4.3711388e-08  9.6123999e-01  9.9876654e-01  9.9996096e-01]\n",
      " [ 7.0710683e-01  4.0684462e-01  7.4440487e-02  1.3249470e-02\n",
      "  -7.0710677e-01  9.1349739e-01  9.9722546e-01  9.9991220e-01]\n",
      " [-8.7422777e-08  5.3005296e-01  9.9182546e-02  1.7665559e-02\n",
      "  -1.0000000e+00  8.4796453e-01  9.9506927e-01  9.9984396e-01]], shape=(5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(mask):\n",
    "    image_mask = tf.reshape(mask, [28, 28])\n",
    "    return image_mask\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=1000):\n",
    "\n",
    "    i = tf.range(dims//2, dtype=tf.float32)\n",
    "    i = tf.expand_dims(i, -2)\n",
    "    \n",
    "    vals = tf.expand_dims(vals, -1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = vals / tf.pow(scale, 2.*i/dims)\n",
    "    \n",
    "    sin = tf.sin(rate)\n",
    "    cos = tf.cos(rate)\n",
    "    \n",
    "#     # expand dims to allow alternating concat\n",
    "#     sin = tf.expand_dims(sin, -1)\n",
    "#     cos = tf.expand_dims(cos, -1)\n",
    "    \n",
    "    encoding = tf.concat([sin, cos], axis=-1)\n",
    "    \n",
    "#     encoding = tf.reshape(encoding, [-1, dims])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(positional_encoding(tf.constant([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]), 8))\n",
    "\n",
    "def img_to_tuples(img):\n",
    "    \n",
    "    height, width, chan = img.shape\n",
    "    length = height * width\n",
    "    vals = tf.reshape(img, [length])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1])\n",
    "    cols = tf.reshape(cols, [-1])\n",
    "    \n",
    "    # permute the order, to ensure the network uses the positional encoding and not the implicit locaiton\n",
    "    idxs = tf.range(length)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    rows = tf.gather(rows, idxs)\n",
    "    cols = tf.gather(cols, idxs)\n",
    "    vals = tf.gather(vals, idxs)\n",
    "    \n",
    "    return vals, rows, cols\n",
    "\n",
    "def random_mask(n_masked_out=None):\n",
    "    \n",
    "    def call():\n",
    "        idxs = tf.range(784)\n",
    "        idxs = tf.random.shuffle(idxs)\n",
    "        if n_masked_out is None:\n",
    "            n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "        else:\n",
    "            n = n_masked_out\n",
    "        idxs = idxs[:n]\n",
    "        return idxs_to_multihot(idxs)\n",
    "    \n",
    "    return call\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask), np.uint8)\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "positions = tf.range(28, dtype=tf.float32)\n",
    "encodings = positional_encoding(positions, 16, scale=28)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "im = ax.imshow(encodings)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def make_dataset_generator(split='train'):\n",
    "    \n",
    "    def split_enumerate(i, data):\n",
    "        data[\"index\"] = i\n",
    "        return data\n",
    "    \n",
    "    def map_add(component, func):\n",
    "        def do(data):\n",
    "            data[component] = func()\n",
    "            return data\n",
    "        return do\n",
    "    \n",
    "    def add_tuples(data):\n",
    "        data['vals'], data['rows'], data['cols'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    \n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask()\n",
    "        mask = tf.logical_and(mask, square_mask)\n",
    "        data['mask'] = mask\n",
    "        return data\n",
    "    \n",
    "    dataset = tfds.load('mnist', split=split, shuffle_files=True)\n",
    "    \n",
    "    # keep track of the index in the original MNIST\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.map(split_enumerate)\n",
    "    \n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(60000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.map(add_tuples)\n",
    "    \n",
    "    dataset = dataset.map(map_add('mask', random_mask(500)))\n",
    "    dataset = dataset.map(add_square_mask)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset_train = make_dataset_generator()\n",
    "dataset_test = make_dataset_generator(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 16:51:28.229811: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape (28, 28, 1)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'int64'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "vals shape (784,)\n",
      "vals dtype <dtype: 'float32'>\n",
      "rows shape (784,)\n",
      "rows dtype <dtype: 'float32'>\n",
      "cols shape (784,)\n",
      "cols dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "index tf.Tensor(58118, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgGKrAsOvz/1k45DiP/vnz981CI6ySTX8eN+378+exARa52W8eGzGwBv/504kpZ/r3TxMDA4PU3T+f5mFI1v9/osDAwMCw8+/f5xiSz/62MDAwMDDk//nzDCbGhJC+wcDAwMCgxcDwEIskAwMDA4OgBQNDM4axdX+PMDAwMCT/+fMHi1f+/72XKBF+5e/ffVgk5//58+fP3z/I/mSBszLeaTHYP3xrgUUfBGiJ6iPrRHXttdcoXBYGNMCIT/I/Ehs9EFAASZLXehnO4FNPJAAApcZLLhcM/5oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7BC5F87BE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nL2SQQ7AIAgEp/3/n6cHBI2JtvFQghph0UUXRAUBFAHbOkQS0Dw3S3Od3ZV9yf9tm1bYsr3Pb3zJnz/R+lfu01auoNQlkSIapEKRFg1wjiRm090YYwrkGb3SCWqysSiImQupxzQxKHwFGzQceQAb3nKVzNY/CQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EE80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAALklEQVR4nGNgGAWjgLpAn+EPgsOEIc3YiVvyDzNunRdZGHBL6uN1UecfvNJEAgDpNgVGioAKPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E6A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(10963, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4nMWPLYsCYRSFD8Pg1xjUoMFgFYRpgkUM4oJFRBCTf8FiMxkEm2G3iH/BsHWLWkVk8xS7H2hYcGDhXDQIOoPvW/W0e557DvcC75bVm/ctDSseSfkrK5m9Jyl07obhgekEDu2xprVKdmF+77IamALsiSb5G/UZhm+yb398VJS1AwCxmXvOAQBMD9zskxYCP/kwNqfn6EhELiLitBQXNUkK/78yClaakeS6oUCR4Y4k6woE1EiS9FqPPwvA1vXvP+AUCK2UnQCCi6daj+KfJJe6rNk5MK6Dr9IVLvtWiuvo7ZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EF10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAsElEQVR4nHWSWxKEMAgEG+9/596PQADLTZUor4GZCAKgKKKSIQiMk4UQAvPJIN2dEAmDgLekQifyAIaBlTiwt+0AFsTB7HNyaVFlpme3PaJZ9VCv7wXyDTKp5CJpKSdlG7v3q0Fc6eMLOjWfE69m0g4bdbNL+Xv10aLMMnxtGAZggJERDIlscVOVj1sps4Dn+FanPi39x22sxu8T/1NXs0061+dJokA4CmJrWz/y2PwHrenwKeUnttQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EF10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAk0lEQVR4nM3POwrCQBSF4Z8wnVUqcQNphEDWEBDS2IXsxcpCcFMhKwjWLkDsbAS7M5BiwAS8t7HRAwOX+TjzgJ/nMBxXnj0Qz9qkEkTk6jQFaMZsQQ1ACLmJEECsnVMv3ksX2Rl7DZzS9Np+YHF/jxujG2NanWEtSn81I8Foyhkhp7cHyUOkm3cdlaB3m6n+hfxJJlsuKxrIWmVIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EB20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(56888, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4nGNgGNSA02/S6v//AmFcFiQpKcsSMwaGf9h0cbW9/Pv394FihVoudCmO2s9///7e7YhNm+rcv3//zrLB7pLyv38PurCjizJCqHvy5+y/MUhIMjA8eI+QhLv2r69ouKwsA8OBWZeuoxlw7+/fv3///nn79u3Hv5+a0Zx7/O/fszNLwxgYGMRafv9dw4gi6bkwkA3O/vtXB7uzGRgYmDb9vQZlYUr++83AjlMSyQxMIWZOhi841Tf+/RuGIeggwcDAwOC6/8/fFYwYknM3KCqWH/nz99dBUUzjGv7+/fv379vtHtjs4qg89mxilCxOt1ANAAAJLln4QP74eQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E6A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAfUlEQVR4nLWQQQ4FIQhDW+5/5zeL4qCbv/BnJEENtKVId4dL3B+8SPWR6hea/EDe09IZwb4skJAwctqcsvvTXUDTBMFCd4SV0QqgjEMmPLWIFG4FnMEUFfslYwYYNLVY3jtV8Gn4eCJUm/9pAUnedsPuVWyhcT8EazZWOioP451ruP29rA8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E6A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nGNgGAVDE3guZGBD8P7q4Fa56e812jjhIk6Zxr8MYTikXBkYGFZgk1BUZDjy5y8DgyguQ9/iEK9kYGBgkMXpFqoBALkoDMhd9pebAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EDC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(26712, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0klEQVR4nGNgGKaAS3/Z/3//FuZjk2Na+ffv3QULzr2bhCnHefnvDh52Bga+Cz9NMCS7fznzMTAwMHCe/SaJLufy/R0DAwMDg/DV7yeg1iAkTdgOMDAwMARd1WB7DBFhQUgq/fatYBAIl/37om0ZpoPqDvz9+/fvUltsHmFgYFn9902uMHY5Xsu/L+uxSzEw2P/964FLzufX3wZGHHJ2F19U4NKncfN9HS45hv1/03HK+fzZzIxTcuYbjLCGA4UvG3DKMaT/FcEpp/60nwWnJA4AAFO1QncgAlhbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAq0lEQVR4nGWSQRIEIQgDm/3/n3sPgoCjNVoTYoggkkMQBa3/82kzSJKQxATrkAcJQELi7CEHtdbSvvmvhSEzQlJZeY489vdIwF8sdqs28QnWFQOJN5R34OfXUABBxLU9gzVL/01pE5+QV+5TnKlVcXH1KSEm01nVqkhCVnu959tW27vtt29G09mrc3dyrWd1XuDqSyGd261hvQ/TybK8HE+/sw2leT3M6maePxOT1j28iCeXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAY0lEQVR4nGNgGN4gH5/ku0l4JC/gkePEIyeMRy6I4S/DatzST3FLLcVjKi7Aa8nwEo/0X9xSvxgYGMmwkOEmQx052hgYGDYz45Sa+Qa3NoUv+AzF6kMmBgYGBgb1p/jiHjsAAFD+EMrZuyA7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EDC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(2452, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4nGNgGDBgcXGTAk5Jm9//HtUZ4ZLd/e/D/y8lKgxy7Kqq3OiS2f+auy/9+7vn44l//6aiS6r+e6XBVfzkyZ7HT26Eo0tKv/7nh9u9Xf/WCgjYJgsIYNjIwMAg+/EfBDyYpYMp++zfnwcTJ+78+fvfpwImdEnu1Y4MDAwMDMoL/v1bIIvLdvaQr/96cDvO4eMPMwYGBgYM0xkYGBgO7GbTwCkJAzBJHn923JI+6z0RgsKaDD+QFdm+/ZaiAmWLnvq3FtUI24//3k0r5OaWKW549m+jMpoFBkt/QAPwX648RIgRSVrTS5lB+yrD56XXf+PzA20BAP7EYYEg40InAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqElEQVR4nH2SVxIDIQxDpUzuf+WXD9zYhTAMxUWWiyRJAoTiHZsU9CIMaYE0zSgB1I9AYsEGkFBpBtoTvQxT3iehCsYiQ72Ij8DvpKTPySeXl8oX9SS1KzBGF1/MH0gjWazrHPLIl0jFt3QgdvQmD3aLrBcty4tp+3Asdt2zlmxtJcdr62oPSHsSlBM3vLosVSMk4yRTNHo01vM7aoqMZeGaNQbAmCshfsawu2K5I+buAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429E0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nGNgGDBgwcCggF+FEW6pDwwMDCoMDOwMDNyYks249an+wyLIBKW/vcXvnuEJ1qAGCQ8DO6YaWAj5MHgiiV7FbagoA8NaNKGPMEYDA8NGZTRJAwYGBoZ//xgY/jHkYjOvmGEaAwNDJwMrbitpDgClqhCa1AXMlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB429EF10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape (28, 28, 1)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'int64'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "vals shape (784,)\n",
      "vals dtype <dtype: 'float32'>\n",
      "rows shape (784,)\n",
      "rows dtype <dtype: 'float32'>\n",
      "cols shape (784,)\n",
      "cols dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "index tf.Tensor(7146, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nGNgGEqAEYXHysfw6TcOlayd//41MDAwMHDzYchpr//376gbAwMDQ/YcJnS5+/++1YkzMDAwMGT/i0aVy7j/78ESKDvy9xxkx7DUPPh3QgrOfftPDiHH7P7v3wlJBiTJJgYGBqjFadsZXgU+R0g+ZGBASCow/KpHkmNYzKAIZ2s9/ZmG4rpL/27B2S/+vUGSkWs69xvJLzPgks61V648+XdreR4jAyxsxfZqTj3LwBCqziDDwfDg8KTHrxgY4JIM4t0xDAwMDK/+nH9+awHcClhAMIkJyF1i+P3/yy+GYQkAZ99O0CePfnUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtklEQVR4nH2TUQ7DIAxDn3v/O799QKjZpCGE1ODYcUIBEEDlXq6guDHiPl6IO9G6drI5353zQDAqZrAHtpFeUhOXopSK+uKm6Ivv4G2icV26B4rbmcratKRIEMh32yDGKs7y9G1O2kG1ayzpOxzhOUVrDJJdUByDHs0mbJGLsoQqMM26Jv67/l5Wx995nSeVE4mzMeviWflZ7c0yF0g59EypleaYgY1bUWJO3RnkCl5PYNFbv8UHZDDfRGl58hcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAZ0lEQVR4nGNgGLqAFa/cP4YGBgYGBgY+rPJHoTQTusR9BPNfNKpUBgPDAxg7koGBEbftb/+h8v+dwK2W4QUqtwmPUiSgxcCQhiZ0C8lCdCPRHIRDGwODGAMDQxyMc4/BWIw414wIAAAlbhGwYr7lKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(3223, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABFElEQVR4nNWQvUuCYRTFf6/2JUKQQdZkYK0ZQYtbQ0EFkQ22RBCEEDQERTQ4Rf+AQ2vQlFtjDe8iLrXZVLoEFcRLFiRlVM+Bhhz8eBwaO8uF+7uXe86F/ySnVvfTDu71mFu8Aj7uG2bCj6pT+XwbAN8v7O8BMN8GDPRNBRpWV8q6TA0ykFqNeqqMNB3fVTUC0J2VN97srDOj50lgVjpt9R051NoQoQs9TVhS+Re90nJO2rOHXj+TlOmwQ5KSkm3YcElSLmSHrm7SB9UjxwrzXzOQ1aaNLVXyQEJ3FtZVeNsB/AWz0AqDOgGYflGi1qkLFSMaiM+H53rfb22b+jSSXjcsN4OSpOJWzGbWF3/Q8WibF/xRP4uSb3fF/mrsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAs0lEQVR4nGWSQQIDMQgCYf//5+lBjSTtpW5QQdRIxshYyMISkmQkKqT/JSkiUUi8aIv+fkxNphAoSHxI7q4e2J302U+7Uid3MudlIlpThaNwuxCJUXgYOK/3VDxDEhMmCwkteBpfaDnTUtrdZaJzic+oJNROh7dpFoW+VaWrcDZ/NHLxD1f6E37WGXWyVZd2mXtZezl61pGTokdZ08/kcY5rRq2eVhR7WOrlcUFmhHOO0PoBWznUSFjIx70AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAi0lEQVR4nGNgGJLgHwPDboYDELYshuzfvwwMfyGsNzswJBkYfuIy9s1fBgYJXJJleF00Ec5aj036L5Q2wqr5JT6TU7bjtRgPUMArC3ESI+nGfoJQj7BKfilhYGBgYPjjhynF/RdTDAmc4sQt95eB4Q8Dw1+GTKySfxkYGG5i18jE8ISBQVUIr8XEAgA6kx/dHtSx5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B001A7640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(8274, shape=(), dtype=int64) which is a tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqElEQVR4nGNgGAyg+pcTbskX/x7CmEzoclH8uPWJPfyHW2eSLAPDNRwape7++/fTDoek/b9//7bikIu+++/ffmY4F9VOV0UGhnN/cUgyMDIwIvFQJIU9/zP8v4vdRual//7/a2LBLsn279+/O+I43Nrw79+fIhxy9r/+/TuMIoLkICcWBoZXODSKv/z374slDsmof//+XUUVwohP7IBtzb/PVsQppSUAAD2oN/R1U1YsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAwklEQVR4nFWSWxKAMAgDE+9/5/UjvOqMTouUbCjGQpJk5cHCErIEEkJkoY6I/LaQLJRFNlMo2TnIhvii5pSdbIcCkTivfB1viPyZ4kAFWuyURqFlfSIj1eeTjCxfUBk7CcFZqLU1SILXZK/hpSnqm3qgN+PQU+9Ru9kjDX0jZbVdG+dil3hat/S3f7sFEN+MR1XtHvV9Z0J4BQqN7WGV3SZPsKNtDHoS7oyM46/ER64Om77gYzK1d16e9g7yMmyHHtQfQhfhVqvGa8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbUlEQVR4nGNgGPTgBcNDPLL/yDO04h/Dv+24JO/++/fTDqeF/xi24pCKJs8tEPAfic2ELCH8kuHf/yyczmFgaGLBLsX2j4GBQRy3jX9wSdhjhCqSg5xQXYcCxBkYGBgscduIL7L+MVzFIzsYAAClIBmywf6FYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(5778, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nGNgGGgQ/+/Ycicccqyn//3/t5QTu6TQvxfePwVx6Cz+485gtBS7nPjX4wwMDFfVsEoWfDVmYGDIXsyKRU7g4RsGBgYGpU/KMBEmhKSF7HYGBgaGe29UsEia/J/BwMDAwLC6BEkQBrY/htDKn3QwdMpaPoMw7r7CNJaf7xCU9R+LnZgAiyQ3OzZJqIPSRX5gaND5N52BgYGBQevDbEzTOC89ZmBgYBC8e1cYi/WW360ZGPSe3FDBIsfAtfnnjh3/7nJjd7nQmn8Hu5mxy9ENAAAojTin+oxW2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvklEQVR4nG2SWxbDIAgFh5zuf8vTDx4SW49JMCBcRhFyopSRU3K4ApwFAYYE7yEhlYiO9yRqXwZpVlVEtLfkx0vEpxYhWbrdt9T3n6o5MqhqsNo8O/BlZtctq9eZJMBAyK4PDBeFle6AXii9FR92B1zBuXv7kb8cDo/GWuobyWpmogp7u5bvKt7n81LlXz1Db4EA5NlkKlFkbPBUd4FGlKIOfyCSd4RzCyOfT9l1NGWm23l1qrA8hHM265KM9i90FuBQRlAybwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B001A7640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbElEQVR4nGNgGGgQz8DA4IRDjvX0P9wahRgYGH4KkmXncQYGhqs45L4S1v5JGYugx7/FRNi8G4vY9sdEaGRg0MEUwhM6DDoM3UQYyo3V2DwIXfQNt0atD9hELxHnFWxg80+8/vl3kBjv0BQAAGliGB9aQjAdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(158, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/UlEQVR4nGNgoAsI+bcQtyTzxa86TNilJGsX6n75G4dVjmXS3x8MlX87sErm/v37g4H/7zdjbJLP/j7RZmDa+HcJphT33F9/kxgYGLy+vDDAkEz9+3cKIwMDA8PFvxXocrLv/u7kY2BgYGCow5Bkmfr3rSmEWfv3JoQB97BQBsPk0zAOI5pkEsO7WXCpQ6iSLGEMB59B2f8Z7qBa6fj3rwSMvf9vDJrkP5gkU8Wv7cxQ42Cy/2GMklaG639Rdcrf+5vAxcDA7v/q9w03NjRvMoT+/Xt+795Tf//+nYIuxcDAufHn37///v59WM6MKcnAkDrzx7XZM0SwSQ0WAAAf+VtHueiG1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArklEQVR4nI2SSRaDQAhEf/Fy/ytXFkwmaX3BRdsgNYCAsQFDHYAxJgyWJp0vFqLC1cxgJI4sMMjICLASQxDYoKpVu7QEC+3muaistL+e7mkt46USc2m0q/BP5qbBQNjyOuvQ9Fynl7A7R2a8q3xdHeKhNPrOHyVJ7G7+RKXEx6Oqo+aaUpwosyziFvOma2F/U4DyeE06NyFjGdlqQ7n0+T1mCZ92ZqS5uLXZmboZ3vsEr4JcsKtoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B000B9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAYElEQVR4nGNgoBNYiE/yKwMTdglJfLomMfxgqGTowK2AH4f4M4YnDAxMG//ikE6CUAZYpP4yMEIYFRAK2dl/mRn+MzAwMNTBBOCSLAwMzFDmf3zuJQDmkK0zhmydowABAFYaDtW6CA3bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B001A7640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataset_summary(d):\n",
    "    for el in d.take(1):\n",
    "        for name, v in el.items():\n",
    "            print(name, \"shape\", v.shape)\n",
    "            print(name, \"dtype\", v.dtype)\n",
    "\n",
    "    for el in d.take(5):\n",
    "        print(\"index\", el[\"index\"], \"which is a\", el[\"label\"])\n",
    "        display_uint8_image(el[\"image\"])\n",
    "        display_mask(el[\"mask\"])\n",
    "        display_uint8_image(tf.reshape(el[\"image\"], [28, 28]) * tf.cast(mask_to_image_mask(el[\"mask\"]), tf.uint8))\n",
    "\n",
    "dataset_summary(dataset_train)\n",
    "dataset_summary(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = layers.Dropout(dropout_rate)\n",
    "    dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        attn_output = attn(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = dropout1(attn_output)\n",
    "        if include_residual:\n",
    "            attn_output = inputs + attn_output\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "#         attn_output = attn_output * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        attn_output = layernorm1(attn_output)\n",
    "        dense_output = dense_net_1(attn_output)\n",
    "        dense_output = dense_net_2(dense_output)\n",
    "        dense_output = dropout2(dense_output)\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 32\n",
    "    pointwise_feedforward_dim = 8000\n",
    "\n",
    "    vals = keras.Input(shape=[784], name='vals', batch_size=batch_size)\n",
    "    rows = keras.Input(shape=[784], name='rows', batch_size=batch_size)\n",
    "    cols = keras.Input(shape=[784], name='cols', batch_size=batch_size)\n",
    "    mask = keras.Input(shape=[784], name='mask', batch_size=batch_size, dtype=tf.bool)\n",
    "    \n",
    "    print(vals.shape)\n",
    "    print(rows.shape)\n",
    "    print(cols.shape)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    rows_pos_enc = positional_encoding(rows, n_embd//2)\n",
    "    cols_pos_enc = positional_encoding(cols, n_embd//2)\n",
    "    \n",
    "    print(rows_pos_enc.shape)\n",
    "    print(cols_pos_enc.shape)\n",
    "    \n",
    "    pos_enc = tf.concat([rows_pos_enc, cols_pos_enc], axis=-1)\n",
    "    print(pos_enc.shape)\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    \n",
    "#     rows = tf.expand_dims(rows, -1)\n",
    "#     cols = tf.expand_dims(cols, -1)\n",
    "    \n",
    "#     m = tf.concat([vals,rows,cols], axis=-1)\n",
    "    \n",
    "    m = layers.Reshape((784, 1))(vals)\n",
    "#     m = layers.Concatenate()([m, rows_pos_enc, cols_pos_enc])\n",
    "    m = layers.Dense(pointwise_feedforward_dim, activation='relu')(m)\n",
    "    m = layers.Dense(n_embd, activation=None)(m)\n",
    "    \n",
    "#     print(m.shape)\n",
    "    \n",
    "    m = m + pos_enc\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=2, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=2, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    \n",
    "    m = layers.Flatten()(m)\n",
    "    \n",
    "    m = layers.Dense(784, activation='relu')(m)\n",
    "\n",
    "    vals_out = layers.Reshape((784,), name='vals_out')(m)\n",
    "    \n",
    "    model = keras.Model(inputs=[vals, rows, cols, mask], outputs=[vals_out])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784, 16)\n",
      "(8, 784, 16)\n",
      "(8, 784, 32)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "rows (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cols (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (8, 784, 1)          0           rows[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (8, 784, 1)          0           cols[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "vals (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_2 (TFOpLambda)  (8, 784, 8)          0           tf.expand_dims_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_3 (TFOpLambda)  (8, 784, 8)          0           tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (8, 784, 1)          0           vals[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_2 (TFOpLambda)      (8, 784, 8)          0           tf.math.truediv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_2 (TFOpLambda)      (8, 784, 8)          0           tf.math.truediv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_3 (TFOpLambda)      (8, 784, 8)          0           tf.math.truediv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_3 (TFOpLambda)      (8, 784, 8)          0           tf.math.truediv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (8, 784, 8000)       16000       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (8, 784, 16)         0           tf.math.sin_2[0][0]              \n",
      "                                                                 tf.math.cos_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_4 (TFOpLambda)        (8, 784, 16)         0           tf.math.sin_3[0][0]              \n",
      "                                                                 tf.math.cos_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (8, 784, 32)         256032      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_5 (TFOpLambda)        (8, 784, 32)         0           tf.concat_3[0][0]                \n",
      "                                                                 tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (8, 784, 32)         0           dense_8[0][0]                    \n",
      "                                                                 tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_2 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_4[0][0] \n",
      "                                                                 tf.__operators__.getitem_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (8, 784, 32)         33568       tf.__operators__.add_5[0][0]     \n",
      "                                                                 tf.__operators__.add_5[0][0]     \n",
      "                                                                 tf.math.logical_and_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (8, 784, 32)         0           multi_head_attention_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (8, 784, 32)         0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (8, 784, 32)         64          tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (8, 784, 8000)       264000      layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (8, 784, 32)         256032      dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (8, 784, 32)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (8, 784, 32)         0           layer_normalization_4[0][0]      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (8, 784, 32)         64          tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_3 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_6[0][0] \n",
      "                                                                 tf.__operators__.getitem_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (8, 784, 32)         33568       layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "                                                                 tf.math.logical_and_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (8, 784, 32)         0           multi_head_attention_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (8, 784, 32)         0           layer_normalization_5[0][0]      \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (8, 784, 32)         64          tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (8, 784, 8000)       264000      layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (8, 784, 32)         256032      dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (8, 784, 32)         0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (8, 784, 32)         0           layer_normalization_6[0][0]      \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (8, 784, 32)         64          tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (8, 25088)           0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (8, 784)             19669776    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "vals_out (Reshape)              (8, 784)             0           dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,049,264\n",
      "Trainable params: 21,049,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss={ \"vals_out\": keras.losses.MeanSquaredError() })\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, n=1):\n",
    "    \n",
    "    dataset = dataset.batch(n)\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        outputs = txformer(batch)\n",
    "        for i in range(n):\n",
    "            print(\"index\", batch[\"index\"][i], \"which is a\", batch[\"label\"][i])\n",
    "            display_uint8_image(batch[\"image\"][i])\n",
    "            display_mask(batch[\"mask\"][i])\n",
    "            display_uint8_image(tf.reshape(batch[\"image\"][i], [28, 28]) * tf.cast(mask_to_image_mask(batch[\"mask\"][i]), tf.uint8))\n",
    "            display_float32_image(outputs[i])\n",
    "\n",
    "def gen_image_from_(dataset, n=1):\n",
    "    \n",
    "    dataset = dataset.batch(n)\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        outputs = txformer(batch)\n",
    "        for i in range(n):\n",
    "            print(\"index\", batch[\"index\"][i], \"which is a\", batch[\"label\"][i])\n",
    "            display_uint8_image(batch[\"image\"][i])\n",
    "            display_mask(batch[\"mask\"][i])\n",
    "            display_uint8_image(tf.reshape(batch[\"image\"][i], [28, 28]) * tf.cast(mask_to_image_mask(batch[\"mask\"][i]), tf.uint8))\n",
    "            display_float32_image(outputs[i])\n",
    "        \n",
    "def image_performance_test():\n",
    "    gen_image(dataset_test, n=3)\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    dataset = dataset.map(lambda data: (data, data['vals']))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10000)\n",
    "    txformer.fit(dataset, epochs=1, steps_per_epoch=4000, batch_size=batch_size, callbacks=[WandbCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clarkemaxw/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:588: UserWarning: Input dict contained keys ['image', 'label', 'index'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(9068, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGGSAEYktauLLy7Dsw3EsylgqPv79+/TY8a+ZmHLCWz/M15pwrEE194cNhuSyd7YMDNJ1Fz/+/Xs/lw9VTuNzANRij6kr/r5s5EWWPPNZHYk39efHWlE4T+trIYpBKgv/xsA5i7/Ko1rDdv4DnP3vGLoDj19jYGCCsvejyfEJ98AlGc+hSXbLXUIYK4Qqp/AsHMH5Z48ix71lHhLvXw6yHPv2+6IMDAgHIQO9E7pur5H4T16Jw5gCYa+eKKIodvhpDmUFnvh7WhXKhkX2RqsFt++LqDM42J3s2v4DzR6elOs//v399WSCGQtCECmZmIgwfDuExX1UBwBD8FOYwq6gTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB41EF0D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkUlEQVR4nH2SQRIEIQwCG8v/f5k9JOOMtWgOGgViSCkLMCAMdRBYwCxIa+kQAHYt9oP0XsBi298DML1yIcuoH7C8lW6WO9/r7vESEsMPFOVDUVQxjdyW05tbciJEwDfK5b502SafAf7FaCCZ6WbihK6NlugywKPUtC7iJjfzEk7denByiRj1yeP49LXavtbn9g9ynG69krOssAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B0004CFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAMElEQVR4nGNgGOzAk35WaeCTPINHTot8O4+Rq/E/VlEmqKQQHp3/8JqbQ6Z7hj8AANDNBarcuekrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B001E6FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAMQCAAAAADDHFD2AAAA3klEQVR4nIVUW64FIQgr9dz9b/l+OIpAUZNJRh6laBEEYFDLCILa5znZbwCMliI8avr+Hmhkzfw4pawhanR8HbUwLjUU2mmbRMYj6rRYExN7am9DWiuC/5v037CUfenDUlTtiejPjs4q8GOJ3Er4lEdGbWyFKHU0WItdwE+8bgi260WsHDe+zzsLN5MyYz/izEolY53WPF0ZP+8r97say4l5Vc7NSvCJODn1N/xip5R843afu8wjnqTW2GRhm8nvwiFojYC/Dau+KztzPi1s5kKvqh4+/Hrx4G3Hmz/+AV2DAR0M9OCRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1x784 at 0x7F7AB41EF0D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(5374, shape=(), dtype=int64) which is a tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGGAQ/+9/PE7J6f9+3+PHISfz/kDvv1IcktN+xxq/PyaIVY733iwGhtn/ChEiTAhmgMIJBoblf+yw6ty4n4eBgeHyB1lsOq8UfGFgYNjG54/DSQwMDJ3/1mPTiSmERXI3PkkEYEEwJV1CdD7udmCQ17yOoUxqzb9Hh8/9+/fv3z42DMk5j8J5GQSe/ptuw4Uh5/rJl4FBafu/WcxYLN82jYFB//m/E0JY5AR+T+Ss/fFvLTY5Bp6X10/++1TBgk2OgaH135cWEexSdAUAQydE4Z7rszYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB41E6130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAj0lEQVR4nH2SURLFIAgDV+9/530fgqVTeHZGW0II1KACgAioRESIF000vg6QTFDMHcGVAMDyPEDGj1xK3YJYMuoyqH6By/yCiPoN36aPvntKAdZuO3pP2FSeJZ/mhqQeecLNT6CfvxB9i1euc8N/C+eFT7QZTGx3KeELd8dZAGt2CafshAyDWo5iyXRI+P8HjvF6nhaUCDAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7CEC5D1C40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAI0lEQVR4nGNgGAUkA1lyNf5bT01nIIzFK9uNV3YfNR0yvAEAG/cEEmyvNvwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB41EF490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAMQCAAAAADDHFD2AAAA20lEQVR4nKVUSRLDMAgTSv7/5h48DgLkTKflktgGiR0AAMT6gEhhuYmi525CjPe/6leGgNNIjBDtIJhnYTR4VYKhXLys1parYXffOke8nDSCml3N0wnPR/ON8PGz1qBUx/DGo6PuacwnqylOc9app4HGdrLH0wm/56gzbP+SY/o4a/cvPwvL5DzhuwxtG46XOWdzkr3lmXX9320isoMaH0+sOhc8vqrtnF+H2ztLX28Ae9IpPUyJZtjL3vDZqzersnXOE93Pf+645Z/HPm3LzN4Eb57VrVA0umcEPjAFARfO1ic2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1x784 at 0x7F7B001E6FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(92, shape=(), dtype=int64) which is a tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAg0lEQVR4nGNgGMRA7d7fvygCTEhsfbn//+twSTIwMDDw4ZPEaSwGYEFiMzIyMDDiUmj58s+fjxK4ZI/++fNXmuoOYmBkxKPYA5+dHxkYGBRwSf76xsCQg0vy7HZUa9AcwMjEiFvy/z9vI1zONX38989SXJIM+n//HMZp7MWNOPVRDwAAD9Eg3/xEdvQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB41DECA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAsklEQVR4nH2SQZIDMQgDW/P/P/ceMJgkm/gwNQhhCXC4R4AYgxCIQAADEskhxuIrYv2ueGU6PhJyoAkGFxs5le56W3IbbonRsIS7+iUzdljAkXRI+yqAZ7RyKw253f1jexq0gPK1vNxeLqkjH6PZ3oI99j23j/P8Tv/K8SUn+HyrSTGqAavHhkSIwcMi1UIqWM/iLOplamdcs8KPR/S28sVxPhddO+rnea/x3coonUco+Af1jdhKHC4SBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7B0004CFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAVklEQVR4nGNgGMRA7R5++T8oPCY02R4ybf2DqhPVWEYGRnw6P+KR/MMgjcvYoywsf5H5LGjGDiWwEpWLFit/GD4b4dT6+C9OKQYGBhRJ9MjeSNhhFAMATroRQ0CCymAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7AB41EF0D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAMQCAAAAADDHFD2AAAA4ElEQVR4nIVUSxaEMAgL6f3P7CxGKBBau/BVfoEQBQAARgAw5GNQ23Q8gi2Wr6/XscDTGkbQcyd0hs8SAorl3qcjf82q995z8l88mwFlyNptDZ29Mw/7qTWci8UnWFQEZ2ne6rRxZdWnJeqe1gEt52hHGmnN29lleqri6vvO7zh5nj1jmz0VPDNm8pYsooxch+1+UkfEDNp1vvRknnJWfAGlt9t3ngNjRjq6MmtDnlb46/1rZq26zypsW7Jv/MPeSKgq67/Bdg3Ok6DFdWvhrWCJ6qFqmOwnfH4o7W6//L9+v14CGpGpkAQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1x784 at 0x7F7AB41EF490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_image(dataset_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(dataset_test, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
