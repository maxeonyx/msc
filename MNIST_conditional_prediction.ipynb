{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "# wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "# config = wandb.config\n",
    "# config.learning_rate = 0.01\n",
    "\n",
    "# callbacks += [WandbCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00]\n",
      " [ 7.0710683e-01  1.3921213e-01  2.4833918e-02  4.4166050e-03\n",
      "   7.0710677e-01  9.9026257e-01  9.9969161e-01  9.9999022e-01]\n",
      " [ 1.0000000e+00  2.7571312e-01  4.9652517e-02  8.8331243e-03\n",
      "  -4.3711388e-08  9.6123999e-01  9.9876654e-01  9.9996096e-01]\n",
      " [ 7.0710683e-01  4.0684462e-01  7.4440487e-02  1.3249470e-02\n",
      "  -7.0710677e-01  9.1349739e-01  9.9722546e-01  9.9991220e-01]\n",
      " [-8.7422777e-08  5.3005296e-01  9.9182546e-02  1.7665559e-02\n",
      "  -1.0000000e+00  8.4796453e-01  9.9506927e-01  9.9984396e-01]], shape=(5, 8), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 01:40:11.264401: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 01:40:11.817124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(mask):\n",
    "    image_mask = tf.reshape(mask, [28, 28])\n",
    "    return image_mask\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=1000):\n",
    "\n",
    "    i = tf.range(dims//2, dtype=tf.float32)\n",
    "    i = tf.expand_dims(i, -2)\n",
    "    \n",
    "    vals = tf.expand_dims(vals, -1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = vals / tf.pow(scale, 2.*i/dims)\n",
    "    \n",
    "    sin = tf.sin(rate)\n",
    "    cos = tf.cos(rate)\n",
    "    \n",
    "#     # expand dims to allow alternating concat\n",
    "#     sin = tf.expand_dims(sin, -1)\n",
    "#     cos = tf.expand_dims(cos, -1)\n",
    "    \n",
    "    encoding = tf.concat([sin, cos], axis=-1)\n",
    "    \n",
    "#     encoding = tf.reshape(encoding, [-1, dims])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(positional_encoding(tf.constant([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]), 8))\n",
    "\n",
    "def img_to_tuples(img):\n",
    "    \n",
    "    height, width = img.shape\n",
    "    length = height * width\n",
    "    vals = tf.reshape(img, [length])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1])\n",
    "    cols = tf.reshape(cols, [-1])\n",
    "    \n",
    "    # permute the order, to ensure the network uses the positional encoding and not the implicit locaiton\n",
    "    idxs = tf.range(length)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    rows = tf.gather(rows, idxs)\n",
    "    cols = tf.gather(cols, idxs)\n",
    "    vals = tf.gather(vals, idxs)\n",
    "    \n",
    "    return vals, rows, cols\n",
    "\n",
    "def random_mask():\n",
    "    idxs = tf.range(784)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "    idxs = idxs[:n]\n",
    "    return idxs_to_multihot(idxs)\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask), np.uint8)\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "positions = tf.range(-28, 28, dtype=tf.float32)\n",
    "encodings = positional_encoding(positions, 16, scale=28)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "im = ax.imshow(encodings)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 01:40:14.655922: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def make_dataset_generator(x, y, seed, typ='single pixel'):\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    # keep track of the index in the original MNIST\n",
    "    def to_dict(i, xy):\n",
    "        image, label = xy\n",
    "        data = {}\n",
    "        data['index'] = i\n",
    "        data['image'] = image\n",
    "        data['label'] = label\n",
    "        return data\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.map(to_dict)\n",
    "    \n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # add a transformation of MNIST images into val, row, col\n",
    "    def add_tuples(data):\n",
    "        data['val'], data['row'], data['col'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    dataset = dataset.map(add_tuples)\n",
    "    \n",
    "    # create a mask of random pixels masked out\n",
    "    def add_mask(data):\n",
    "        data['mask'] = random_mask()\n",
    "        return data\n",
    "    dataset = dataset.map(add_mask)\n",
    "    \n",
    "    # mask out a square region as well as random pixels\n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask()\n",
    "        data['mask'] = tf.logical_and(mask, square_mask)\n",
    "        return data\n",
    "    dataset = dataset.map(add_square_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate training pairs\n",
    "    \n",
    "    def single_pixel(data):\n",
    "        data['target_val'] = tf.cast(data['image'][data['target_row'], data['target_col']], tf.float32)\n",
    "        \n",
    "#         mask_out_target_pixel = False\n",
    "#         if mask_out_target_pixel:\n",
    "#             target_idx = data['target_row'] * 28 + data['target_col']\n",
    "#             target_mask = idxs_to_onehots(target_idx)\n",
    "#             data['mask'] = tf.logical_and(data['mask'], target_mask)\n",
    "        \n",
    "        # offset positions relative to target pixel so target is at 0,0\n",
    "        data['row'] = data['row'] - tf.cast(data['target_row'], tf.float32)\n",
    "        data['col'] = data['col'] - tf.cast(data['target_col'], tf.float32)\n",
    "        \n",
    "        return (data, data['target_val'])\n",
    "    \n",
    "    def single_pixel_random_rowcol(data):\n",
    "        data['target_row']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        data['target_col']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        \n",
    "        return single_pixel(data)\n",
    "        \n",
    "    def many_single_pixels(data):\n",
    "        rows = tf.range(28)\n",
    "        cols = tf.range(28)\n",
    "        cols, rows = tf.meshgrid(rows, cols)\n",
    "        \n",
    "        rows = tf.reshape(rows, [-1])\n",
    "        cols = tf.reshape(cols, [-1])\n",
    "        \n",
    "        image = data['image']\n",
    "        val = data['val']\n",
    "        row = data['row']\n",
    "        col = data['col']\n",
    "        mask = data['mask']\n",
    "        label = data['label']\n",
    "        index = data['index']\n",
    "        \n",
    "        def data_plus_pixel_index(i):\n",
    "            new_datum = {}\n",
    "            new_datum['pix_index'] = i\n",
    "            new_datum['target_row'] = rows[i]\n",
    "            new_datum['target_col'] = cols[i]\n",
    "            return new_datum\n",
    "        \n",
    "        def add_original(new_datum):\n",
    "            \n",
    "            new_datum['index'] = index\n",
    "            new_datum['val'] = val\n",
    "            new_datum['row'] = row\n",
    "            new_datum['col'] = col\n",
    "            new_datum['image'] = image\n",
    "            new_datum['mask'] = mask\n",
    "            new_datum['label'] = label\n",
    "            \n",
    "            return new_datum\n",
    "        \n",
    "        d = tf.data.Dataset.range(784)\n",
    "        d = d.map(data_plus_pixel_index)\n",
    "        d = d.map(add_original)\n",
    "        d = d.map(single_pixel)\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    # single pixel example. the row & col are translated by a random\n",
    "    # amount and the target val is the new pixel at 0,0\n",
    "    if typ == 'single pixel':\n",
    "        dataset = dataset.map(single_pixel_random_rowcol)\n",
    "    \n",
    "    # 'many single pixels' generates 784 single pixels from each image,\n",
    "    # and the target vals are each pixel in turn, translated so that\n",
    "    # they are at 0,0\n",
    "    elif typ == 'many single pixels':\n",
    "        dataset = dataset.interleave(many_single_pixels, block_length=784)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "def make_datasets():\n",
    "\n",
    "    train = make_dataset_generator(x_train, y_train, seed=192_168_1_1)\n",
    "    test = make_dataset_generator(x_test, y_test, seed=10_1_1_1, typ='many single pixels')\n",
    "    \n",
    "    return train, iter(test)\n",
    "\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 693 which is a 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA50lEQVR4nN3QvUpCcRgG8MePJcFBziQ4NHUDJQgKIo1B0JBtXUKSFyBuLgVNjs6K3kCjgYtBB1edikL4L1qKgjzPOQ3WgdM57eK7vfx4P4Edj9PHxqUVTrXXFcV+MhRny+nGlTKheJSzmiKzP2nch2NgAqD8vE2jYQ3s/9Z9c/Tyu5C/7WH6KuW67UWwxuoNPkiKJh+w2pQUSZHzYz8VPp31Wo5Ur9pS1WdFw1HdUPPWAa5Ju9N58KxiyJ4hlxcAkl2KPPHwfjtreAYASJzflTIxD78kOaPbvw+PAABuysBT8z143t7EN6cxasmQ2QMKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA42022E50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAv0lEQVR4nF2S2w7AMAhCD/7/P7MHL9UtWRqxImhlhEH1Q8YABGDAorFOoQBLCalAqxiKKmkx8CgIoalY1Zk09opb0wjqQPeKiNLe1ZpD4219W1AjOmL0ksXryRZfdH+xsgVEU1gzZBegmMFZzom5+tuzktPs7qf0/4xFa8ndpKc8tO/KFQ3pXtlLV7doJyCVvBlSTCjPqlWrbUH9Wp6B9zR+u7l2//Dz2f175AOF1oA4N0VkX6Vp7ay9Z376yfABBWlbO36qjusAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB30F1A5B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvUlEQVR4nN3PvwpBcRjG8a8/C3UWJmVwEUrKZlTKYnQJlMEoF0C5ADeg3IALMBnOzkRKnYUkSs8rA5HO7+zybE+fente+PFU55HU31wEeE48nLkB+VdNfmEpk50AuZ0LV7AGmstnjbuu+1GbtpGDCuUxRs9FswVCpiBM/b1kSCYdQ3i/Xg2zQdc3635TIAjEkRQtyWf6oU4gzZBoAB4yqfjGkZBJ1ABI14eQeOPJzO7h/2IAtJtQye+cv/9HHti0S4I64ZRAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAC501D77C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 68 which is a 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAo0lEQVR4nMWQwRUCIQxEv9Zh+pBCpJC1DhrhWQdaRxoZD7g8Nq4edU55fCaZBP4sq1L5gJokV95jRapG3vOaqyUAl3wJTHq9lLpEs6uu3xIU+cTSYF2L2mwMEarSqDdt+ksFjr1+RHg9fXFmDefdIoQBb6RdCoC99S1T/hoWxadFzbfUNN/XfD5o8jCmSDUDWHZ5jJ+bVpXODptJlzPALYT7qZ4Zhlh/4L0g9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAC501D7A90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAz0lEQVR4nGWSQRLDMAgDd5n+/8vqAbCTNoc4EyEhYYzBGADm2K8i8IO5FQVAAO1jMdEMoaH3U0DEYa+oAlYLxAMFNKGtHK/X7PToytPvDVfanLKNEUim9KZcuelVk7KPPONIeWbiJBhu4t9QH8wU4Ljh8VaCZIMYI+TKbBQkU+fJ899sEwmffyM3W71IAjl3TelNlyePXpPkJ8oBQ0ulxy03NDWed4FCcpR3wdIXtc6aW/N7XM4CpXX6ymx0O4+1FLrZ7hrMjD7PXPTWNRT4Aj4xa0Ndu3aYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAC501D7A90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnklEQVR4nMWRwRHDIAwEN+lDfYRCQiHug0YY1+H0oUbODxIbZHsmn0zuhVhxHAL+LEMqlwihfMaKhJHPzpprSQAu+RQtee+UOhEOu+qnLQHyjqWNNU3qq6EAULpmBKtLDQEAyALuALzs0H/b4EyKcI9hB9/SZawxnS/72nykpn6+Ngw0ebxGqu0vs+Mxfl6Emoptz9m9nw+A+cvR/UQrpXZJ4212WXgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70708580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix_index shape ()\n",
      "pix_index dtype <dtype: 'int64'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 290 which is a 8\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C7AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C7C70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C78E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 290 which is a 8\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70725D60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 290 which is a 8\n",
      "pix_idx: 782\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB22459EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 290 which is a 8\n",
      "pix_idx: 783\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB22459EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 489 which is a 1\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgUlEQVR4nGNgGAQg+CRuOYN37xEcJjRJK4H9uCU5GNbjNvbsPw2ccnq/TrDgNNaP5dcfnJJSDItwW/n4txhOOcu/+5C5qMaWM17HLSmM20IGhsMvtHHKSb8+g8JHMVZCZAVuycj/X3Fb+fK/CW7JD3eZcRvLcOgvHp2vWHBL0gkAAEC6H4M4KgKfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvUlEQVR4nHWSQZJFIQgDu7n/nTMLxIdVf9goRkyISDAABtNbhHiAXg3n3kRdjABBED2gNCAiBGLiVHZ0IfYShCIPzCYtHH0nDh/BV9690D2UOIViZ0PgkBhfQrBaoAJZmEAKiECMqyqrz4Rwfd32EZyz1nDaux/xK2qXrZqv3TH0O7icHj2fTMeVOZyPfzgv3fFqNztj8sTkxWfvjcmrHX9eu6/XsS2LGx9v3artAWJNwi8X6z/MBUbeUQvwB2wHVDhvACetAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAf0lEQVR4nGNgGAzgJG4pg3fvERwWNEkrASYEhwlNkoMhFrexZxk0cMrp/TqBZBGasX4sv/7glJRiWITbyse/xXDKWf7FrY9hw7+puCUP/0PhojnolTZOjdKvz+DWKSGyArdk5P+vuN3z8r8JbskPd5lxG8twCE8gfHiFHvf0BwBTZB5Gi3nEDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 489 which is a 1\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgUlEQVR4nGNgGAQg+CRuOYN37xEcJjRJK4H9uCU5GNbjNvbsPw2ccnq/TrDgNNaP5dcfnJJSDItwW/n4txhOOcu/+5C5qMaWM17HLSmM20IGhsMvtHHKSb8+g8JHMVZCZAVuycj/X3Fb+fK/CW7JD3eZcRvLcOgvHp2vWHBL0gkAAEC6H4M4KgKfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C75E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvUlEQVR4nHWSQZJFIQgDu7n/nTMLxIdVf9goRkyISDAABtNbhHiAXg3n3kRdjABBED2gNCAiBGLiVHZ0IfYShCIPzCYtHH0nDh/BV9690D2UOIViZ0PgkBhfQrBaoAJZmEAKiECMqyqrz4Rwfd32EZyz1nDaux/xK2qXrZqv3TH0O7icHj2fTMeVOZyPfzgv3fFqNztj8sTkxWfvjcmrHX9eu6/XsS2LGx9v3artAWJNwi8X6z/MBUbeUQvwB2wHVDhvACetAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70725D60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAf0lEQVR4nGNgGAzgJG4pg3fvERwWNEkrASYEhwlNkoMhFrexZxk0cMrp/TqBZBGasX4sv/7glJRiWITbyse/xXDKWf7FrY9hw7+puCUP/0PhojnolTZOjdKvz+DWKSGyArdk5P+vuN3z8r8JbskPd5lxG8twCE8gfHiFHvf0BwBTZB5Gi3nEDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shape_summary(data):\n",
    "    for name, v in data.items():\n",
    "        print(name, \"shape\", v.shape)\n",
    "        print(name, \"dtype\", v.dtype)\n",
    "\n",
    "def el_summary(data):\n",
    "    print(\"index\", data[\"index\"].numpy(), \"which is a\", data[\"label\"].numpy())\n",
    "    if 'pix_index' in data:\n",
    "        print(\"pix_idx:\", data[\"pix_index\"].numpy())\n",
    "    display_uint8_image(data[\"image\"])\n",
    "    display_mask(data[\"mask\"])\n",
    "    display_uint8_image(tf.reshape(data[\"image\"], [28, 28]) * tf.cast(mask_to_image_mask(data[\"mask\"]), tf.uint8))\n",
    "\n",
    "def train_summary(d):\n",
    "    data, target = next(iter(d))\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(iter(d))\n",
    "    el_summary(data)\n",
    "\n",
    "def test_summary(d):\n",
    "    data, target = next(d)\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    for i in range(780):\n",
    "        next(d)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "\n",
    "train_summary(dataset_train)\n",
    "\n",
    "\n",
    "# TODO: TEST DATASET GENERATOR DOES NOT WORK HOW I EXPECT.\n",
    "#       IT SHOULD PRODUCE 784 EXAMPLES with the SAME image and mask, then change\n",
    "#       to a different image and mask.\n",
    "\n",
    "test_summary(dataset_test)\n",
    "\n",
    "# reset datasets after summary, because it consumes elements\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = layers.Dropout(dropout_rate)\n",
    "    dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        attn_output = attn(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = dropout1(attn_output)\n",
    "        if include_residual:\n",
    "            attn_output = inputs + attn_output\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "        attn_output = attn_output * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        attn_output = layernorm1(attn_output)\n",
    "        dense_output = dense_net_1(attn_output)\n",
    "        dense_output = dense_net_2(dense_output)\n",
    "        dense_output = dropout2(dense_output)\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 20\n",
    "    pointwise_feedforward_dim = 200\n",
    "\n",
    "    val = keras.Input(shape=[784], name='val', batch_size=batch_size)\n",
    "    row = keras.Input(shape=[784], name='row', batch_size=batch_size)\n",
    "    col = keras.Input(shape=[784], name='col', batch_size=batch_size)\n",
    "    mask = keras.Input(shape=[784], name='mask', batch_size=batch_size, dtype=tf.bool)\n",
    "    \n",
    "    print(val.shape)\n",
    "    print(row.shape)\n",
    "    print(col.shape)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    row_pos_enc = positional_encoding(row, n_embd//2)\n",
    "    col_pos_enc = positional_encoding(col, n_embd//2)\n",
    "    \n",
    "    print(row_pos_enc.shape)\n",
    "    print(col_pos_enc.shape)\n",
    "    \n",
    "    pos_enc = tf.concat([row_pos_enc, col_pos_enc], axis=-1)\n",
    "    print(pos_enc.shape)\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    \n",
    "    m = tf.expand_dims(val, -1)\n",
    "#     m = tf.stack([val, row, col], axis=-1)\n",
    "\n",
    "    m = layers.Dense(pointwise_feedforward_dim, activation='relu')(m)\n",
    "    m = layers.Dense(n_embd, activation=None)(m)\n",
    "    \n",
    "#     print(m.shape)\n",
    "    \n",
    "    m = m + pos_enc\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=False)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=1, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    \n",
    "    m = layers.Flatten()(m)\n",
    "    \n",
    "    m = layers.Dense(200, activation='relu')(m)\n",
    "    m = layers.Dense(1, activation=None)(m)\n",
    "    \n",
    "    target_val = layers.Reshape([], name='target_val')(m)\n",
    "    \n",
    "    model = keras.Model(inputs=[val, row, col, mask], outputs=[target_val])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784, 10)\n",
      "(8, 784, 10)\n",
      "(8, 784, 20)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "row (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_10 (TFOpLambda)  (8, 784, 1)          0           row[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_11 (TFOpLambda)  (8, 784, 1)          0           col[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "val (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_4 (TFOpLambda)  (8, 784, 5)          0           tf.expand_dims_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_5 (TFOpLambda)  (8, 784, 5)          0           tf.expand_dims_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_12 (TFOpLambda)  (8, 784, 1)          0           val[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_4 (TFOpLambda)      (8, 784, 5)          0           tf.math.truediv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_4 (TFOpLambda)      (8, 784, 5)          0           tf.math.truediv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_5 (TFOpLambda)      (8, 784, 5)          0           tf.math.truediv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_5 (TFOpLambda)      (8, 784, 5)          0           tf.math.truediv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (8, 784, 200)        400         tf.expand_dims_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (8, 784, 10)         0           tf.math.sin_4[0][0]              \n",
      "                                                                 tf.math.cos_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (8, 784, 10)         0           tf.math.sin_5[0][0]              \n",
      "                                                                 tf.math.cos_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (8, 784, 20)         4020        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (8, 784, 20)         0           tf.concat_6[0][0]                \n",
      "                                                                 tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_8 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_9 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (8, 784, 20)         0           dense_16[0][0]                   \n",
      "                                                                 tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_4 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_8[0][0] \n",
      "                                                                 tf.__operators__.getitem_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (8, 784, 20)         13300       tf.__operators__.add_9[0][0]     \n",
      "                                                                 tf.__operators__.add_9[0][0]     \n",
      "                                                                 tf.math.logical_and_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_4 (TFOpLambda)          (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (8, 784, 20)         0           multi_head_attention_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_13 (TFOpLambda)  (8, 784, 1)          0           tf.cast_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (8, 784, 20)         0           dropout_8[0][0]                  \n",
      "                                                                 tf.expand_dims_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (8, 784, 20)         40          tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (8, 784, 200)        4200        layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (8, 784, 20)         4020        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (8, 784, 20)         0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (8, 784, 20)         0           layer_normalization_8[0][0]      \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (8, 784, 20)         40          tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (8, 15680)           0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (8, 200)             3136200     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (8, 1)               201         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "target_val (Reshape)            (8,)                 0           dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,162,421\n",
      "Trainable params: 3,162,421\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, n=1):\n",
    "    \n",
    "    dataset = dataset.batch(n)\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        outputs = txformer(batch)\n",
    "        for i in range(n):\n",
    "            print(\"index\", batch[\"index\"][i], \"which is a\", batch[\"label\"][i])\n",
    "            display_uint8_image(batch[\"image\"][i])\n",
    "            display_mask(batch[\"mask\"][i])\n",
    "            display_uint8_image(tf.reshape(batch[\"image\"][i], [28, 28]) * tf.cast(mask_to_image_mask(batch[\"mask\"][i]), tf.uint8))\n",
    "            display_float32_image(outputs[i])\n",
    "\n",
    "def gen_image_many_pixels(dataset):\n",
    "    \n",
    "    # assume dataset is a 'many single pixel dataset'\n",
    "    # so it has runs of 784 examples, one for each pixel in an mnist digit\n",
    "#     dataset = dataset.take(784)\n",
    "#     batch_size = 32\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     reconstructed_image = np.zeros([28, 28])\n",
    "#     for batch, batch_targ in dataset:\n",
    "#         inputs = [batch['val'], batch['row'], batch['col'], batch['mask']]\n",
    "#         out_vals = txformer(inputs)\n",
    "        \n",
    "#         # np can do this yay\n",
    "#         reconstructed_image[batch['target_row'], batch['target_col']] = out_vals\n",
    "\n",
    "    reconstructed_image = np.ones([28, 28]) * 230\n",
    "    for row in range(28):\n",
    "        for col in range(28):\n",
    "            data, targ = next(dataset)\n",
    "            inputs = [data['val'], data['row'], data['col'], data['mask']]\n",
    "            inputs = [tf.expand_dims(x, 0) for x in inputs]\n",
    "            \n",
    "            out_vals = txformer(inputs)\n",
    "            reconstructed_image[data['target_row'], data['target_col']] = out_vals\n",
    "    \n",
    "    reconstructed_image = np.clip(reconstructed_image, 0, 255)\n",
    "    \n",
    "    image = data['image']\n",
    "    mask = data['mask']\n",
    "    \n",
    "    print(\"index\", data[\"index\"], \"which is a\", data[\"label\"])\n",
    "    display_uint8_image(image)\n",
    "    display_mask(mask)\n",
    "    display_uint8_image(tf.reshape(image, [28, 28]) * tf.cast(mask_to_image_mask(mask), tf.uint8))\n",
    "    display_float32_image(reconstructed_image)\n",
    "        \n",
    "        \n",
    "def image_performance_test(n=5):\n",
    "    for i in range(n):\n",
    "        gen_image_many_pixels(dataset_test)\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10000)\n",
    "    \n",
    "    global callbacks\n",
    "    callbacks += []\n",
    "    \n",
    "    txformer.fit(dataset, epochs=10, steps_per_epoch=10000, batch_size=batch_size, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 651s 16ms/step - loss: 2633.6489\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 652s 16ms/step - loss: 1698.5370\n",
      "Epoch 3/10\n",
      "21014/40000 [==============>...............] - ETA: 5:09 - loss: 1477.4326"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_960565/3908081262.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_960565/1949207126.py\u001b[0m in \u001b[0;36mfit_one_epoch\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtxformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(62, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuUlEQVR4nGNgGEqAEcHsKzj3QPro+QPP/2Eqc7kX6DVz59u3vw7JYkouOwKhiwO5MSWvzscQYoIx2Nmv4JaUluLH4+4V30QZGBgYulyx6GS4zeHHwMDAYOyGTfI8AwcDA0OoxjlsxjIdecLNwDDrnyU2nf9WSZUxcpoyYtHHwMDAcPDf9j3rsetkYAgouxP1GJdnGBgYGJJx6cR0IxrfBZ/kg1fncUveEdbBY+fvr7glpd9fx+dEYgEAAykwVcqeaN4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA240D0D90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArElEQVR4nHWSWRbEIAgEi9z/zjUfsmgympcYoYFuAUABRFnP+ttskku0EGXOs4BwQNud7+RtR22DL/hsldeHACKj7NqA0ch1bnbDcfBushm937WB/jhPxZ+4SxgCUYz/ObvwPf5a9BI5os3m+eYvczHTdgFCYuECbPaGYTwgQQSQX8HIkOpEz8E2W2UfToe6YdNjlISfxTCSXsk2+W0pu+TRZF8d8j12VXbG2h+e8ctSLNTN0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA1C7EEF70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAVklEQVR4nGNgGKrg35k1DAzRDExYpFwYGBhm7nxLwIBAbhq4asiAg/+2M6xHFkAOSHumOwyPGSzx6UeSxBYF5IH2FzgcxMDAwHBHAo8kA4Mm1VyBCwAAUs0N3qvXKZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA24213E50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABkUlEQVR4nG2SO2tUURSFv3Xm3plJJiLjQFSGzBSp1EKwEcSgIugvUAsrq/TWluI/8N8EBgQFMZjUIWlMCCbIBAOJ8XHPsjj3MRJ3tc9eZz/WYgmBhflPZAJzDhNGSOXTQPqYAICswRRwWZYMEBJkgIWA8cwJAWwLofz2q1G7o9Tg9K+M0Lq7fri/8WY8uNJt50HNWBDDRx/WeuNvJ72VUYyg5iBBe3PHG9uT435GTItcMVE+/H5EiA6XLx1OY80ugYvXOikJrSApBDU7OX18PwdwjNG2XfUJuPhue6UDZHlVgUBSUEunc3e6MP/62VDMiIAEXzfn7g3QkxfX87ozK5X09O2tfi/vvzzePaivDIAj4L3V9w+Wno7n86JmkVWMzJf8+fLDhQtHBf90lvlk52Tr7OdB0k4qOxMaP09/jJZv3Jz8xqmYlaws2N0v1s9WW6jEaicIoz/o43gtGkCeOShx8+KnvaKkKQeRZFCCB8Uvoco4ICUTSqDu1UyoinLGrCWtlLixST3J532rCqEyB/wF1paW43yH4NQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA24126D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(106, shape=(), dtype=int64) which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABKklEQVR4nGNgoD9gRGIzxWgzmJ/81f/3I7oqXpXJlv/+/Tv+/uq+N7UcKFI8Zpv//ft3Mn9imh6D3aMd01Bkzf79+/d3jQSEM+vtP2WIPXD522YhLyCsJy8YtJB1spcZC8I5MguPz8TtBeVnM9GMRQBWQwncGqv//fPHJSf65B+Ga+FAVwrGYkEI6hsyMDBs+MAQxMDQ9xhVfeytz//+/fv3+kXl13+HWVF0yhxQYvhUx3DyzhOrvUwMv36j6DP79+8AKwMDAwPPzn///+1FkdPb+e+dIwMDA4Pn8X8Xzr97oYwkx/v4378ABgYGBq93/9YyMdS8MUWSFPr3758+g2bIvPf/QuUYGBhcVZAkmav+/dsx/e2RhmhlNkx/M3tmvZy6h5MdU4buAAAMAmyrwMOulQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6E2130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnklEQVR4nH2SSw7FMAgDx9G7/5XnLSBJF6WRorQYsPkgACogogjQdt6OAmvAiJJXrNMNkRWzBs7y+NAzHYv23aeqXh/BU2oLG6mF34iEsbWM0KecVjI5iM51GrCaG3P0xdyRpbrfg01nTX9e2Xq2yP0+hdnL5skmqUytYfMsgGRbsP8ebHb/fVy8FOzxt82uyrtKAXOIY3WgS+/6e1x/ROCCu6os0hAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA2431FFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAeklEQVR4nGNgGAaAg4GBgYGBCbukND5JLXySPvgkyQf++CSVyTaWjWydKMCM4d8BVgYGBgaenQz/0eT0djIwOEKY/xjOYzj2HwMDAwOD1zuGtQwMNQymaHL6EEYoAwMDgyuyFHMVA8OO6QwMODzvyfCSgYGTZH/QAAAAc8gQdCWDK7gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA1C7E72B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABpUlEQVR4nK1SO0vDUBQ+Nw9jrNTa4gutVWkpjW8UcXASxUUQnIUuOjkJjv0P7g6ugiA4iIsPfODQIijiUEVroq2PGq1p7cM0N9eh1iR09Wwf3/nuOd93LsD/FM0gM7QAJuC5j2ITNje6570b2QfEgqpbSQQU3z/p/FKcKT91mDM9i9hOX3VHC7bVHUcjmrM7FseGkppYofbpz+0waBgg2e3LKNgg9dzmah1WVAIAAF22hq0CMXZppQDoX+A52l0aZoyFSAIAfk1UzfhQTLrSAKiKIJB7lNGVZ2OmJQmBKUiRWw0qlAgAJ4Va5VImFiXiEOH6YrIafauXnoglIcbeLAR41XchumziHbbE55odzHAjjrvm4LfAhOO6eZRj+X5vzj8e8rduymp6XUAmpX16sSYVFm9EriOnsPlkaWSJpIcX6hOiXOSDY5y4PdDGF+GPpHpDPandgyzrHmpLr53g9kZXnpStILuaiZykXU1THzvXZ0WQ4gTpZSVLi++nkmqren1JnKcBiFY+MwBiAl54jOKCzhPV7KHUglhME6JVxPxvZfnG8APkkqGtS7fGfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA241D0850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(1016, shape=(), dtype=int64) which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABC0lEQVR4nGNgoAdQTr37f8dsDWxSTLFvf575/P3vNEZMOcktf/caMjDIfvzLiSGn/eLvbF4GBgaGV38F0eWk1v6tEmCASOaiS4a+NIBa9eqvFbrkXFcY69UeZnRJbpgTm/64YfMKAwMDA4PIm78OOCW5L91mgjKZMCS5ZK/9wykZzbcep6kK77ZguJWBgYHBzE2LgXXBXy+4AAuUFhKudBJn+/qOSfr7V6Z/KHpEG579/fv3wNq1B/7+/ft3tQmyHNuZ37fmiIiwMsgv/l1b9vnfqyVIkpV/T8owMPBH7/n49zQDg56DFrLO1nd/v3/58u3v35vVrMjikAC1jWGI5rx5e92arzi9OCgAAI2JXPPzvTi+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C78CDF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjElEQVR4nH2SWRIEIQhDY9//zm8+VLZOT1nlAoEEQRJnSZI4pxDkaxv20e4oo0lPII+BYmmgAEP4h/OydH2VBQZBZzO+S4nT0jN7BKPM18tnhfiQWWVKwZRZU3rJNuRYc5sJbztdnfzRUgfhcYGSpGW/rmgz3eTKWp5251x8ASoF3Wbmq/drTi/U4f0BZ/mChdhG7E4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA24126D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAPklEQVR4nGNgoAtIJaTg7zRGfLJUdAoCvMJvLDMNrLx0G4/kexpYyLoAj+QjBgZ7PNL4g+czPkk9vFqHKAAAo/ULYfhBhxAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA243343D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABqUlEQVR4nJ1RTUsbURQ952WiMSJKxpLRqEQ0+EEkwZCCG6so3bgT/AFZuBDcCu4Ff4C/odBluxZx0UIL4iIQ6kZRwcSqDDURoxXjuy4mMxOjK+/qvnfO/TjnAu8NvngQhFBLI0gBSUHQisXEOjooaaEAMACChCICmYVP1s3XI8MsEQIHFAigQWXOW7XN3ZbRSWv/Wuum+YnccjYMtG3tL5oGnUo3ov++PALAU0vRrmgBoLyNrYiqAQDGZ78Xahp1UACoj6tSEYDg+c6vcpPezk43DeUGXPX09Lui535mFRhQTtuXdsmMpTSgNWA4VlE8RixpFyniLgRACCAAAImNibPHOtXwG0bClzqcXY8d77LeyAXb76IZM1CdmO8t79mmrcUHGYqnR8YGQ61XUWlLP9ye/PcrGUytTAXPzy4rTJldmfH+P0UfVMPx/sJ2CeXTsYPeZLJ7hP4mauDzEtfy8iHcE4/kT4amD7/VvErpSPWU7gRmesi+Pr74+/vJNYUCGJNR/aPKYJ9Rte/rlhACx1IKhBT4PtHNycYTNMWbv29TX7F8GuEdFngGyYaH2jUKGTwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA240D6100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(199, shape=(), dtype=int64) which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nGNgGGBg1fL/75uWTGYsUiLu+//8/fPnz58GTDm9V3/+QCQf6mNIev6BSf7ZwQ4RYkJRcMzbJ42BgcFFFl2nYMqWBmYGBtU/f/78UcHhYmRJJnTJdAYGhu0PsWts/vznz1017HJu3//8+bMFh9z/v3//3sBwKwMDAwOD9/e/f/58d8Qq5/f5z98/fy7LYJML+AgJoRdtXOhSOgnv4cHnCxVjgVDcfUGCcHWvP6LqU/0DAX///Pnzp5sBVWc6jP99BcOTLjTJewwMDAwMFw91/n2N6VLhWX/+zApjx+pD+gIAuxN3/uzlwH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA1C7E72B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvElEQVR4nHWS0Y4FIQhDT/3/f+59gCJmdh2TMYClrQAY8HxZZlLYtr1KAeWXOgEWljU4zsbu3efGnQ6JHzWiEYsOAp2AJqXiUbD46XV1uBJpQ6tpSrcFxr5R+95p4V7NxqOAbb8ADrjESB5y5dOYE+HXrHZ6PEo+XM8YXhEJHNRzn0aKNmFteuy3YOHHhKWkIqcwjXKIY+bjYqLj/7vWaJxPXvvlPldvuWLtfwV/tPXD/KXLzLBl5HWaUf8BfqfNaRN5eZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70725D30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAi0lEQVR4nM3QuwkCURBG4Y9dM0sw3kwswAoWTKxBLGRBsAnbMBfsxFwQMZ0LRq64j4uZnmiYw8z8DD9miSvloDwFQdM3CxHx2SraahaT7M041/WG7gLYHpuSyoB6UWUc3qNFR+yEyzeDPVIaM6tcksf4xvVdJNj31NwNaSjQFIQUubTDn205ZNxf8wTrNScW/TxQ+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA240D0D90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABkElEQVR4nKVRvy9DYRQ99/vee6hXgnYgBh0EISZE/BgkLCY/JonBgH/CYPJXGEQiJlYRicSAxW8igli00koTefqo1/K+a2i1r61B4i5fcs895557PiBbhP8UFdApr0ZM0EJ1txYDXEwSUsi+g8jmZKPh4WZfKTt6JzqlfLO31s5VpqllQVZNs8PV0lVmRffdVRYUPwIqXBWQVvzh5Km216QCJgPpswZ5cB8LzIUGFxaTeVeZQb1MB1HLZSJxGZKUY2aGPgEwHA28F1XenbnSpgLiYyfFJacCpI3eJRPXbYWGiAGQHpyfCorX1QgXswDyTx9/WOEZvVQSZA5txJLP63ksdyexHJnv8Csub3rEp/JmSyDZtdDu14Wb2t+9OUyz15BRPzDe6ROOIc1+ilXH87JEPZNjfvIpRczCDF68EKlcQrKhubLCcRQMdp34SuTLY4j4NdxqpF1dJuMr25aVLnCr3mtqyuzH8tPl65Q3SwBgitrCPl6yol9uSTSAHPfZR09Mv+QGQAhWxT9ROvXnxjeVTZKXdk6sGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA2434F3A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(226, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABDElEQVR4nGNgGLSAVzvA54s/drn88//+/fvljCLGBGNo6jMwMFyTZmBgYM7XYETVyHL1379//7YEMjAIl/77t5IZRVLz379/79/EMzC03Pr0718zE4qxXqd/Mnz7LMveUamyM5dh8z8UnTrqzj///ft39d+/f28KNwliuHf/PwiYg80zIqsf/fv3798kXux+PfLv37+deVj8CQP6zkJYNQY9+vfv37+Ts9yxyDFv+be6esWXf/8eYzGWw4vh2vJLF1+W3VTA1Mn9718vA2vCnxRrHqyS79Qa7v3ftikQq+Sfu//e/Pv3gg9TkusTJIQ+YQ0Fl3f//v37d6EfmxwDQ+Hx55M6eAWwS1IPAAANyH17s2V/YwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA240D6220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmElEQVR4nJ2SQRIDIQgEm1T+/+XOAVHcxN2qeEEHGMAhwJDAAAQg6j6NBHkcl4zUgrUwUN6QnAZWuDB5LsdG5oYmbKMol+C0FzYrdzRmtNbbpBPVHMJq46vij57PPvQ46x3ns/8p9y719Tfr3ShxdG5a1m+6/ZulvWMFmgCW/oVs2eO15HfV6BEDc63FihpsWXZ5bQtULMIHr5mTipqT/4gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA1C7EE850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nO2OoQ2AMBREH9ShSFiCbsMGjEAQzEnHQFRVkwsKQ/oDBIPos+/ucvBjBlutIFMKEQDc1FcZC0C36LrghYjjmarffC08YYcotoxpEiCR7LKCIWYA2m/X7jkAc0wXfuX9RhsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA245C0790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABXElEQVR4nGNgQAHMDPQFjJwYInAWk6qF6IPTT/5i08eUf+ri62OJbAwMDMKirFAxmCSHqpTo56eMPAwM8osL9JhRJX/8+PT5ztWTXxhkZph7Mv9nYGBgYGCBSdqx8//9ycjIL5hk/Pv1t3+okh9ePJLXYf7DY6b86cTSGwyokhc4RAXEjNR5uL9c2HjiD5okw0mGv2bqQkx/zxU8+IfFNwrzb334+iQayXtIkt+EBRn+vvmDEGBBktTW4/z1+5uC8qe/7zAkGZ3YP7wTkPP/e+PJ+//oxjIr/Tx/9RePmqboT0YMnWxSbG/f/vj59suf1/8wJL+/kZZivP6Rgfn5O0wHyUry6Rs+kuQSXPEfU1Jfgpn1vxgXmzgD4390f/LIs/5j+M/+///hp7AQQkh+vX7r79/vnz/vanzEgAlYrBe+eLprtjkTFjkGBga5jv2pUozY5RgY2FlwyWACAKr6dswJkyeyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA243D7B20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_image(dataset_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(dataset_test, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
