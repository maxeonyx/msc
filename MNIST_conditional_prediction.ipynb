{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "# wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "# config = wandb.config\n",
    "# config.learning_rate = 0.01\n",
    "\n",
    "# callbacks += [WandbCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00]\n",
      " [ 7.0710683e-01  1.3921213e-01  2.4833918e-02  4.4166050e-03\n",
      "   7.0710677e-01  9.9026257e-01  9.9969161e-01  9.9999022e-01]\n",
      " [ 1.0000000e+00  2.7571312e-01  4.9652517e-02  8.8331243e-03\n",
      "  -4.3711388e-08  9.6123999e-01  9.9876654e-01  9.9996096e-01]\n",
      " [ 7.0710683e-01  4.0684462e-01  7.4440487e-02  1.3249470e-02\n",
      "  -7.0710677e-01  9.1349739e-01  9.9722546e-01  9.9991220e-01]\n",
      " [-8.7422777e-08  5.3005296e-01  9.9182546e-02  1.7665559e-02\n",
      "  -1.0000000e+00  8.4796453e-01  9.9506927e-01  9.9984396e-01]], shape=(5, 8), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 19:57:08.181381: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 19:57:08.732340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(mask):\n",
    "    image_mask = tf.reshape(mask, [28, 28])\n",
    "    return image_mask\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=1000):\n",
    "\n",
    "    i = tf.range(dims//2, dtype=tf.float32)\n",
    "    i = tf.expand_dims(i, -2)\n",
    "    \n",
    "    vals = tf.expand_dims(vals, -1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = vals / tf.pow(scale, 2.*i/dims)\n",
    "    \n",
    "    sin = tf.sin(rate)\n",
    "    cos = tf.cos(rate)\n",
    "    \n",
    "#     # expand dims to allow alternating concat\n",
    "#     sin = tf.expand_dims(sin, -1)\n",
    "#     cos = tf.expand_dims(cos, -1)\n",
    "    \n",
    "    encoding = tf.concat([sin, cos], axis=-1)\n",
    "    \n",
    "#     encoding = tf.reshape(encoding, [-1, dims])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(positional_encoding(tf.constant([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]), 8))\n",
    "\n",
    "def img_to_tuples(img):\n",
    "    \n",
    "    height, width = img.shape\n",
    "    length = height * width\n",
    "    vals = tf.reshape(img, [length])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1])\n",
    "    cols = tf.reshape(cols, [-1])\n",
    "    \n",
    "    # permute the order, to ensure the network uses the positional encoding and not the implicit locaiton\n",
    "    idxs = tf.range(length)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    rows = tf.gather(rows, idxs)\n",
    "    cols = tf.gather(cols, idxs)\n",
    "    vals = tf.gather(vals, idxs)\n",
    "    \n",
    "    return vals, rows, cols\n",
    "\n",
    "def random_mask():\n",
    "    idxs = tf.range(784)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "    idxs = idxs[:n]\n",
    "    return idxs_to_multihot(idxs)\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask), np.uint8)\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "positions = tf.range(-28, 28, dtype=tf.float32)\n",
    "encodings = positional_encoding(positions, 16, scale=28)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "im = ax.imshow(encodings)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 19:57:11.599176: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def make_dataset_generator(x, y, seed, typ='single pixel'):\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    # keep track of the index in the original MNIST\n",
    "    def to_dict(i, xy):\n",
    "        image, label = xy\n",
    "        data = {}\n",
    "        data['index'] = i\n",
    "        data['image'] = image\n",
    "        data['label'] = label\n",
    "        return data\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.map(to_dict)\n",
    "    \n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # add a transformation of MNIST images into val, row, col\n",
    "    def add_tuples(data):\n",
    "        data['val'], data['row'], data['col'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    dataset = dataset.map(add_tuples)\n",
    "    \n",
    "    # create a mask of random pixels masked out\n",
    "    def add_mask(data):\n",
    "        data['mask'] = random_mask()\n",
    "        return data\n",
    "    dataset = dataset.map(add_mask)\n",
    "    \n",
    "    # mask out a square region as well as random pixels\n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask()\n",
    "        data['mask'] = tf.logical_and(mask, square_mask)\n",
    "        return data\n",
    "    dataset = dataset.map(add_square_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate training pairs\n",
    "    \n",
    "    def single_pixel(data):\n",
    "        data['target_val'] = tf.cast(data['image'][data['target_row'], data['target_col']], tf.float32)\n",
    "        \n",
    "        mask_out_target_pixel = True\n",
    "        if mask_out_target_pixel:\n",
    "            target_idx = data['target_row'] * 28 + data['target_col']\n",
    "            target_mask = idxs_to_onehots(target_idx)\n",
    "            data['mask'] = tf.logical_and(data['mask'], target_mask)\n",
    "        \n",
    "        # offset positions relative to target pixel so target is at 0,0\n",
    "        data['row'] = data['row'] - tf.cast(data['target_row'], tf.float32)\n",
    "        data['col'] = data['col'] - tf.cast(data['target_col'], tf.float32)\n",
    "        \n",
    "        return (data, data['target_val'])\n",
    "    \n",
    "    def single_pixel_random_rowcol(data):\n",
    "        data['target_row']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        data['target_col']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        \n",
    "        return single_pixel(data)\n",
    "        \n",
    "    def many_single_pixels(data):\n",
    "        rows = tf.range(28)\n",
    "        cols = tf.range(28)\n",
    "        cols, rows = tf.meshgrid(rows, cols)\n",
    "        \n",
    "        rows = tf.reshape(rows, [-1])\n",
    "        cols = tf.reshape(cols, [-1])\n",
    "        \n",
    "        image = data['image']\n",
    "        val = data['val']\n",
    "        row = data['row']\n",
    "        col = data['col']\n",
    "        mask = data['mask']\n",
    "        label = data['label']\n",
    "        index = data['index']\n",
    "        \n",
    "        def data_plus_pixel_index(i):\n",
    "            new_datum = {}\n",
    "            new_datum['pix_index'] = i\n",
    "            new_datum['target_row'] = rows[i]\n",
    "            new_datum['target_col'] = cols[i]\n",
    "            return new_datum\n",
    "        \n",
    "        def add_original(new_datum):\n",
    "            \n",
    "            new_datum['index'] = index\n",
    "            new_datum['val'] = val\n",
    "            new_datum['row'] = row\n",
    "            new_datum['col'] = col\n",
    "            new_datum['image'] = image\n",
    "            new_datum['mask'] = mask\n",
    "            new_datum['label'] = label\n",
    "            \n",
    "            return new_datum\n",
    "        \n",
    "        d = tf.data.Dataset.range(784)\n",
    "        d = d.map(data_plus_pixel_index)\n",
    "        d = d.map(add_original)\n",
    "        d = d.map(single_pixel)\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    # single pixel example. the row & col are translated by a random\n",
    "    # amount and the target val is the new pixel at 0,0\n",
    "    if typ == 'single pixel':\n",
    "        dataset = dataset.map(single_pixel_random_rowcol)\n",
    "    \n",
    "    # 'many single pixels' generates 784 single pixels from each image,\n",
    "    # and the target vals are each pixel in turn, translated so that\n",
    "    # they are at 0,0\n",
    "    elif typ == 'many single pixels':\n",
    "        dataset = dataset.interleave(many_single_pixels, block_length=784)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "def make_datasets():\n",
    "\n",
    "    train = make_dataset_generator(x_train, y_train, seed=192_168_1_1)\n",
    "    test = make_dataset_generator(x_test, y_test, seed=10_1_1_1, typ='many single pixels')\n",
    "    \n",
    "    return train, iter(test)\n",
    "\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 441 which is a 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0UlEQVR4nGNgGLrAbv//v5382OXK3/w5/+bPAmxS7H2vHjcx6Jx6LIhFcsGfx0YMDAxRf8Qw5bL+/vFnYGBgcPvjDRNigktyMTF+Y2BgYOBmLMbUqfXiTxong77+kz8OWOw0//PnyI6/d/88FsA0luGc+5J/H67JM+z9gM0vDAycTGF/7ipjl2NgYPj/dweCw4QqJ/bv+yqcGif9XYdTzufH3xgkLqqxLizfruHSaPjyrykuOc6jf57jtLHpz58snJIL/2xgQRFAdVDHH5w6SQMA2U5FIASOiNsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nFWSS5ZFMQgCi5ze/5arB2JyXwY5+SgCGgkIxCBAMEZIpCs26q4cSNJwhPQDWKhJ5edWbPhCtlIk3hu9TFyAkwyxLWW/ZDJvtUXa90NpFUlmM4QDyRwv1friJZN9f0LgDJeoioQ8/LJ3t+edKduvAVn3F5xfSRt3YupfxsSUnKuztQtwe3PG9PEhPq55+sanHQjK8nz+GJVr5c9UCKLonErooTqlmru6fsxYvX/mKfn0QeJKWWnOJA78p2Vu5+5g7VDfKf8mlO3ME1lfO2//KvyTIyqLrQEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5DC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAp0lEQVR4nGNgGLrAjuH/305ckm/+MLz5swCbDDsDw+MmBp1Tj7FrfGzEwMAQ9UcMt71uf7wxBUv+/4Uw/u7DlNRi+JPGwKCv/+SPAxbzzP/8OcLw9y7DYwEskqzODIdWMDBg9woDAwMDE8MfBmWcboW5CqoUFTB+x6mPYdJf3HIMP/DITWD4bIRT8iVuUzkZGJ7jNvXPH1Q+qle2sODUuPCPBW5TSQMA92Yvero8RbwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5D90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 811 which is a 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgGHDQOO3c/2+B7NglY57//fv3bykOrRziE3/9/ciHy2Spt39/2eOQk7709+8+XPqu/P371xmrFLvPxb9/vyWxYpFist/89+/fV+ZYZBLbrv39+/fvY1ks2jr+QsBHXSQNSPL/5r1lYODp4cJirD4rKwOLw8m/fyVxeISBIfLv3wxUY40qOZHknVEldVqioFzeGAaGa6hmxf39GsDOwMDAwLn779/vSqiS8rf//l2mwsAUefrv36+N6M5QvvH379tbt//+/fsrDIsrJ0CCYIstNi+w+hx7MnWqMyMOH9IPAAClymFpmfxdhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5B50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nGNgQAH/sTKJAdiVYxVlIt10ktXj0onHHFxS5FuNW/t/bOLkBz5FuogyDqfRRNqJx7fo1pDlC2TT/mOVIMUk7D4HAFulHuQApo8XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D78480100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGElEQVR4nGNgGAWjADv4S0WzblDRrIECALVrAdaboimnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7E38C95F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix_index shape ()\n",
      "pix_index dtype <dtype: 'int64'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 945 which is a 2\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nMWRoYsCYRDF3y0G2y1o3002xWYRwWayypVVvOwfYDKIRv0DRDCKQbSY7hCuL4jxODiLcP2K8Hi7hpM9l9svXBCnzMf8Zt684QPuF8XWWyhJ4aD4F/ZIkaT4MYqKVvRar36yU49mU5d8KL0jA2CTg1PYGXa3SHomYxPp1YDSrsi5AY4p0klm7pH6dFKJLOuT6hpEt5JihV+NbC0fnIaGfb7IjoHtqe/Of7w8XLxUACsA8AwAs3jPgtGXkeynY9B+uYaa9uwrWdiP7XIIoBoAsAIsGwnOmp73RZFPyUfdPs5Mcm8PHYVM7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F55B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAK0lEQVR4nGNgIBr8p6oMeYDa5o2CAQFYohFrzOKIbuzChNLGfzQahxLc0gCYWwz0kYjtegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F55B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGElEQVR4nGNgGAWjgN7gL6bQXvq7glQAAC9wAbua0VaoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F57F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 945 which is a 2\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nMWRoYsCYRDF3y0G2y1o3002xWYRwWayypVVvOwfYDKIRv0DRDCKQbSY7hCuL4jxODiLcP2K8Hi7hpM9l9svXBCnzMf8Zt684QPuF8XWWyhJ4aD4F/ZIkaT4MYqKVvRar36yU49mU5d8KL0jA2CTg1PYGXa3SHomYxPp1YDSrsi5AY4p0klm7pH6dFKJLOuT6hpEt5JihV+NbC0fnIaGfb7IjoHtqe/Of7w8XLxUACsA8AwAs3jPgtGXkeynY9B+uYaa9uwrWdiP7XIIoBoAsAIsGwnOmp73RZFPyUfdPs5Mcm8PHYVM7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5CA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAK0lEQVR4nGNgIBr8p6oMeYDa5o2CAQFYohFrzOKIbuzChNLGfzQahxLc0gCYWwz0kYjtegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D78480100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGElEQVR4nGNgGAWjgN7gL6bQXvq7glQAAC9wAbua0VaoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 945 which is a 2\n",
      "pix_idx: 782\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nMWRoYsCYRDF3y0G2y1o3002xWYRwWayypVVvOwfYDKIRv0DRDCKQbSY7hCuL4jxODiLcP2K8Hi7hpM9l9svXBCnzMf8Zt684QPuF8XWWyhJ4aD4F/ZIkaT4MYqKVvRar36yU49mU5d8KL0jA2CTg1PYGXa3SHomYxPp1YDSrsi5AY4p0klm7pH6dFKJLOuT6hpEt5JihV+NbC0fnIaGfb7IjoHtqe/Of7w8XLxUACsA8AwAs3jPgtGXkeynY9B+uYaa9uwrWdiP7XIIoBoAsAIsGwnOmp73RZFPyUfdPs5Mcm8PHYVM7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F52E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAK0lEQVR4nGNgIBr8p6oMeYDa5o2CAQFYohFrzOKIbuzChNLGfzQahxLc0gCYWwz0kYjtegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F52E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGElEQVR4nGNgGAWjgN7gL6bQXvq7glQAAC9wAbua0VaoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 945 which is a 2\n",
      "pix_idx: 783\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nMWRoYsCYRDF3y0G2y1o3002xWYRwWayypVVvOwfYDKIRv0DRDCKQbSY7hCuL4jxODiLcP2K8Hi7hpM9l9svXBCnzMf8Zt684QPuF8XWWyhJ4aD4F/ZIkaT4MYqKVvRar36yU49mU5d8KL0jA2CTg1PYGXa3SHomYxPp1YDSrsi5AY4p0klm7pH6dFKJLOuT6hpEt5JihV+NbC0fnIaGfb7IjoHtqe/Of7w8XLxUACsA8AwAs3jPgtGXkeynY9B+uYaa9uwrWdiP7XIIoBoAsAIsGwnOmp73RZFPyUfdPs5Mcm8PHYVM7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F57F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAK0lEQVR4nGNgIBr8p6oMeYDa5o2CAQFYohFrzOKIbuzChNLGfzQahxLc0gCYWwz0kYjtegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F57F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGElEQVR4nGNgGAWjgN7gL6bQXvq7glQAAC9wAbua0VaoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5D90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 309 which is a 3\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGDCgM/fjv3///t8PY0YWZWRgYGBg4D/ztv8zg5F8JNdO3z/oGuc28DIwMDAwSN3/Z4lhqjCUZtqHRRIGxP694sYpOfVfGU654H93OXHJOf4+w48iwITEFmG88BGnqQzL/k3DLcnQ+2enP25Zq1tfeqVxyrKv/PfRGacsR8ePe4q4jc75twfKYsKUfIZTG5vr5p//pqMJciswMDAw6Da++PfvYxIsxhmh9ArliwwSVtysDMfWrYCbywKlr4UZMzCcenLswYZ/uJ1KDwAASOtAgAfWMOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqUlEQVR4nHWSSRLDQAgDm1T+/2XlwCbsyhw8wyaBDIAA5VeA3RNLh6TM64L0u2fPoiJuaBOer3eS18pv6TI+EuUF1p21lY8ZuSRgph2WEqQYJ9yiVNGAV5GW7jZfEE2eRkOwhU2HDOYqcChedoKPW5/NbLAIWYOSBuWh+trGk2N8IRAEKCoS3PNnD1rJbtbwV+T9S9KFkIPUfp5FHAHv3FcEq++RfDdHkh8FTNc1PdS2HwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D78480100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAaklEQVR4nGNgGDCgQ4yif9uxCDZA6fsMljg1MuE3Fq/aMpz6ghnu4jOWH4/cHLwOmoZP8g+DPx7ZW/i0sq/899EZpyxHBwODIl534QH/cMq4bv6JS6oRq+gKSPD8w27kP4Z/DCdIcxtNAACEWhaP72busgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5E80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 309 which is a 3\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGDCgM/fjv3///t8PY0YWZWRgYGBg4D/ztv8zg5F8JNdO3z/oGuc28DIwMDAwSN3/Z4lhqjCUZtqHRRIGxP694sYpOfVfGU654H93OXHJOf4+w48iwITEFmG88BGnqQzL/k3DLcnQ+2enP25Zq1tfeqVxyrKv/PfRGacsR8ePe4q4jc75twfKYsKUfIZTG5vr5p//pqMJciswMDAw6Da++PfvYxIsxhmh9ArliwwSVtysDMfWrYCbywKlr4UZMzCcenLswYZ/uJ1KDwAASOtAgAfWMOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F5580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqUlEQVR4nHWSSRLDQAgDRSr//3LnwCbsyhw8wyaBjJCQRH6RZPfE0gFkXhek3z17FlXohjbh+XoneS1+w2V8JOIF1p21lY8ZuSTQTDssJUgxTrhFqaIBryKW7jZfEE2eRkNoC5tOGMxV4FC87AQfN5/NbLAIrEFgUB6qr208OcZXCiGFRFQkdM+fPWglu1nDX5H3L8GFwEFqP88ijoB37iuC1fdIvpsjyQ8tTtg09rs/xwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D78480100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAaklEQVR4nGNgGDCgQ4yif9uxCDZA6fsMljg1MuE3Fq/aMpz6ghnu4jOWH4/cHLwOmoZP8g+DPx7ZW/i0sq/899EZpyxHBwODIl534QH/cMq4bv6JS6oRq+gKSPD8w27kP4Z/DCdIcxtNAACEWhaP72busgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F50A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shape_summary(data):\n",
    "    for name, v in data.items():\n",
    "        print(name, \"shape\", v.shape)\n",
    "        print(name, \"dtype\", v.dtype)\n",
    "\n",
    "def el_summary(data):\n",
    "    print(\"index\", data[\"index\"].numpy(), \"which is a\", data[\"label\"].numpy())\n",
    "    if 'pix_index' in data:\n",
    "        print(\"pix_idx:\", data[\"pix_index\"].numpy())\n",
    "    display_uint8_image(data[\"image\"])\n",
    "    display_mask(data[\"mask\"])\n",
    "    display_uint8_image(tf.reshape(data[\"image\"], [28, 28]) * tf.cast(mask_to_image_mask(data[\"mask\"]), tf.uint8))\n",
    "\n",
    "def train_summary(d):\n",
    "    data, target = next(iter(d))\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(iter(d))\n",
    "    el_summary(data)\n",
    "\n",
    "def test_summary(d):\n",
    "    data, target = next(d)\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    for i in range(780):\n",
    "        next(d)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "\n",
    "train_summary(dataset_train)\n",
    "\n",
    "\n",
    "# TODO: TEST DATASET GENERATOR DOES NOT WORK HOW I EXPECT.\n",
    "#       IT SHOULD PRODUCE 784 EXAMPLES with the SAME image and mask, then change\n",
    "#       to a different image and mask.\n",
    "\n",
    "test_summary(dataset_test)\n",
    "\n",
    "# reset datasets after summary, because it consumes elements\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    if dropout_rate is not None:\n",
    "        dropout1 = layers.Dropout(dropout_rate)\n",
    "        dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        m = attn(inputs, inputs, attention_mask=mask)\n",
    "        if dropout_rate is not None:\n",
    "            m = dropout1(m)\n",
    "        attn_output = m\n",
    "        if include_residual:\n",
    "            m = inputs + m\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "        m = m * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        m = layernorm1(m)\n",
    "        m = dense_net_1(m)\n",
    "        m = dense_net_2(m)\n",
    "        if dropout_rate is not None:\n",
    "            m = dropout2(m)\n",
    "        dense_output = m\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 20\n",
    "    pointwise_feedforward_dim = 200\n",
    "\n",
    "    val = keras.Input(shape=[784], name='val', batch_size=batch_size)\n",
    "    row = keras.Input(shape=[784], name='row', batch_size=batch_size)\n",
    "    col = keras.Input(shape=[784], name='col', batch_size=batch_size)\n",
    "    mask = keras.Input(shape=[784], name='mask', batch_size=batch_size, dtype=tf.bool)\n",
    "    \n",
    "    print(val.shape)\n",
    "    print(row.shape)\n",
    "    print(col.shape)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    row_pos_enc = positional_encoding(row, n_embd//2)\n",
    "    col_pos_enc = positional_encoding(col, n_embd//2)\n",
    "    \n",
    "    print(row_pos_enc.shape)\n",
    "    print(col_pos_enc.shape)\n",
    "    \n",
    "    pos_enc = tf.concat([row_pos_enc, col_pos_enc], axis=-1)\n",
    "    print(pos_enc.shape)\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    \n",
    "    m = tf.expand_dims(val, -1)\n",
    "#     m = tf.stack([val, row, col], axis=-1)\n",
    "\n",
    "    m = layers.Dense(pointwise_feedforward_dim, activation='relu')(m)\n",
    "    m = layers.Dense(n_embd, activation=None)(m)\n",
    "    \n",
    "#     print(m.shape)\n",
    "    \n",
    "    m = m + pos_enc\n",
    "    \n",
    "    m = m * tf.expand_dims(tf.cast(mask, tf.float32), axis=-1)\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=32, n_dense_dim=pointwise_feedforward_dim, dropout_rate=None)(m, masks=mask, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=32, n_dense_dim=pointwise_feedforward_dim, dropout_rate=None)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=None)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=None)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=1, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    \n",
    "    m = layers.Flatten()(m)\n",
    "    \n",
    "    m = layers.Dense(200, activation='relu')(m)\n",
    "    m = layers.Dense(1, activation=None)(m)\n",
    "    \n",
    "    target_val = layers.Reshape([], name='target_val')(m)\n",
    "    \n",
    "    model = keras.Model(inputs=[val, row, col, mask], outputs=[target_val])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784, 10)\n",
      "(8, 784, 10)\n",
      "(8, 784, 20)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "row (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (8, 784, 1)          0           row[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (8, 784, 1)          0           col[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "val (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (8, 784, 5)          0           tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_1 (TFOpLambda)  (8, 784, 5)          0           tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (8, 784, 1)          0           val[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin (TFOpLambda)        (8, 784, 5)          0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos (TFOpLambda)        (8, 784, 5)          0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_1 (TFOpLambda)      (8, 784, 5)          0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_1 (TFOpLambda)      (8, 784, 5)          0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (8, 784, 200)        400         tf.expand_dims_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (8, 784, 10)         0           tf.math.sin[0][0]                \n",
      "                                                                 tf.math.cos[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (8, 784, 10)         0           tf.math.sin_1[0][0]              \n",
      "                                                                 tf.math.cos_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (8, 784, 20)         4020        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (8, 784, 20)         0           tf.concat[0][0]                  \n",
      "                                                                 tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast (TFOpLambda)            (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (8, 784, 20)         0           dense_1[0][0]                    \n",
      "                                                                 tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (8, 784, 1)          0           tf.cast[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (8, 784, 20)         0           tf.__operators__.add[0][0]       \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and (TFOpLambda (8, 784, 784)        0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (8, 784, 20)         53140       tf.math.multiply[0][0]           \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "                                                                 tf.math.logical_and[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_1 (TFOpLambda)          (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (8, 784, 20)         0           tf.math.multiply[0][0]           \n",
      "                                                                 multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (8, 784, 1)          0           tf.cast_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (8, 784, 20)         0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 tf.expand_dims_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (8, 784, 20)         40          tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (8, 784, 200)        4200        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (8, 784, 20)         4020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (8, 784, 20)         0           multi_head_attention[0][0]       \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (8, 784, 20)         40          tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_1 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_2[0][0] \n",
      "                                                                 tf.__operators__.getitem_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (8, 784, 20)         53140       layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "                                                                 tf.math.logical_and_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_2 (TFOpLambda)          (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (8, 784, 20)         0           layer_normalization_1[0][0]      \n",
      "                                                                 multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (8, 784, 1)          0           tf.cast_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (8, 784, 20)         0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (8, 784, 20)         40          tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (8, 784, 200)        4200        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (8, 784, 20)         4020        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (8, 784, 20)         0           multi_head_attention_1[0][0]     \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (8, 784, 20)         40          tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (8, 15680)           0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (8, 200)             3136200     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (8, 1)               201         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "target_val (Reshape)            (8,)                 0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,263,701\n",
      "Trainable params: 3,263,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, n=1):\n",
    "    \n",
    "    dataset = dataset.batch(n)\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        outputs = txformer(batch)\n",
    "        for i in range(n):\n",
    "            print(\"index\", batch[\"index\"][i], \"which is a\", batch[\"label\"][i])\n",
    "            display_uint8_image(batch[\"image\"][i])\n",
    "            display_mask(batch[\"mask\"][i])\n",
    "            display_uint8_image(tf.reshape(batch[\"image\"][i], [28, 28]) * tf.cast(mask_to_image_mask(batch[\"mask\"][i]), tf.uint8))\n",
    "            display_float32_image(outputs[i])\n",
    "\n",
    "def gen_image_many_pixels(dataset):\n",
    "    \n",
    "    # assume dataset is a 'many single pixel dataset'\n",
    "    # so it has runs of 784 examples, one for each pixel in an mnist digit\n",
    "#     dataset = dataset.take(784)\n",
    "#     batch_size = 32\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     reconstructed_image = np.zeros([28, 28])\n",
    "#     for batch, batch_targ in dataset:\n",
    "#         inputs = [batch['val'], batch['row'], batch['col'], batch['mask']]\n",
    "#         out_vals = txformer(inputs)\n",
    "        \n",
    "#         # np can do this yay\n",
    "#         reconstructed_image[batch['target_row'], batch['target_col']] = out_vals\n",
    "\n",
    "    reconstructed_image = np.ones([28, 28]) * 230\n",
    "    for row in range(28):\n",
    "        for col in range(28):\n",
    "            data, targ = next(dataset)\n",
    "            inputs = [data['val'], data['row'], data['col'], data['mask']]\n",
    "            inputs = [tf.expand_dims(x, 0) for x in inputs]\n",
    "            \n",
    "            out_vals = txformer(inputs)\n",
    "            reconstructed_image[data['target_row'], data['target_col']] = out_vals\n",
    "    \n",
    "    reconstructed_image = np.clip(reconstructed_image, 0, 255)\n",
    "    \n",
    "    image = data['image']\n",
    "    mask = data['mask']\n",
    "    \n",
    "    print(\"index\", data[\"index\"], \"which is a\", data[\"label\"])\n",
    "    display_uint8_image(image)\n",
    "    display_mask(mask)\n",
    "    display_uint8_image(tf.reshape(image, [28, 28]) * tf.cast(mask_to_image_mask(mask), tf.uint8))\n",
    "    display_float32_image(reconstructed_image)\n",
    "        \n",
    "        \n",
    "def image_performance_test(n=5):\n",
    "    for i in range(n):\n",
    "        gen_image_many_pixels(dataset_test)\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10000)\n",
    "    \n",
    "    global callbacks\n",
    "    callbacks += []\n",
    "    \n",
    "    txformer.fit(dataset, epochs=1, steps_per_epoch=5000, batch_size=batch_size, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clarkemaxw/conditional-mnist/env/lib/python3.8/site-packages/keras/engine/functional.py:582: UserWarning: Input dict contained keys ['index', 'image', 'label', 'target_row', 'target_col', 'target_val'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 504s 100ms/step - loss: 4168.1875\n"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(110, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEklEQVR4nGNgoD9gROMb+QY9vNL8HZtS003f//79+285GxY5q9f/bmT7Ff/764cpp/32bzMXA4MpNkmO7X+7GRgY5G/+OyGEKXnunxUDQ9q/f7fksFi5/q+Tcu3HvycsmaECLCjScxllGRh+Rj56iqmz6+/ff193Pu/iwGIqg/vXv//+HpDGJsXAoLnk3/93mA5lYGBgYMj5+Pf8yb+PJLDJpX38e9hA8+PfSZhS7DP+/SrhZGDw+PvLDENS4++/SgYGBgaRv3+tMCSP/rvFz8DAwKD974suupzzr78uDAwMHOkv/tYgRKEhxMPMwC1kG6lq+H91F4apKo///nv39+/fA7nY4ln/2b+/99Ix3UJ3AAAdi2nuruWYVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444FCB80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2klEQVR4nF2SSQ4EMQgDy6j//+WaA5B0zyUiYrONI5EYAQISgwDEAOfDKQ343ETslpy67lK1y+xBorOL1xa7FUwAF08XTW76UQGdyM1VFqcEEu5yS+Kga5oTS/527urensIvmtyYMpChnJEQUKBiwDTwfSEkX5X9CPzi2hSaxvz9xnrHAzWDXNHEgScVL/GBQuYm9SIvpIWcycU1wtJrdwRqpU3IeGieUEJrJgvLOQDPTl1UWZGEOrIGDbHdQXZsgw/gGQmmHXUv/TL3XMWstVYLhaS6ndybX01+b2W+76G1TkkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4450C790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nMXPMWoCURDG8T8KslbCVoJgitiEEAiCQrb0AHqLtTA5gJATWFrlCFvnBBEbEQzY6RYaCKhNAoLbLMzEYgMhvNk2mWbm8eOb9x78fzVZPudQC0RyLEBXA1ATP7K40nXIy9rFmpnv4qsCoWpct9ZK5xJkdlc0L92+AZNxzaARogn7kWcGE1F5MQVAv34dCz/jPbKcQ9VKhUemt1fI2LAnTSkDQttF0eH3EBhviSsA15xuHEuz1j/Io7GVnk+0QKKSY4130U8QHqyPsEM2fVP+ts7Y10WNzK/YtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44329C70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjklEQVR4nI2SSxLAIAhDE8d9739auigRVGzLwqkJn2cHXlBQH6az7V5EBwDapBEAjI9Jr6K5roIW/QhyHhAzlUIoaTeR5u+0jIS+vEBTDU4r1fNoOnpOX6OVqos7bYI+mnNl0fxY6eaJ9bsy6ApzlceW/Gz702Q2y187gJxAYAHYx5VI7rOsL0BWmr7HxA0SLRtk2OZG4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7F587D94C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(1000, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuUlEQVR4nGNgGKZAM+LV/39zcEje/vfv379fWTAuE7LcVnkGBgYGFlaskhZ/M7SWIPFZkNga7LtmMUnjsHHhv08alf/+/evDpvMRA881BgYGhofY7JzxFYeRDAwMDAwVh9+kPP13nR+HtFj/v3+xuPQGfPm3gQmHnMDVf18NcRn66t+feFyGbvz3LxyX3JS/t9RxWMhU9PejAy59rv/+eeGSE/74byILLsn4fzMZcckxrLwhgVMOKwAA9HE8stYeCxYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D784C1E50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0ElEQVR4nF2SWw7EMAgDx1Hvf+XZDyCPrVSlSsB2JyCI1GIvIGAkGKHfUC+EDyQC9J61gGDJ0Du13Q7X5z4dVy+Jq7H6XLSCuXQJSFYcv/A8kWVSdU+fKU/H+TYtr9EqGjvuZnKn6LNUvG+KLtlgALJCTDaRUXVXn1BDCBQWPn+x05HC2HUPeRFWTt2RLUNjBuuFz+Y3N3R7O7J23x/ciHxshevMmPbsuckEyYxV6KHzAViJVulj7MgJmNDgT+xG1/NrbNG5mczNTqmHtQ5R4AeMm8a0+VkYuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7F597F16D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAOUlEQVR4nGNgGLkgizbG5lOuk4lUnfxk2zlIABPD34/45L1wynz8x8CCU/LfTEaccitvSBDhMCQAAHiDCLjGra7QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7F58768B50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgElEQVR4nJWRQQ7EIAwDx1XO+//feg8t2wRC1fUJxXI8gD4MCQsAj8lBkqiKxtBIRxMAkOe1c8XRBwGhbRLjqJMfy0mrybpYQBVoKe861Zgr9Z72tanpcV8nK40ek2fJbq2z6Wq5rs3uON9XeaBtFevoLsidtmt51ECF/vNXEvYXeTsWWC2Q5VkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7E38C95CD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(52, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8ElEQVR4nGNgGCKA09eIgYEFU5zJ1sLEnfH6jHNo4oaiDBLlZ/79f9yQzY4mJbf734vrr/59nO8lhGGc0M1///79vpuphMUJQsf//fu5yAm7+3b/+3dEHofbLT/8W8yEQ47h3r+FrOhiMH8qifxaLespHXJ9zd5nGBpL//34+Pnf6/sf/r1Jw5A89u/fyQlWYgzyfb9eZmB4ZHooP4TV8m89Iy6HMeT8+8eJS07x4r8NuHQ6PPn3UherDF/Stl//nvvD+VATZP4z+PApuSswfJrX9wRdz6efv/5+//pxR4ocsijMbj1mgYef3uD0AlUBANfMW+hirJD8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D285A96A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAeUlEQVR4nJ2SUQrAIAxDX2T3v3L2YXU6WAYWVNqQtNTIyEC/BYaeqmoTq2SG34UFap8Q0JTAb9UcbgRqlM3TEsb9YaaBIvN8Q3l94csi9Xx9sae8+mq6Txi4ds9Zj5x6T9UZqAA0HL0/YjUze193WG6A0JAT2DWWdQPW6i8evefWwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D285A8F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAN0lEQVR4nGNgGAWjYKAAI4SS+c/gw6fkrsTA0Nf3BF3Np5+//n7/+pEhBasJegyOSiI0dSMCAACKdAvKDLH1/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44378B50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGPkZcANmPDIjUqOSo4wSQCFKwBF75j+awAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444F54C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(339, shape=(), dtype=int64) which is a tf.Tensor(6, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4ElEQVR4nGNgoAfgVOte/f/fY00IjwVFjnV6LAPDwY8iftcx9TEt+Pt7uwIbAycbFjMX/n0UgcM+9QV/f9jhckzF37/auOQE9v9t4cAlWf73Bh8uOeaDf1NwyTHo/L2NIcaEYK7GJ4kJ4MH37aNMYDgDAwPDs+mY5vt+/AsFrzvR5Rx+/v37uFyYi8tp348/fayokkWf+r6/62BjYGBgcLr91x1VUsyfIe7v31wGBgYGhva/0zBsZYz99ndLmqKi4rS/f1gxZBmsNkPd9JAFU5KB2Xbiyb9/D5eJYZEbDAAAXVBVx7aiE48AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4450C790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzklEQVR4nFWSSxYEIQgDK76+/5VrFnwce2ErSAKJILgLtUNQYoQIwSBhPnfVrVkUAxgJwq4LooPwYgHHUKhMQ2ruJSdVbB3XomvE4Rpqw+YrQLon8KNzVpekCyV8cyszXMEVQI9/lWtREM5gDmeLiehnKbRAA5AtYKFXsD1vqCxYIc7j0qiWrKdr2J+CViUAIaVP9Zx19zp8uykvsI1odyyP5Ln5nHu0Y8ttvO9hmx+3R+hL1Y+E67ntp5HzPC4FQ4KY/I+x/2W/orJ0Q/oDBE3rb2QPJHEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425A9D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqElEQVR4nM3SsQ4BQRDG8X8cl1CISn8FCl6BwlNoqESnPq8gkiuvp5IoFUoPolRKlELkC83R7M6WYqrd/WVmNpmBn0R7tXuZKABSPz4PxFR9soaRUbEDDKx2C6lrWeNoCYCoB3QasJNNPblvpe8pCv2HDWyBrOVDgQS6LB16wDmFGtzJKm7qlRhg6EATJjAvbrmbOb6JGUmSg6cuexXjLnsw6n924T/jDdl0I04BHJ+CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44319130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nMVSOxbAIAgLPube/7bpgLZSPh06lE1CYoLKAUFWBDBSZJbmPEBAaM2TXvYPcHc7UzFhiviZQlY6kB0YZbcdB1m3/+VNI2SN1/c0kKTr0hyP+xywPue1Wx+lZmKFKe58GAK5RohpXj3BCXz4JifPnBpM8GrKkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7F597F16D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(266, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEUlEQVR4nMWPr0tDYRSGH2RiUBgoKEzUhYHFYvBHc8E4YcPhBP+ABU0WBZNlYWgyrahlBhWTigPTyvwHbhCrrgibTnQovN9nkF3m/W7Wt5zDeTjnfQ/8u5IVUxgKRwOVtpHuRsJY9Eo6rkkzYbCkzzwbUrkz6PHR4EmWvRJAqzOK+OwoBXUYhUMHLqQAhpc3u3wiXT1nidspC2v1Rvt3mowkSUaSlA4E8h5/am2njKtFY4wxhT4omrTjaaG6fwnMWyfQNFyvvgPjMedq7F4PUQCqfiB/cy7B1yswNjlB6zmwmZE8YLYoNXPBs+eS17t1YaWXFcdzV2qcSlZPWffNpTdJ0s16v8sgfiB9bCfD0F/qGwpDb2TyvVHNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4433BD00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAXElEQVR4nJ1R2QoAIAjb///0eghk2gw06NAdVoIAgFjvQXIueEJNUdw6SuvGKjOFMlbpNEaUGbTukraSp/GLJs4AM8+ajoWaaVu4fVg9tBKVZo6UQmCKEkr9jcAPFvg6xgdWbGkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44319130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAIUlEQVR4nGNgGATgH26p73i0/YUTOORwglaCKkbBKEAHAIW+Bm+4k7WFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7F587D94C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgklEQVR4nJVSOQ7AIAyzEVP//193aBVygVqWQCwb5+AFABBRj8YTCUAFHetayBw5E5hejIk+/EtkQIusdiAB0FFn9Uojn92ewNnkrF2fZLvGt7JWbC/LE5j+VJwmI7NzNHtInqkOg9uhvaEfYKyzriWw2qeXILhphzqVzK2pEOQT7dyQNhNaeIiyzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4433BD00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 503s 101ms/step - loss: 2989.9995\n"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(458, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nNXQMUsCcRgG8OcOozTUvMipUHSx4SY/QaPgKDr4ARwU2gP7DC4O0RiCg7WFu6REkCAKrk2JdsahaWo+R4NLd74NjT3r7334v+8f+Lc5LQ8sa1A+3BYldjHhfPrUXj94XTsOPCfbt0cA0szc1O0WMljzAAAOaLFrxzO++AEAWpHWq27H3RorAOBvcXYZcC6U4SwFxBtcpLa3dV9zqRfmfD6RrtRa/DDZC0oGhElWfprqGBi/yUVcsWkyIVvE7O+V2JAxySpQX2flNxW3ijs1JzaDBn2IvhvHUnNUUvJYLLV9sRriVC+y4xHRe//1uRoKP7vJI0fZ3+xP+QYsLF/zx8NxRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444C7490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAcklEQVR4nK2SQRLAIAwCwf//OT04RkPFdjr1pisEExkwi4F2YHtIAAEDRylrm5DVMfc0abnUpMLotlwjaM15fpNnWrLKuUI1rk1IOt0Zm1rj+lOHnBBo7o3Am8Z/gnUqMqOqlGz/BLIjE9C37g+dlABAXoFLFTYk/1QXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444C7D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvElEQVR4nGNgGLJAc+rzf/+eTxXGlGHUqPr099vnU+f+HOFlYUWTzP/799xaUQYGhrC/4Yu3o8rJv/nLwMXAwMDAIPD339/L6OY+4GdgYGBgEKr9+++ZLqoUO8PfpQwMDAz8x/5+rRNE1xj+92sIA4Pxob8MIcT6bmgCn38bGRhS/x3CKin25i8fg/K7NzJwESaE5KsJjNkMP34KcWPVKv/3s27t34tc2G3d+vv7r5c4Q/bE31fRRDidMAAAmtVBJ4/0o9sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444C7DC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACH0lEQVR4nI2SO2hUURCG/zn33Lt3X9mgskFJYgxRELUw+AgSQsRXLGMT+9QGxMrC0speO23SaxEbiwhCJILgA4sYQnR12TzIY5N93L1n75mx2N2blE418PHP/DP8hLgIQgQR/F/R4YZAIgQh8GHoOAQ45KHJUE0iYwFAA1Ag7aeg2cm5u82QyKlY2I5SUSrjpTJ+uUcnbbTTrRcCNm0loJStqbGMSo8tTn//8OBRKgoRQ6HQyV6eRq06+Kp2rTzV+FpqShuKhH5+ZArF19srSz4mes68fxx0ICmVn3iY2Zz9VNwzxzLoWntrpDPWSdPRG/2o/90NODd8H1vRsiEBFACIlbV5lOYMp/OjT/p/PZte51jJkUd2r3Zhtbdyeqav8HRx0yiOd0JHhReXBu590XcGSi+XqsqF7SgVDP1YX5scHwUQrhZFKSe+k7TnVhuJoXHg97c3y8ye1fUYKmPA5QBozL9bKWuVpH1B220UNSN2cllw8UqyaqNwY7sSuwX5JP7QRGmufvZ4OuCGMXzwWwYcty83+zGo3PxZDiwI8ViI5Qjh1lA14t6TPoO0Sx0orhaxG4XzF0906bsDKeV6iRiSJOCqPwvm9rnJwf5TR9LaBrEhCXfc7ixzd35EGnRrwdGthFHrTvKy6eywf/2q7D//XDDZSr15kD5SXkIinUxFgWkaIRE+lFvlEAuUYghYqJV71WbEAEgYQtCOamn+AdjZ9gz2K9TAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44418BE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(664, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABFElEQVR4nGNgGHDApT919/8tVgLY5LQv/v379+/fv9edMOV8fvx9cHPdm79//x6TRZfjef4nm4+BwbDL5/Hfdy5okgV/U6AsvhV/pzIwMDAhSaq+3wllfTr94Rqazvv7YSyFe/sY0HTCFTvOlZ+P7qCOm4IMDAwMAtu//irkZ2BgYGBBkrzNY7yHQY5lqaA6ywMMb3Ld/sEQsLZ3LT+GDAMDA0Pi959H9NU44HxkY7WE/rJtvogkgCRp0i37hZOB8T82M41f/L257O9fVSxSyic/vu81Zwz8m4spF/Pr7xZJBgaWe1gkI769tWdmYJA99fu7HYaD7NhXXjTyURA3KttyC0PntL+v7/z6+/fvOWwupS8AAGM2YxnLnsJUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4464DFA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbklEQVR4nI2QWw4AIQgDvf+lZz905VkiiRqkxcG1dnCWxZSFAk5gMgge5edoi5i7PQZnI5Pjml5NcSrCkcBaedJguYVOMDWnJvJ3bbJ+NCuwhGZCCuhZ0M9AP/D/fiWNZvKFgNlH6USr08SJT6B9iHJVq0Pm7bAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4450C790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAPklEQVR4nGNgGARg6m4GBiusMtqUG/73HaZYCuXG4gGOWEUFKTFyLZn6/lJiKTZwkuE9Rfp/k6WL6t4gCwAAlu8JsY80nWkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44221C10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACLElEQVR4nDWSXWtUVwBF1z7nzMy900RNo44milg/qkTwByiIqAh9qyCU4kvBF/v/+qC0qU8qWCgUpajEOMaJmpAPJ9Zk7p17dh8G9+t62bCWCoQRFrIsEuXnbAgQACNbBEA4lxGDbAcwICErzEZpamGIAOwAqDx1KSCFi3PT5eE7qU0hbFAHpYWtQQ6Kcz+NH+zc3K/HncVsG1QAndooheKeyRHo3/9y4gVGBYhWzUzYW7gqgzBNfP6brQT4+96jUP0ayVgwmHPMn+IYJ+QL31Tl+Fw0WKuzD1dmbhFiYyvhsLe19u2dlvsnGC++m97M67JAKIGXzSft/tk/8uPqSr3R/XLW+m8VohJAzKf/Tq+9oaL7+ewVYZbfQHYCfPDYxpNDU6Mz+fDPYAMfAZQI1snmXRjeje9DY5BhaVtIOcn2y3Pd0XfLrz4c/MHIqN7ZqgwkI7bpDd68rIp1YZBiHAJWCsadp1mjaJ0EyTjMTu+oApUWyaQW56/ZTKa9Z/9sNiTjdhzfPu6IJx6xRouv6kYkwbFeZ2rcmki3+NjzYGmcRUggWp1qkoJN/cdg5lZRjp1JwQ5VEdvBgDHv+9W6j/5yfX9Uk1CzuX2gbUAginL3DKPfl1J0VkciHDjfne/JK4fq1lqaZ/j2foaGJEze7o8Gl3f/ertv7sN6nL/x72poyEalJx9pz/etViV1922Oiro2qJCRAaIBnE6vDRtjIOhr07LaUUo3HPKE8T+GBxmSp9zXvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425ADF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(943, shape=(), dtype=int64) which is a tf.Tensor(6, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgoB8Qqv77Apec2PG/f0qxS3E0vP2zOZoJq5za6j9/Zitj16d74M8XX3bscuZv/r/PwuEUkxd/zrjgkNPe9+e1KQ45tdV/3sjjkGM48Oc1Trma/++zZJvCubHJhXz622Ly4s/fPixy3Jf/LDR49f3D3xUIMXgwcWsyrHfgzT6A1ca8P0sY9u1muPw3AFPO6OtfLdu/rt1/pyEJskBpZfb/DOF3/DL/b8Riqt/vP5Pn/PnzJx2rne1//vz9830RH7IYI9z8XD+GK3MuYtU4aAAA+F9TQq3xozMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D285A6FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbklEQVR4nJWRQRLAIAgDG6f//3J6AKuMMbYeXcAlgtf+NMMUhIPjobvW1ppWa+t4I0Q4Wzr4oVPaImwhbXOV3SZmFf4PfoLStMP9Z/ts/ZtrAOB7uXSCkQoUZLiAYyxkhvkr7JMOQhOTCeUdbecDRJAbMz1q2esAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7E38C95D30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAPElEQVR4nGNgoB8Qqv77go7WjQI6A7/ffybP+fPnTzpW2fY/f/7++b6ID1mMEcZgyfVjuDLnIo0dSCEAAAW1EmJmr6ySAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444CF7C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACRklEQVR4nE2SzUuUURTGn3Pufcd5nVFLDT8i0IiGYmgw0p1/QtCqVUVQm9atCqpFiyAQ/4CoTdQ/ULsIghYpoVZQBEUfDmpamqPjfLzvvfdpMTPS3Rzu+cHhnOd5BO0nAkJgEUiQAADbYe0aAEL4f1OEKiCMAEEJT4QOFAVsl02NxrtONc54V621x4qqaHagbn1XiDIme/ri8Ve36wQsIFajrJEaM1GSmsFC7/X814eUFqRQosG9XdM39F3z0/d7MT+7EpSEBeCipqmmHpub3WPFa73l8oONZhPSgqQGk6jzNpqcwdzTt+sNATsLWckoxJi4eAlvZj9V0i4fPEEFVB37q41EchO3SvMzq77ncGkoCCKxEEHQatJUPXqn9PlxuTIhN9evCJRQgFT1IcoNT5VQo89XDv29sUcJjhakgsyOnhg5D5hpup7Ld1cSCWypL1HfSOl9QpJ0SX3uamGwryejbVfUhiM2AoCtfrN0b6HuhdqxLCRcWzwJYPH18Jkn3+qJcv9OCFj7AGD5RXlnvJGNNSSpY0eEKB6/AGC74M4++jESN9e39pMgkpiMAjhVPLfkgo9ltEYvoAKQ4Js/FwBAgam4+8/qVhTZ1hcMCL9eOmwDWwfeNdZkZxMqKtJyBfCy8vvZscLzo8WP+XpIHQIJAaCxID95sL5iYx1rfKntVnwa0pS0AIxKSOZNtiqZbFzfaLquHQ920qcmiqyjD6Y7N7DcZK6SqvMdhQCTkilZS3caDAzK/cSLIDHW0gemHoSjqAPwD8HfJK8tvYXEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D285A6FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(439, shape=(), dtype=int64) which is a tf.Tensor(6, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAz0lEQVR4nGNgoBOwfHw4GJdcxvu/p1RwyFV8/5DOhV2KZdK/C3q4zFz695kELjnPX59EcMkpPvjjjUuOof3vLFQBJgTTI5fhhQYuK//9/fv37/fLC1iwyLn//7fVvfH5v38NWCQ5/v3QZWCQrPv1Sw2bsWcYGBgYGPr+amE6yIDhDgMDAwPDJ2yuFYfSQtgkRRgYGRgYGPRyjl7DlDzIwMzAwMCc/X8VFvcwvPuVwyyw4u86ZmySjr/+fvj874Q4NjkGBsdt+w6lsmKXGxwAAPijQc787SbQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4464DFA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArElEQVR4nGWSWw7EMAgDh2jvf+XZD5LGtJVKUcE2LwABwHbU9vVE4tGwM3KNX4gdE2BxmeuyeAt5cRqSQa6nSBFWSR20gAUVfOOb4Kj9o2MA8UmOxpy8bzkDPaaVGvbYT0rt/0U6Vo/P3MyzCinqAKZMNB11jZ3AL5Xblyq85by5ey+uFifXUSNxoo1JOY/FNMZgnhsYHZzXaAdZu9FzXNZuwVnGUMzLjknFov6f7LlrWuteFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425ADF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAdUlEQVR4nGNgoBs4DGMwoctkMDCw49b3gYELuwTLpH8Merh0Lf37TIIU18GA4gM8ku0Ms/DI/iVg9HdkDnIg/GNwZ/jw7x9unZIETGbQwmYsBDRgtxMChBiu4TH1ECFrMcC7XwzMDAx/GZhJ1gkBrGTqowsAAE73FDuiqTePAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444CF850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACkElEQVR4nC2TzYuVZRiHf7/7uZ/3Y45nnKMjShZFjZkSpdAhhopoZS6CGmNcVtROhFa2khbRf9BGkYJZhBstKJLUiDIMgkkCzQaatCbHMzpfZ74653mf97lbOKtrde2uiwQJAKD6wGSJFLIyAyAANlH3khEE6QwgACUIUAChwFmgjM51Qg0DTQnSxG2BtHmz/1/CzldvfRW0ToAoCYH65iuN4+sTl0JWvvvmhc6VWKxUoMIE2fDqvhN2f3d7aVKOjp/6vjm43FohQAd6tWLXl6d/fuwYG1vmzn/RDbWxigAVzJLI9md+C1tHTxRnrt5fWmMlvoc6KQ3JRSxcMStmyn9/WnIDWNPasycQA7xllkIl/jBm9zaOvLx9yNBP3okAyAttOpPsvTFrD2+99f6hPPPasqQC2Goo2k5kZJy4+8vizPzVKIiLIdYCcNgeeaFVto6Y8fbI0OF7+7XqhZqoFcCqv+NHp8ffsMXzv+84BbYmFxwyXVE1sLGRLr41stM++6FTRgBLT3fX+3UPRgcOCBoNN/rOyX/i0Nuvg9j49PPVYIAaGDKEZffroT3dfGwsEXLvUqjMAAEs9lwy8+0drce/+5ik3X40CAAoABjJfM/M5bmp5jKB05c7Pkq9WUKmPbPpv1PtkANnv50N2rJNkw+1brJau/hs+cTMa5j4I1Z5OU8YFQDmx/S6W5h8/pMyw9SBb7rOQgJIJSh594aZq298sOvDMz/ufmrOx+XomSCEy8fPhSrbN1jf/XP667+uvfRi48mYnNOMDq4ZY4V820cXrhUPX+/qcwfziU4FoVEIAQ0o9xZT/WyjcnlZyGwF0Cgg4YzJaW6V9SGDjLref5A6AdKrGiQNiEB9WeYPFvkfZVohHHZk9dMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425ADF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(470, shape=(), dtype=int64) which is a tf.Tensor(8, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCklEQVR4nGNgGGSAEZWnFyyi6vL/h/V5DIW6cd1///69u/rv32B0KblVP//+PVJkxp7x95c7uuS5vxecnRgYGFJ+/Q1Al1O+NVucgYGBIebnryomTLfJMTAwMMje/nsAi7uL34SwMbBM/3tFFIukxeu/s+V2/D1piUWOgaHp7+dnf5eJYJVjYJj/9+9dFDkkh8maMzCI2WHXx7r77y2pt38csEpm/v0RzGDz5pY+Nsknf3sYGBja/87HImfz47kxAwMDw/on2piSIX8TGBgYGBgm/r2C6dpXv9sMISwuuCQLjHHorcSey5sZGAwZzmKx1GDp979///79u44Nq2eips74uyqaB6scNQEAVGBdAY7vzEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444C7DC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAJElEQVR4nGNgoCH4T0vDqQkGxqFDJnjIBpT7kAph9J9Sc2B6AY8QB/lQLZFLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44319130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44221C10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/klEQVR4nIWSXXqCMBBFz52kgHY73f+utBAwtw9BsIptXu4k55vfjL5uBhwYIHKSkGuphnwzAI2hjABGC4g+AaQmp0FiBp2lBJH6BEMffYIB8KVcbNIHPRkNSyWsYamS9d2y9GTIQEpCTahAapycoZWwSpynfmU4mniveGN3KO2+uxGxcHDW3iMfwfXEH+wfKL1k1VrfHPabrCLeh/UxXEcCIcH8EhFgGTP4Svk8CDCR8QggP6G6QNud9OtdAOqCjE6l26apTeVTCUT3WP+Dfxd2+/6HiPdWHAJu7TLvnn5qxVfKWWwLwESYsdkj940wQC0QVENqiQJDqTagJH4Ar01jqgeBrpoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444CF7C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 503s 101ms/step - loss: 2635.3474\n"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(832, shape=(), dtype=int64) which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzklEQVR4nGNgGJ6AO3zK479/teB8FgYGBgYGWYvXB4yUTVz1GA+810LXEvrn+4uvfz6drRNlbf+DkGVkYGBgYJDJcf31fcOeawwMYtef2nzCsI4dZsZEhCALlP4KpYW+TsLt4H3PkTgsCJad6aG7/pa/6xkY/3/rRtOjcOnPnz9///z58+fv2TBk1zIwMDAw8AWaMNhpH8phYHj4lQEbmPJXFInHhCb7nwG3JMfPf7h98ncFHp2ogAkfF03yHz5JvMYyiLLhllwhw8ZAJAAAs2c9+Os2klIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44221C10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuUlEQVR4nG2SSQ7EMAgEq9D8/8s9B7zFiSVLMUtDEQwgEDAYAGOQtH3YCOtI9HxHwri21MwZcVPcjOipNBOG1wjBLX7E/s4mdi9gQgF6+XaHl4FDqrx9/RL9zpyM5ZV2DqpagkfmbHji3l7AFHqhxgk3gN8oBioPss0SpPg4LZQewg1DGqF25M2arvkuGgQ/ZJstjAldnCva9R2ecWPJVp027L2rRT0kQ5Z0oc3Sm3bUY/3XtbRzZxH+6WlfNYmE5gAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44418820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASUlEQVR4nGNgGElAFpmjBWcxMTAwMDA8xqdTBpnDRz0XEQ3qGepxS/75ezYMp6SWFk6pIQVWIHOY8KlEk/wryoZbcoUMGwORAADjsghV4SxPUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44418910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuklEQVR4nGNgGG7gpRoDAwMDA59iIiNEgBFJ8muTuEKKiNym75/2blnMhKbz+sv9/z/oMDCwcv/9H4lp8P90BgYGBgbmf39dMSWfmEPofzkIMRYYQwZmwguEJLrlDIx2WL3DwMDAwGD7518oQiFCfAXnbxYh279/vm2pfIqpK5uJ9f+faiTlyHZe/ff7O+OT/zitZP6ngds9DH+FkTgYXmHHJukJ5WOoRgK/ufEY+48Bj+RufTzGUgkAAB57L7CsHNMpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44221C10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(145, shape=(), dtype=int64) which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAe0lEQVR4nGNgGGCQ8g+P5JW/qHwmZM4lPBo9f/8VwKnz+S+GINxaX/87gtvONf+lhHBKrmPg5cQp+Zjhxy+ckgwM0riN/f6ZEbeDHi77H4zb2DMM2rglPzK4MOAG/17ikfz/zw+nsQz//+O2Ew2gSX7ApzbwrRA+aToAAJBcIE7ppxgwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44418EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAj0lEQVR4nI2SyxbEMAhCufn/f2YWzagxzsNVCwjYU6x5sLQkAVRYQrIe8p5tN5BGCCSwRGrPaZs0koqbIvvPlm5ZScdy0eD+MLVNjlheCb0dHcIFj9QiwLjO8ZnnTPtsGWddJXGiq+iJAGve7Jn37MhVXzmK4G2LzyqHrVWr8OmU34WSpBRsJPu/8S3hWyYv1/IqQ/bMvdsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444CF940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAe0lEQVR4nGNgGGCQ8g+P5JW/qHwmZM4lPBo9f/8VwKnz+S+GINxaX/87gtvONf+lhHBKrmPg5cRt7t/HojjlNP7+U8dp7PfPjLgd9HDZ/2CckgxnGLRxO4jh30vcOvGD///8cOtk/I/H2H/MeCQ/oEU3Cgh8K4RHlh4AAMJSIVLYtqcrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7F587E4DC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAZklEQVR4nGNgGNRABZXLhMJzxKMx/jQjbsn//+VwGyvK4IvH3P8XmHFL/vpvjNNYhu8MRrgluRn4cEtKMdzH5yJN3DrRAJokny0+xSfwBCDDf358dnLhlnRi+I3bVKb/HPhcNNAAAAsAD/T6hR/CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44221C10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(710, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAi0lEQVR4nM2RsRUDIQxDP3nZK85kJyaDzZyCgwPCUaWIOizZljD8Mcx1R0nud6y8IEnfmkpWjQGhsT6pAzzbIx7kaPAyAPKgdPdSNuuar7GBEY+190UnRJhj1J0lxIrUlgUg9X84G3rnjUFrcftpBmAadpYoySDC0ZUudOcQM1I75lA+ZwjyzudP8QGp/WZ8CtnUgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D285A9D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nMWRMQ4AIAgD+f+nzxWVhhIHu0ByLY0xIokoReuw6D9hvMwmid8mjjm6qW3sfdPrbbEyYOZF1MlxLXU9BZD05Rs0XpNaJduY2PbSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D444CF940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAJUlEQVR4nGNgGMngPxkyxMlTFzTQ1baBsZdAXOzHJetAfafgAAAikAb7XEXAGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44418820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB9klEQVR4nCXSTW8bZRhG4XM/8844dowT0SoKldJSpH4gFlD1a9n++q6o1E0RFQVStyo4CqA0dmyP531uFtmc/SUdTbw3VMGLVX/QxLD5+cdf15LBpqC+gUeXdXGHcP+a92sAK5wBKqO8Oj5bvBJf3jguAduAArGZ3eo+/TbQMHvIAMBI2AoyUVsTqChOAOwAuO4wape2G/IrLIHHtqEY2K3qBX4OCk1X4Ae7yVwokLW8+jP1eN+IkyPDvemMUCgE0hY0cSS0zXf+4Qb9CuHiAIrlVIbs26t2WkdrYQiQ3d7Mp6MUlhgfd+PF13tpqQjkdnJjHwmM2Y7+K6tGoIIDSougKjDs5YHe9wABydHkoMlKZEXU/Dj8tfiHTBfLefjlqBDJpxNSuZifYhGyJlDGKz/rwovf86WWZ6fCsoFiVC8Fvjr/ANBXwNiSQzKCUvpzYPCs2L5jW9dOMJFXKyBQ3PXhN/emAWgswD+Vf0+Fn01k+s8f6/35WlAAd+NtJvB4H+vifC5lt0YOCbWjetiAp1j0FdxtEQSm21s2Je7mk0HCs8b22x2G4qZ2dfPu+DhmMwzbcDsEiXGhlkuJneoByDTDJABbVknV66cYCuA//v5+Kyzy2200cgHvzpYbO7O/8C8AieZdGGHiYvdhk5xuzrHBlhot/gfgUSOfRS+lowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7E38C95FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(122, shape=(), dtype=int64) which is a tf.Tensor(7, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhklEQVR4nGNgGIrg4jl/QZyS+//9+zqrQYUNq6Tsnsf//v37t1MTu14Fn+03zv57VI/LbDF+/UPf1XA7rOjfHASHCUPalRmnTqM/fwJwm/vrIT5j2UXxSIq54JTUZvp6H6eVRf/W4LNzPx5JRgbckqL/8UgmMuCRZPh7F8MVcPDiMz6d1AEAFgYmHjFkWGIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D285B1D00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAd0lEQVR4nI1SQRIAIQii/v9n9mCZZup6qEkSlAKIFQRpT3uBZGmBk77D1D3xKl9BBIhZMBKuf4dSl1y5CDvT3WjDyIB7N6NSR5jcoD5W39SLFEP28bwwAURsyaxXiaP2qokVpA4TWM/fyj38I+sLrqS4SOlkW/sBQfZPumac8TUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425A3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425A880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABd0lEQVR4nH2Syy9DQRjFf3fulFuPCiWpqBCPxHNBaNKNRxGPhNjY+TfFwspCQiKE2GFT0YSUXIqq9t6xmDu3VWI2c+acOd+c78uAWa22AMDir5WM/CNqXorqWdSKCpibr9KBKJOBvpyKLdkdjq9ZzaWacvo8dDXR0FW5vrA945QLKrLipIGV+7fDh2ThGc84+4ajle7y9qvwYc9n9vJAhYEaE5+5/PHgmL3hbJUVPBQDDQmluw8XXJeRz1IBKL5bqtoaANOJwmPsVOFk9hXSUzVp4SzY10oKxptPlF83BIDeEjD55vmh0/ZMhP47BTzGaiak8ZK0KjdZC3hy6wsykQZY1DF/vNdpQKbKhRfyBqg60SYsZe9+1Ykt2rE6B96HB5CRM6aV4HK0DAy4AKnWI+Ms9og4cFuxIGrFgWwuH462XW9TCbmzDsi1zTbCCQUJz1m4jwLzr9mXMJXlGBQfjf/+mkII03JSOL/kFiG05+9PHanBTQB8A7EMaFbYHXW4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D78445EB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(673, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCUlEQVR4nNXQrUtDcRjF8aPeKRpkC1uaRUwOLW5i0WARRZNaLGIRDAZBEOwiC2JRh8EowzQMQxD9G4StCBM2hjM4XwYyGLvuq0HDvd7fDYue+nkOHB7pXyeWqn7VFk0SXkp/ANgTHhrffwbI3hTY/kPR1TrwmLQ6OiuVIbeNPgCvmXlJejp12/QncBeRJO3mb89iDgsVoRoPSAolJm1gw4GzUI6qd2wvXwLgPihJ1g8mpMaJBod/b4vrNUdzBmdycdee7p235vvV4dbcC7Qy/Z4XTElaLgDHHpKkvnQDyFpGvABa10brObCBI2NNK0BuIGC0kTrYa+aeNoEFH1MTzsN+eEmqy8/azzcOcoPc5xhsmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D44418910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2ElEQVR4nG2SS47FMAgEq1Duf+WeBWBbT5NVAoT+ASQkhPQrWyGGn0cIQkwhbnGeXmGoO+7Th0SKEP/rAgXac2TxBTB8GLAbw87MgHd6mEvetUMAUBpf7Wa2sqjR5NYGyhjcgo3z2OSdhRqFKzHJQCepNezYyFou1XwkMQ7hEDGUHRY2vqMI+uPlkMcCKJyEPVm3tUiZY2lj39Css5Ms+tE5CfxY0Vzi0TmFzczAlzvJUT9rvpEQ+ugCRk+ecySOTjIGs9fXP2XD6v3Oge0lHQeO/UPnilolf/8nkShbqEuGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425A3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nNXQPUuCcRSG8YvSoobIoSZHN7GlgqYGt6Qma2kRP0Cz0B5+gSIaGiOagiCEoM8Q5JZJDW3RC4QQPnnRkIFPz9+gsXs8P24O58C/Th54LYdkZv34DTRaSlodBGj5U7IV1AdgBHJxm7tFnlgF4CBuxQ/1ahaA7eblYX7AMnf4uJAGMotE/dXfWRGyTMzvNO9VZHpQ9YYzVBDby7GdX0NFEneM1Z67Lw0ogb3TqeQLgI2WuBekSd7R81QQT9TeRdDGidTdYI1N8Zp00AodiKrhHlvi2hCjC0fDDNgf/QX/mE9XMVsP320mJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425A880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzElEQVR4nNXQv2rCUBzF8SPVBC1iFRUEO/ggHRyVjHHopODkEBC7FFHnPkWd2tfoY4h7NIjVzi0tX4eAkuRmc+lZ7p8P5/7gSv877aLdMMtt1cEPeDNQq7cJWNs1aMYpN97DiyRZ0xh1tkd4zUuSrKeoPQC834SHvl+KIADLgtxKJufAqp7AwB0S7A7AINlkHy6MojN//7hkYvqCchOAZ5PprgKwMJq6U2BuNn0BMzPde4CX0tuljpMeU5+UBNRjV9nz7uPn+zO1edWcACU4cdAOSV0VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7D4425A3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OwnedIterator' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_967357/1029948376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_967357/348604042.py\u001b[0m in \u001b[0;36mgen_image\u001b[0;34m(dataset, n)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OwnedIterator' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "gen_image(dataset_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(dataset_test, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txformer.save_weights(f\"./models/pretty_good_model_i_guess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
