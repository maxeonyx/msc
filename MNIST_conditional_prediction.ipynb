{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "# wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "# config = wandb.config\n",
    "# config.learning_rate = 0.01\n",
    "\n",
    "# callbacks += [WandbCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00]\n",
      " [ 7.0710683e-01  1.3921213e-01  2.4833918e-02  4.4166050e-03\n",
      "   7.0710677e-01  9.9026257e-01  9.9969161e-01  9.9999022e-01]\n",
      " [ 1.0000000e+00  2.7571312e-01  4.9652517e-02  8.8331243e-03\n",
      "  -4.3711388e-08  9.6123999e-01  9.9876654e-01  9.9996096e-01]\n",
      " [ 7.0710683e-01  4.0684462e-01  7.4440487e-02  1.3249470e-02\n",
      "  -7.0710677e-01  9.1349739e-01  9.9722546e-01  9.9991220e-01]\n",
      " [-8.7422777e-08  5.3005296e-01  9.9182546e-02  1.7665559e-02\n",
      "  -1.0000000e+00  8.4796453e-01  9.9506927e-01  9.9984396e-01]], shape=(5, 8), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 01:40:11.264401: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 01:40:11.817124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(mask):\n",
    "    image_mask = tf.reshape(mask, [28, 28])\n",
    "    return image_mask\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=1000):\n",
    "\n",
    "    i = tf.range(dims//2, dtype=tf.float32)\n",
    "    i = tf.expand_dims(i, -2)\n",
    "    \n",
    "    vals = tf.expand_dims(vals, -1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = vals / tf.pow(scale, 2.*i/dims)\n",
    "    \n",
    "    sin = tf.sin(rate)\n",
    "    cos = tf.cos(rate)\n",
    "    \n",
    "#     # expand dims to allow alternating concat\n",
    "#     sin = tf.expand_dims(sin, -1)\n",
    "#     cos = tf.expand_dims(cos, -1)\n",
    "    \n",
    "    encoding = tf.concat([sin, cos], axis=-1)\n",
    "    \n",
    "#     encoding = tf.reshape(encoding, [-1, dims])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(positional_encoding(tf.constant([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]), 8))\n",
    "\n",
    "def img_to_tuples(img):\n",
    "    \n",
    "    height, width = img.shape\n",
    "    length = height * width\n",
    "    vals = tf.reshape(img, [length])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1])\n",
    "    cols = tf.reshape(cols, [-1])\n",
    "    \n",
    "    # permute the order, to ensure the network uses the positional encoding and not the implicit locaiton\n",
    "    idxs = tf.range(length)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    rows = tf.gather(rows, idxs)\n",
    "    cols = tf.gather(cols, idxs)\n",
    "    vals = tf.gather(vals, idxs)\n",
    "    \n",
    "    return vals, rows, cols\n",
    "\n",
    "def random_mask():\n",
    "    idxs = tf.range(784)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "    idxs = idxs[:n]\n",
    "    return idxs_to_multihot(idxs)\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask), np.uint8)\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "positions = tf.range(-28, 28, dtype=tf.float32)\n",
    "encodings = positional_encoding(positions, 16, scale=28)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "im = ax.imshow(encodings)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 01:40:14.655922: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def make_dataset_generator(x, y, seed, typ='single pixel'):\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    # keep track of the index in the original MNIST\n",
    "    def to_dict(i, xy):\n",
    "        image, label = xy\n",
    "        data = {}\n",
    "        data['index'] = i\n",
    "        data['image'] = image\n",
    "        data['label'] = label\n",
    "        return data\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.map(to_dict)\n",
    "    \n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # add a transformation of MNIST images into val, row, col\n",
    "    def add_tuples(data):\n",
    "        data['val'], data['row'], data['col'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    dataset = dataset.map(add_tuples)\n",
    "    \n",
    "    # create a mask of random pixels masked out\n",
    "    def add_mask(data):\n",
    "        data['mask'] = random_mask()\n",
    "        return data\n",
    "    dataset = dataset.map(add_mask)\n",
    "    \n",
    "    # mask out a square region as well as random pixels\n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask()\n",
    "        data['mask'] = tf.logical_and(mask, square_mask)\n",
    "        return data\n",
    "    dataset = dataset.map(add_square_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate training pairs\n",
    "    \n",
    "    def single_pixel(data):\n",
    "        data['target_val'] = tf.cast(data['image'][data['target_row'], data['target_col']], tf.float32)\n",
    "        \n",
    "#         mask_out_target_pixel = False\n",
    "#         if mask_out_target_pixel:\n",
    "#             target_idx = data['target_row'] * 28 + data['target_col']\n",
    "#             target_mask = idxs_to_onehots(target_idx)\n",
    "#             data['mask'] = tf.logical_and(data['mask'], target_mask)\n",
    "        \n",
    "        # offset positions relative to target pixel so target is at 0,0\n",
    "        data['row'] = data['row'] - tf.cast(data['target_row'], tf.float32)\n",
    "        data['col'] = data['col'] - tf.cast(data['target_col'], tf.float32)\n",
    "        \n",
    "        return (data, data['target_val'])\n",
    "    \n",
    "    def single_pixel_random_rowcol(data):\n",
    "        data['target_row']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        data['target_col']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        \n",
    "        return single_pixel(data)\n",
    "        \n",
    "    def many_single_pixels(data):\n",
    "        rows = tf.range(28)\n",
    "        cols = tf.range(28)\n",
    "        cols, rows = tf.meshgrid(rows, cols)\n",
    "        \n",
    "        rows = tf.reshape(rows, [-1])\n",
    "        cols = tf.reshape(cols, [-1])\n",
    "        \n",
    "        image = data['image']\n",
    "        val = data['val']\n",
    "        row = data['row']\n",
    "        col = data['col']\n",
    "        mask = data['mask']\n",
    "        label = data['label']\n",
    "        index = data['index']\n",
    "        \n",
    "        def data_plus_pixel_index(i):\n",
    "            new_datum = {}\n",
    "            new_datum['pix_index'] = i\n",
    "            new_datum['target_row'] = rows[i]\n",
    "            new_datum['target_col'] = cols[i]\n",
    "            return new_datum\n",
    "        \n",
    "        def add_original(new_datum):\n",
    "            \n",
    "            new_datum['index'] = index\n",
    "            new_datum['val'] = val\n",
    "            new_datum['row'] = row\n",
    "            new_datum['col'] = col\n",
    "            new_datum['image'] = image\n",
    "            new_datum['mask'] = mask\n",
    "            new_datum['label'] = label\n",
    "            \n",
    "            return new_datum\n",
    "        \n",
    "        d = tf.data.Dataset.range(784)\n",
    "        d = d.map(data_plus_pixel_index)\n",
    "        d = d.map(add_original)\n",
    "        d = d.map(single_pixel)\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    # single pixel example. the row & col are translated by a random\n",
    "    # amount and the target val is the new pixel at 0,0\n",
    "    if typ == 'single pixel':\n",
    "        dataset = dataset.map(single_pixel_random_rowcol)\n",
    "    \n",
    "    # 'many single pixels' generates 784 single pixels from each image,\n",
    "    # and the target vals are each pixel in turn, translated so that\n",
    "    # they are at 0,0\n",
    "    elif typ == 'many single pixels':\n",
    "        dataset = dataset.interleave(many_single_pixels, block_length=784)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "def make_datasets():\n",
    "\n",
    "    train = make_dataset_generator(x_train, y_train, seed=192_168_1_1)\n",
    "    test = make_dataset_generator(x_test, y_test, seed=10_1_1_1, typ='many single pixels')\n",
    "    \n",
    "    return train, iter(test)\n",
    "\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 693 which is a 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA50lEQVR4nN3QvUpCcRgG8MePJcFBziQ4NHUDJQgKIo1B0JBtXUKSFyBuLgVNjs6K3kCjgYtBB1edikL4L1qKgjzPOQ3WgdM57eK7vfx4P4Edj9PHxqUVTrXXFcV+MhRny+nGlTKheJSzmiKzP2nch2NgAqD8vE2jYQ3s/9Z9c/Tyu5C/7WH6KuW67UWwxuoNPkiKJh+w2pQUSZHzYz8VPp31Wo5Ur9pS1WdFw1HdUPPWAa5Ju9N58KxiyJ4hlxcAkl2KPPHwfjtreAYASJzflTIxD78kOaPbvw+PAABuysBT8z143t7EN6cxasmQ2QMKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA42022E50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAv0lEQVR4nF2S2w7AMAhCD/7/P7MHL9UtWRqxImhlhEH1Q8YABGDAorFOoQBLCalAqxiKKmkx8CgIoalY1Zk09opb0wjqQPeKiNLe1ZpD4219W1AjOmL0ksXryRZfdH+xsgVEU1gzZBegmMFZzom5+tuzktPs7qf0/4xFa8ndpKc8tO/KFQ3pXtlLV7doJyCVvBlSTCjPqlWrbUH9Wp6B9zR+u7l2//Dz2f175AOF1oA4N0VkX6Vp7ay9Z376yfABBWlbO36qjusAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB30F1A5B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvUlEQVR4nN3PvwpBcRjG8a8/C3UWJmVwEUrKZlTKYnQJlMEoF0C5ADeg3IALMBnOzkRKnYUkSs8rA5HO7+zybE+fente+PFU55HU31wEeE48nLkB+VdNfmEpk50AuZ0LV7AGmstnjbuu+1GbtpGDCuUxRs9FswVCpiBM/b1kSCYdQ3i/Xg2zQdc3635TIAjEkRQtyWf6oU4gzZBoAB4yqfjGkZBJ1ABI14eQeOPJzO7h/2IAtJtQye+cv/9HHti0S4I64ZRAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAC501D77C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 68 which is a 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAo0lEQVR4nMWQwRUCIQxEv9Zh+pBCpJC1DhrhWQdaRxoZD7g8Nq4edU55fCaZBP4sq1L5gJokV95jRapG3vOaqyUAl3wJTHq9lLpEs6uu3xIU+cTSYF2L2mwMEarSqDdt+ksFjr1+RHg9fXFmDefdIoQBb6RdCoC99S1T/hoWxadFzbfUNN/XfD5o8jCmSDUDWHZ5jJ+bVpXODptJlzPALYT7qZ4Zhlh/4L0g9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAC501D7A90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAz0lEQVR4nGWSQRLDMAgDd5n+/8vqAbCTNoc4EyEhYYzBGADm2K8i8IO5FQVAAO1jMdEMoaH3U0DEYa+oAlYLxAMFNKGtHK/X7PToytPvDVfanLKNEUim9KZcuelVk7KPPONIeWbiJBhu4t9QH8wU4Ljh8VaCZIMYI+TKbBQkU+fJ899sEwmffyM3W71IAjl3TelNlyePXpPkJ8oBQ0ulxy03NDWed4FCcpR3wdIXtc6aW/N7XM4CpXX6ymx0O4+1FLrZ7hrMjD7PXPTWNRT4Aj4xa0Ndu3aYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAC501D7A90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnklEQVR4nMWRwRHDIAwEN+lDfYRCQiHug0YY1+H0oUbODxIbZHsmn0zuhVhxHAL+LEMqlwihfMaKhJHPzpprSQAu+RQtee+UOhEOu+qnLQHyjqWNNU3qq6EAULpmBKtLDQEAyALuALzs0H/b4EyKcI9hB9/SZawxnS/72nykpn6+Ngw0ebxGqu0vs+Mxfl6Emoptz9m9nw+A+cvR/UQrpXZJ4212WXgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70708580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix_index shape ()\n",
      "pix_index dtype <dtype: 'int64'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 290 which is a 8\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C7AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C7C70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C78E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 290 which is a 8\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70725D60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 290 which is a 8\n",
      "pix_idx: 782\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB22459EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 290 which is a 8\n",
      "pix_idx: 783\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGMTg0r+9qAJMCGa/1v/DuPRN+P13JzMDQ+rHjx9no8tpvv57N5uBIf3H37973NElb/y9o8nAkPHtb6okF7qc2tu/VQwMDHv/vjXDtHHK3/VsDAwpX/4mYsopf/y7iMGx5PPfR2qYkgV//+7Z/uXv379/bzljk4SB6xlwUaRAOHf0C8OViwxqNeJoOuXdu9y5GU79deSPf/O3BtNaBgaGzX8dGRhW/X2MxVgGhqkMDAwMKxmE3LBJMjAIMDBc+MqhhM1YmVvPaxkYFv+FuRdF55MZgtk2XOxY3cPAwLDt79+jf/9mYJeUnvD3L05JBqlJf/+uMcZlMHUAABS0Z+4azD16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3klEQVR4nE2SS5LFMAgDW67c/8o9CwFvvEhcttEPQERFBEXdzccsIxKBgAA86QmSQOxVMFO12M6+FDwPMwFcTMGvIELmaLACZEX8/qTK0o+nY9Zcv+75/QY+kteCFlYSxMTAd+9IcYOCIb61G5RAlEWLcTn57xPCu5g2YXsQ9JXJ7cxRk/BtE0azkwxI3hqBEXMvYjaWNKYJuSE0vly/L+QROFraS0eA2yzG7A1FLtssx7Gfn68Yp2T4mtrDFKaBs5p//CfDny5uBL2R2Nzrc4ZrvZ5i4ZtBzI5eVbfiD86WyOfwgfKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB22459EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM2QPwtBcRSGn9wyMBhFyuZTWKRcq3wGURaZ+QJG2Yw2KYNJuZOsPoBMJgPDlWTwYvP397uTwTud8z6955wO/LOu3nsfeqkvt7k9qKkDFd/3+yZah+oZzdxvtAZqJ1USkU+S2QvA094wscc4DBxlPwk2AUys8l9mQ0hCQtQebuiZWS58x1k6tOIfybTbcaOgXIwdLePGCcDQek8OyqJgYkVKwMEcTK1ow8DyiCbbbAT7l7QIggEs2YWRlf5Gd2fePqIsz9b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 489 which is a 1\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgUlEQVR4nGNgGAQg+CRuOYN37xEcJjRJK4H9uCU5GNbjNvbsPw2ccnq/TrDgNNaP5dcfnJJSDItwW/n4txhOOcu/+5C5qMaWM17HLSmM20IGhsMvtHHKSb8+g8JHMVZCZAVuycj/X3Fb+fK/CW7JD3eZcRvLcOgvHp2vWHBL0gkAAEC6H4M4KgKfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvUlEQVR4nHWSQZJFIQgDu7n/nTMLxIdVf9goRkyISDAABtNbhHiAXg3n3kRdjABBED2gNCAiBGLiVHZ0IfYShCIPzCYtHH0nDh/BV9690D2UOIViZ0PgkBhfQrBaoAJZmEAKiECMqyqrz4Rwfd32EZyz1nDaux/xK2qXrZqv3TH0O7icHj2fTMeVOZyPfzgv3fFqNztj8sTkxWfvjcmrHX9eu6/XsS2LGx9v3artAWJNwi8X6z/MBUbeUQvwB2wHVDhvACetAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAf0lEQVR4nGNgGAzgJG4pg3fvERwWNEkrASYEhwlNkoMhFrexZxk0cMrp/TqBZBGasX4sv/7glJRiWITbyse/xXDKWf7FrY9hw7+puCUP/0PhojnolTZOjdKvz+DWKSGyArdk5P+vuN3z8r8JbskPd5lxG8twCE8gfHiFHvf0BwBTZB5Gi3nEDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 489 which is a 1\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgUlEQVR4nGNgGAQg+CRuOYN37xEcJjRJK4H9uCU5GNbjNvbsPw2ccnq/TrDgNNaP5dcfnJJSDItwW/n4txhOOcu/+5C5qMaWM17HLSmM20IGhsMvtHHKSb8+g8JHMVZCZAVuycj/X3Fb+fK/CW7JD3eZcRvLcOgvHp2vWHBL0gkAAEC6H4M4KgKfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C6C75E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvUlEQVR4nHWSQZJFIQgDu7n/nTMLxIdVf9goRkyISDAABtNbhHiAXg3n3kRdjABBED2gNCAiBGLiVHZ0IfYShCIPzCYtHH0nDh/BV9690D2UOIViZ0PgkBhfQrBaoAJZmEAKiECMqyqrz4Rwfd32EZyz1nDaux/xK2qXrZqv3TH0O7icHj2fTMeVOZyPfzgv3fFqNztj8sTkxWfvjcmrHX9eu6/XsS2LGx9v3artAWJNwi8X6z/MBUbeUQvwB2wHVDhvACetAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA70725D60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAf0lEQVR4nGNgGAzgJG4pg3fvERwWNEkrASYEhwlNkoMhFrexZxk0cMrp/TqBZBGasX4sv/7glJRiWITbyse/xXDKWf7FrY9hw7+puCUP/0PhojnolTZOjdKvz+DWKSGyArdk5P+vuN3z8r8JbskPd5lxG8twCE8gfHiFHvf0BwBTZB5Gi3nEDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAA3C7C6F70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shape_summary(data):\n",
    "    for name, v in data.items():\n",
    "        print(name, \"shape\", v.shape)\n",
    "        print(name, \"dtype\", v.dtype)\n",
    "\n",
    "def el_summary(data):\n",
    "    print(\"index\", data[\"index\"].numpy(), \"which is a\", data[\"label\"].numpy())\n",
    "    if 'pix_index' in data:\n",
    "        print(\"pix_idx:\", data[\"pix_index\"].numpy())\n",
    "    display_uint8_image(data[\"image\"])\n",
    "    display_mask(data[\"mask\"])\n",
    "    display_uint8_image(tf.reshape(data[\"image\"], [28, 28]) * tf.cast(mask_to_image_mask(data[\"mask\"]), tf.uint8))\n",
    "\n",
    "def train_summary(d):\n",
    "    data, target = next(iter(d))\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(iter(d))\n",
    "    el_summary(data)\n",
    "\n",
    "def test_summary(d):\n",
    "    data, target = next(d)\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    for i in range(780):\n",
    "        next(d)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "\n",
    "train_summary(dataset_train)\n",
    "\n",
    "\n",
    "# TODO: TEST DATASET GENERATOR DOES NOT WORK HOW I EXPECT.\n",
    "#       IT SHOULD PRODUCE 784 EXAMPLES with the SAME image and mask, then change\n",
    "#       to a different image and mask.\n",
    "\n",
    "test_summary(dataset_test)\n",
    "\n",
    "# reset datasets after summary, because it consumes elements\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = layers.Dropout(dropout_rate)\n",
    "    dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        attn_output = attn(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = dropout1(attn_output)\n",
    "        if include_residual:\n",
    "            attn_output = inputs + attn_output\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "        attn_output = attn_output * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        attn_output = layernorm1(attn_output)\n",
    "        dense_output = dense_net_1(attn_output)\n",
    "        dense_output = dense_net_2(dense_output)\n",
    "        dense_output = dropout2(dense_output)\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 12\n",
    "    pointwise_feedforward_dim = 200\n",
    "\n",
    "    val = keras.Input(shape=[784], name='val', batch_size=batch_size)\n",
    "    row = keras.Input(shape=[784], name='row', batch_size=batch_size)\n",
    "    col = keras.Input(shape=[784], name='col', batch_size=batch_size)\n",
    "    mask = keras.Input(shape=[784], name='mask', batch_size=batch_size, dtype=tf.bool)\n",
    "    \n",
    "    print(val.shape)\n",
    "    print(row.shape)\n",
    "    print(col.shape)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    row_pos_enc = positional_encoding(row, n_embd//2)\n",
    "    col_pos_enc = positional_encoding(col, n_embd//2)\n",
    "    \n",
    "    print(row_pos_enc.shape)\n",
    "    print(col_pos_enc.shape)\n",
    "    \n",
    "    pos_enc = tf.concat([row_pos_enc, col_pos_enc], axis=-1)\n",
    "    print(pos_enc.shape)\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    \n",
    "    m = tf.expand_dims(val, -1)\n",
    "#     m = tf.stack([val, row, col], axis=-1)\n",
    "\n",
    "    m = layers.Dense(pointwise_feedforward_dim, activation='relu')(m)\n",
    "    m = layers.Dense(n_embd, activation=None)(m)\n",
    "    \n",
    "#     print(m.shape)\n",
    "    \n",
    "    m = m + pos_enc\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=False)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=1, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    \n",
    "    m = layers.Flatten()(m)\n",
    "    \n",
    "#     m = layers.Dense(200, activation='relu')(m)\n",
    "    m = layers.Dense(1, activation=None)(m)\n",
    "    \n",
    "    target_val = layers.Reshape([], name='target_val')(m)\n",
    "    \n",
    "    model = keras.Model(inputs=[val, row, col, mask], outputs=[target_val])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784, 6)\n",
      "(8, 784, 6)\n",
      "(8, 784, 12)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "row (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (8, 784, 1)          0           row[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (8, 784, 1)          0           col[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "val (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (8, 784, 3)          0           tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_1 (TFOpLambda)  (8, 784, 3)          0           tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (8, 784, 1)          0           val[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin (TFOpLambda)        (8, 784, 3)          0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos (TFOpLambda)        (8, 784, 3)          0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_1 (TFOpLambda)      (8, 784, 3)          0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_1 (TFOpLambda)      (8, 784, 3)          0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (8, 784, 200)        400         tf.expand_dims_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (8, 784, 6)          0           tf.math.sin[0][0]                \n",
      "                                                                 tf.math.cos[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (8, 784, 6)          0           tf.math.sin_1[0][0]              \n",
      "                                                                 tf.math.cos_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (8, 784, 12)         2412        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (8, 784, 12)         0           tf.concat[0][0]                  \n",
      "                                                                 tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (8, 784, 12)         0           dense_1[0][0]                    \n",
      "                                                                 tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and (TFOpLambda (8, 784, 784)        0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (8, 784, 12)         4908        tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.math.logical_and[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast (TFOpLambda)            (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (8, 784, 12)         0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (8, 784, 1)          0           tf.cast[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (8, 784, 12)         0           dropout[0][0]                    \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (8, 784, 12)         24          tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (8, 784, 200)        2600        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (8, 784, 12)         2412        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (8, 784, 12)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (8, 784, 12)         0           layer_normalization[0][0]        \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (8, 784, 12)         24          tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_1 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_2[0][0] \n",
      "                                                                 tf.__operators__.getitem_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (8, 784, 12)         4908        layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "                                                                 tf.math.logical_and_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (8, 784, 12)         0           multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_1 (TFOpLambda)          (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (8, 784, 12)         0           layer_normalization_1[0][0]      \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (8, 784, 1)          0           tf.cast_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (8, 784, 12)         0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 tf.expand_dims_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (8, 784, 12)         24          tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (8, 784, 200)        2600        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (8, 784, 12)         2412        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (8, 784, 12)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (8, 784, 12)         0           layer_normalization_2[0][0]      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (8, 784, 12)         24          tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_2 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_4[0][0] \n",
      "                                                                 tf.__operators__.getitem_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (8, 784, 12)         4908        layer_normalization_3[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "                                                                 tf.math.logical_and_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (8, 784, 12)         0           multi_head_attention_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_2 (TFOpLambda)          (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (8, 784, 12)         0           layer_normalization_3[0][0]      \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (8, 784, 1)          0           tf.cast_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (8, 784, 12)         0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (8, 784, 12)         24          tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (8, 784, 200)        2600        layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (8, 784, 12)         2412        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (8, 784, 12)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (8, 784, 12)         0           layer_normalization_4[0][0]      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (Sli (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (Sli (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (8, 784, 12)         24          tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_3 (TFOpLamb (8, 784, 784)        0           tf.__operators__.getitem_6[0][0] \n",
      "                                                                 tf.__operators__.getitem_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (8, 784, 12)         4908        layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "                                                                 tf.math.logical_and_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (8, 784, 12)         0           multi_head_attention_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_3 (TFOpLambda)          (8, 784)             0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (8, 784, 12)         0           layer_normalization_5[0][0]      \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_6 (TFOpLambda)   (8, 784, 1)          0           tf.cast_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (8, 784, 12)         0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 tf.expand_dims_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (8, 784, 12)         24          tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (8, 784, 200)        2600        layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (8, 784, 12)         2412        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (8, 784, 12)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (8, 784, 12)         0           layer_normalization_6[0][0]      \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (8, 784, 12)         24          tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (8, 9408)            0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (8, 1)               9409        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "target_val (Reshape)            (8,)                 0           dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 52,093\n",
      "Trainable params: 52,093\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, n=1):\n",
    "    \n",
    "    dataset = dataset.batch(n)\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        outputs = txformer(batch)\n",
    "        for i in range(n):\n",
    "            print(\"index\", batch[\"index\"][i], \"which is a\", batch[\"label\"][i])\n",
    "            display_uint8_image(batch[\"image\"][i])\n",
    "            display_mask(batch[\"mask\"][i])\n",
    "            display_uint8_image(tf.reshape(batch[\"image\"][i], [28, 28]) * tf.cast(mask_to_image_mask(batch[\"mask\"][i]), tf.uint8))\n",
    "            display_float32_image(outputs[i])\n",
    "\n",
    "def gen_image_many_pixels(dataset):\n",
    "    \n",
    "    # assume dataset is a 'many single pixel dataset'\n",
    "    # so it has runs of 784 examples, one for each pixel in an mnist digit\n",
    "#     dataset = dataset.take(784)\n",
    "#     batch_size = 32\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     reconstructed_image = np.zeros([28, 28])\n",
    "#     for batch, batch_targ in dataset:\n",
    "#         inputs = [batch['val'], batch['row'], batch['col'], batch['mask']]\n",
    "#         out_vals = txformer(inputs)\n",
    "        \n",
    "#         # np can do this yay\n",
    "#         reconstructed_image[batch['target_row'], batch['target_col']] = out_vals\n",
    "\n",
    "    reconstructed_image = np.ones([28, 28]) * 230\n",
    "    for row in range(28):\n",
    "        for col in range(28):\n",
    "            data, targ = next(dataset)\n",
    "            inputs = [data['val'], data['row'], data['col'], data['mask']]\n",
    "            inputs = [tf.expand_dims(x, 0) for x in inputs]\n",
    "            \n",
    "            out_vals = txformer(inputs)\n",
    "            reconstructed_image[data['target_row'], data['target_col']] = out_vals\n",
    "\n",
    "    image = data['image']\n",
    "    mask = data['mask']\n",
    "    \n",
    "    print(\"index\", data[\"index\"], \"which is a\", data[\"label\"])\n",
    "    display_uint8_image(image)\n",
    "    display_mask(mask)\n",
    "    display_uint8_image(tf.reshape(image, [28, 28]) * tf.cast(mask_to_image_mask(mask), tf.uint8))\n",
    "    display_float32_image(reconstructed_image)\n",
    "        \n",
    "        \n",
    "def image_performance_test(n=5):\n",
    "    for i in range(n):\n",
    "        gen_image_many_pixels(dataset_test)\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10000)\n",
    "    \n",
    "    global callbacks\n",
    "    callbacks += []\n",
    "    \n",
    "    txformer.fit(dataset, epochs=10, steps_per_epoch=40000, batch_size=batch_size, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clarkemaxw/conditional-mnist/env/lib/python3.8/site-packages/keras/engine/functional.py:582: UserWarning: Input dict contained keys ['index', 'image', 'label', 'target_row', 'target_col', 'target_val'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3999/40000 [=>............................] - ETA: 32:02 - loss: 6272.1436"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_image(dataset_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(dataset_test, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
