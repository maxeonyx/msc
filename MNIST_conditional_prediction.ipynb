{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:34iw632g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:34iw632g). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crimson-flower-39</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist/runs/2g5j90vf\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist/runs/2g5j90vf</a><br/>\n",
       "                Run data is saved locally in <code>/am/monterey/home1/clarkemaxw/conditional-mnist/wandb/run-20211001_204346-2g5j90vf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00]\n",
      " [ 7.0710683e-01  1.3921213e-01  2.4833918e-02  4.4166050e-03\n",
      "   7.0710677e-01  9.9026257e-01  9.9969161e-01  9.9999022e-01]\n",
      " [ 1.0000000e+00  2.7571312e-01  4.9652517e-02  8.8331243e-03\n",
      "  -4.3711388e-08  9.6123999e-01  9.9876654e-01  9.9996096e-01]\n",
      " [ 7.0710683e-01  4.0684462e-01  7.4440487e-02  1.3249470e-02\n",
      "  -7.0710677e-01  9.1349739e-01  9.9722546e-01  9.9991220e-01]\n",
      " [-8.7422777e-08  5.3005296e-01  9.9182546e-02  1.7665559e-02\n",
      "  -1.0000000e+00  8.4796453e-01  9.9506927e-01  9.9984396e-01]], shape=(5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(mask):\n",
    "    image_mask = tf.reshape(mask, [28, 28])\n",
    "    return image_mask\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=1000):\n",
    "\n",
    "    i = tf.range(dims//2, dtype=tf.float32)\n",
    "    i = tf.expand_dims(i, -2)\n",
    "    \n",
    "    vals = tf.expand_dims(vals, -1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = vals / tf.pow(scale, 2.*i/dims)\n",
    "    \n",
    "    sin = tf.sin(rate)\n",
    "    cos = tf.cos(rate)\n",
    "    \n",
    "#     # expand dims to allow alternating concat\n",
    "#     sin = tf.expand_dims(sin, -1)\n",
    "#     cos = tf.expand_dims(cos, -1)\n",
    "    \n",
    "    encoding = tf.concat([sin, cos], axis=-1)\n",
    "    \n",
    "#     encoding = tf.reshape(encoding, [-1, dims])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(positional_encoding(tf.constant([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]), 8))\n",
    "\n",
    "def img_to_tuples(img):\n",
    "    \n",
    "    height, width = img.shape\n",
    "    length = height * width\n",
    "    vals = tf.reshape(img, [length])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1])\n",
    "    cols = tf.reshape(cols, [-1])\n",
    "    \n",
    "    # permute the order, to ensure the network uses the positional encoding and not the implicit locaiton\n",
    "    idxs = tf.range(length)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    rows = tf.gather(rows, idxs)\n",
    "    cols = tf.gather(cols, idxs)\n",
    "    vals = tf.gather(vals, idxs)\n",
    "    \n",
    "    return vals, rows, cols\n",
    "\n",
    "def random_mask():\n",
    "    idxs = tf.range(784)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "    idxs = idxs[:n]\n",
    "    return idxs_to_multihot(idxs)\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask), np.uint8)\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "positions = tf.range(-28, 28, dtype=tf.float32)\n",
    "encodings = positional_encoding(positions, 16, scale=28)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "im = ax.imshow(encodings)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def make_dataset_generator(x, y, seed, typ='single pixel'):\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    # keep track of the index in the original MNIST\n",
    "    def to_dict(i, xy):\n",
    "        image, label = xy\n",
    "        data = {}\n",
    "        data['index'] = i\n",
    "        data['image'] = image\n",
    "        data['label'] = label\n",
    "        return data\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.map(to_dict)\n",
    "    \n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # add a transformation of MNIST images into val, row, col\n",
    "    def add_tuples(data):\n",
    "        data['val'], data['row'], data['col'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    dataset = dataset.map(add_tuples)\n",
    "    \n",
    "    # create a mask of random pixels masked out\n",
    "    def add_mask(data):\n",
    "        data['mask'] = random_mask()\n",
    "        return data\n",
    "    dataset = dataset.map(add_mask)\n",
    "    \n",
    "    # mask out a square region as well as random pixels\n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask()\n",
    "        data['mask'] = tf.logical_and(mask, square_mask)\n",
    "        return data\n",
    "    dataset = dataset.map(add_square_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate training pairs\n",
    "    \n",
    "    def single_pixel(data):\n",
    "        data['target_val'] = tf.cast(data['image'][data['target_row'], data['target_col']], tf.float32)\n",
    "        \n",
    "#         mask_out_target_pixel = False\n",
    "#         if mask_out_target_pixel:\n",
    "#             target_idx = data['target_row'] * 28 + data['target_col']\n",
    "#             target_mask = idxs_to_onehots(target_idx)\n",
    "#             data['mask'] = tf.logical_and(data['mask'], target_mask)\n",
    "        \n",
    "        # offset positions relative to target pixel so target is at 0,0\n",
    "        data['row'] = data['row'] - tf.cast(data['target_row'], tf.float32)\n",
    "        data['col'] = data['col'] - tf.cast(data['target_col'], tf.float32)\n",
    "        \n",
    "        return (data, data['target_val'])\n",
    "    \n",
    "    def single_pixel_random_rowcol(data):\n",
    "        data['target_row']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        data['target_col']  = tf.random.uniform([], minval=0, maxval=28, dtype=tf.int32)\n",
    "        \n",
    "        return single_pixel(data)\n",
    "        \n",
    "    def many_single_pixels(data):\n",
    "        rows = tf.range(28)\n",
    "        cols = tf.range(28)\n",
    "        cols, rows = tf.meshgrid(rows, cols)\n",
    "        \n",
    "        rows = tf.reshape(rows, [-1])\n",
    "        cols = tf.reshape(cols, [-1])\n",
    "        \n",
    "        image = data['image']\n",
    "        val = data['val']\n",
    "        row = data['row']\n",
    "        col = data['col']\n",
    "        mask = data['mask']\n",
    "        label = data['label']\n",
    "        index = data['index']\n",
    "        \n",
    "        def data_plus_pixel_index(i):\n",
    "            new_datum = {}\n",
    "            new_datum['pix_index'] = i\n",
    "            new_datum['target_row'] = rows[i]\n",
    "            new_datum['target_col'] = cols[i]\n",
    "            return new_datum\n",
    "        \n",
    "        def add_original(new_datum):\n",
    "            \n",
    "            new_datum['index'] = index\n",
    "            new_datum['val'] = val\n",
    "            new_datum['row'] = row\n",
    "            new_datum['col'] = col\n",
    "            new_datum['image'] = image\n",
    "            new_datum['mask'] = mask\n",
    "            new_datum['label'] = label\n",
    "            \n",
    "            return new_datum\n",
    "        \n",
    "        d = tf.data.Dataset.range(784)\n",
    "        d = d.map(data_plus_pixel_index)\n",
    "        d = d.map(add_original)\n",
    "        d = d.map(single_pixel)\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    # single pixel example. the row & col are translated by a random\n",
    "    # amount and the target val is the new pixel at 0,0\n",
    "    if typ == 'single pixel':\n",
    "        dataset = dataset.map(single_pixel_random_rowcol)\n",
    "    \n",
    "    # 'many single pixels' generates 784 single pixels from each image,\n",
    "    # and the target vals are each pixel in turn, translated so that\n",
    "    # they are at 0,0\n",
    "    elif typ == 'many single pixels':\n",
    "        dataset = dataset.interleave(many_single_pixels, block_length=784)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "def make_datasets():\n",
    "\n",
    "    train = make_dataset_generator(x_train, y_train, seed=192_168_1_1)\n",
    "    test = make_dataset_generator(x_test, y_test, seed=10_1_1_1, typ='many single pixels')\n",
    "    \n",
    "    return train, iter(test)\n",
    "\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 551 which is a 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nL2Qr0/DUBDHPyWoYVu74aB2BNlZKrtaJmF/AwmygMcNcCSojSKb4JagcaVyyWYgYwrkuz7ESrP+srtn7u7zvve+72D7YWwWroft6FV4m1Qv+qLW5+dx3djdpKu7RWjZ9tn+6dt9RXrSAsAUFTUbGYjuAuzUwUSnNMKe8freCD39UlTacOlnuZP9Pod90IdZrnVYHrY3eJZUUplOf/tl5scq31A2It/tx8Fi+cB34l5YyznHRZ2MTQBiFbT/mwZA59yEmzlYR17v+qn43EiJ+mqDPRMVtEperkRPsN1RKmlQ9knnUybDmagaHRBp0ZJKNKwicGIl46Bbh7Ydfw6TbxE91l9bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7074326100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nJWQQRYAIQhCvf+l/2xqNCNLFz1SEdEsBHYLNvy/ggyeZSu1hPvBsqAL6lXVLsrR8aPy0WRtMd+VAS6sfNQnrdly7GqMKfkLrobhnnOyJD7clIC1rixF7oR85WQ8xCu4iQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F707422DA00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAPklEQVR4nGNgGAzgLw3N3k62TnJddRCbYBDZzsANgq6SrVUEj5wclI4m0+xmbIJr8Gj4z/CfAWdEke9HagMApM4KMg4K+VkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70003E5F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 395 which is a 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA40lEQVR4nNXQr4oCURQG8MMGZYIYJrkwGMRNlmWD7pZ5AWGb4iuYNm4Tk2Xt+gCGaTJYhw2zyIKCmCwGk8EgumD7Pq/hWsbhtC2edDm/czl/RP4rXvY/Ou4Ra5TtEQ0NK+Dfs4YB2NLsk+xr5q0wzyv2+IttQfu4ALqavRPrstaQhjX79P1brIPjjO19OIZuEgPwzVbNQFQT9rRB6IhIPdghha/Al4jIN9YfEefJlaZk/zqVR9O2yYcrGmPKjjsw50VpYIbD5DwjgLMlQOAUF282cScgABCnafoIuchip5m2+4gLLglvTvMp00gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C064040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nJWQQRYAIQhCvf+l/2xqNCNLFz1SEdEsBHYLNvy/ggyeZSu1hPvBsqAL6lXVLsrR8aPy0WRtMd+VAS6sfNQnrdly7GqMKfkLrobhnnOyJD7clIC1rixF7oR85WQ8xCu4iQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7040771EB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAR0lEQVR4nGNgoBYwxi99GJ9kGNVcgQB/cUtdx6fvDxqfiQpuYWBgsCBL10q8sn82kWUoAwMDQy8DgyxOY6F0NJlm4wn5IQ0AuMwJrBd4sYUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F707422DA00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix_index shape ()\n",
      "pix_index dtype <dtype: 'int64'>\n",
      "target_row shape ()\n",
      "target_row dtype <dtype: 'int32'>\n",
      "target_col shape ()\n",
      "target_col dtype <dtype: 'int32'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "val shape (784,)\n",
      "val dtype <dtype: 'float32'>\n",
      "row shape (784,)\n",
      "row dtype <dtype: 'float32'>\n",
      "col shape (784,)\n",
      "col dtype <dtype: 'float32'>\n",
      "image shape (28, 28)\n",
      "image dtype <dtype: 'uint8'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'uint8'>\n",
      "target_val shape ()\n",
      "target_val dtype <dtype: 'float32'>\n",
      "index 33 which is a 4\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA80lEQVR4nGNgGFAgwIRHcvcGHwRHjIWBgQFJ9Tk/VQSnoB1VZ86/Qjjb9cc/VJ2ZSAod2M6hSOpII0m6MjShSFryMXyFsdlZvx9CtpHnxr+1cE7Yv5kozpn9758+nLMDIgkz1i+U4cFNmBy/GIo+vtP//hnAeQb//pkhSar8+7eWGVlSGSHnfubfLUkGRh4eHnYeHkYGg39rGRkYGBhYGBgYGBj8jRjYqxmY0xkYzxkyxGx2Y/jwH6Hz5T8YuPXw8r/TD/6ZMiB0Nk1mePTyC8O+MwynuN7Y5RpfvIXkHhZTUxkuOG/Kv2kMOIGoMhduycELALWdTCRsFwZNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C064A30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nJWQQRYAIQhCvf+l/2xqNCNLFz1SEdEsBHYLNvy/ggyeZSu1hPvBsqAL6lXVLsrR8aPy0WRtMd+VAS6sfNQnrdly7GqMKfkLrobhnnOyJD7clIC1rixF7oR85WQ8xCu4iQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAANElEQVR4nGNgGO7AFVOoEI3PhF3nKUzJr3CWGUWOIhb8wyu7lplsg6fgMRsmcZls04cnAADyUAfBiO7gqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F700073FEE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 33 which is a 4\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA80lEQVR4nGNgGFAgwIRHcvcGHwRHjIWBgQFJ9Tk/VQSnoB1VZ86/Qjjb9cc/VJ2ZSAod2M6hSOpII0m6MjShSFryMXyFsdlZvx9CtpHnxr+1cE7Yv5kozpn9758+nLMDIgkz1i+U4cFNmBy/GIo+vtP//hnAeQb//pkhSar8+7eWGVlSGSHnfubfLUkGRh4eHnYeHkYGg39rGRkYGBhYGBgYGBj8jRjYqxmY0xkYzxkyxGx2Y/jwH6Hz5T8YuPXw8r/TD/6ZMiB0Nk1mePTyC8O+MwynuN7Y5RpfvIXkHhZTUxkuOG/Kv2kMOIGoMhduycELALWdTCRsFwZNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nJWQQRYAIQhCvf+l/2xqNCNLFz1SEdEsBHYLNvy/ggyeZSu1hPvBsqAL6lXVLsrR8aPy0WRtMd+VAS6sfNQnrdly7GqMKfkLrobhnnOyJD7clIC1rixF7oR85WQ8xCu4iQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAANElEQVR4nGNgGO7AFVOoEI3PhF3nKUzJr3CWGUWOIhb8wyu7lplsg6fgMRsmcZls04cnAADyUAfBiO7gqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 33 which is a 4\n",
      "pix_idx: 782\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA80lEQVR4nGNgGFAgwIRHcvcGHwRHjIWBgQFJ9Tk/VQSnoB1VZ86/Qjjb9cc/VJ2ZSAod2M6hSOpII0m6MjShSFryMXyFsdlZvx9CtpHnxr+1cE7Yv5kozpn9758+nLMDIgkz1i+U4cFNmBy/GIo+vtP//hnAeQb//pkhSar8+7eWGVlSGSHnfubfLUkGRh4eHnYeHkYGg39rGRkYGBhYGBgYGBj8jRjYqxmY0xkYzxkyxGx2Y/jwH6Hz5T8YuPXw8r/TD/6ZMiB0Nk1mePTyC8O+MwynuN7Y5RpfvIXkHhZTUxkuOG/Kv2kMOIGoMhduycELALWdTCRsFwZNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C064820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nJWQQRYAIQhCvf+l/2xqNCNLFz1SEdEsBHYLNvy/ggyeZSu1hPvBsqAL6lXVLsrR8aPy0WRtMd+VAS6sfNQnrdly7GqMKfkLrobhnnOyJD7clIC1rixF7oR85WQ8xCu4iQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F700073FCD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAANElEQVR4nGNgGO7AFVOoEI3PhF3nKUzJr3CWGUWOIhb8wyu7lplsg6fgMRsmcZls04cnAADyUAfBiO7gqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038C280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 33 which is a 4\n",
      "pix_idx: 783\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA80lEQVR4nGNgGFAgwIRHcvcGHwRHjIWBgQFJ9Tk/VQSnoB1VZ86/Qjjb9cc/VJ2ZSAod2M6hSOpII0m6MjShSFryMXyFsdlZvx9CtpHnxr+1cE7Yv5kozpn9758+nLMDIgkz1i+U4cFNmBy/GIo+vtP//hnAeQb//pkhSar8+7eWGVlSGSHnfubfLUkGRh4eHnYeHkYGg39rGRkYGBhYGBgYGBj8jRjYqxmY0xkYzxkyxGx2Y/jwH6Hz5T8YuPXw8r/TD/6ZMiB0Nk1mePTyC8O+MwynuN7Y5RpfvIXkHhZTUxkuOG/Kv2kMOIGoMhduycELALWdTCRsFwZNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C064A30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAX0lEQVR4nJWQQRYAIQhCvf+l/2xqNCNLFz1SEdEsBHYLNvy/ggyeZSu1hPvBsqAL6lXVLsrR8aPy0WRtMd+VAS6sfNQnrdly7GqMKfkLrobhnnOyJD7clIC1rixF7oR85WQ8xCu4iQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAANElEQVR4nGNgGO7AFVOoEI3PhF3nKUzJr3CWGUWOIhb8wyu7lplsg6fgMRsmcZls04cnAADyUAfBiO7gqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038C280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 666 which is a 7\n",
      "pix_idx: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuklEQVR4nGNgGJ6AzXPxtn///m3DJqe58++/v+fu/d0KE2CBS8UbRzLtXrv/y16GSxjaPP793CDPwMDg8/evCIbk7H+tDAwMDAwH/t7BtLBpDjsDAwOD+b8fJjgdPO/vdZxy9r9eaSB4TKiSgswvbuDUefhvPU456f/vpHBK9v2zxykn8PmfEDIfxUGcXA9+4JQMYdj2DadkNOMJBpyS//+/weke4Ud/ccoxqP5Fk2RBYj/b+RS3TgwAAEumPT1N4lSbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C064160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAT0lEQVR4nMWSQQqAQAwDB///5/GyyMJqAitiDj00pGlDAQFAUEZZIA7CqfcEE1lw718MG92Um9u+uTMpjz3Zb/hip5xtSqgPjtRcvH5DOAF+hCTgWh4jZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F700073FEE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F700073FC40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 666 which is a 7\n",
      "pix_idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuklEQVR4nGNgGJ6AzXPxtn///m3DJqe58++/v+fu/d0KE2CBS8UbRzLtXrv/y16GSxjaPP793CDPwMDg8/evCIbk7H+tDAwMDAwH/t7BtLBpDjsDAwOD+b8fJjgdPO/vdZxy9r9eaSB4TKiSgswvbuDUefhvPU456f/vpHBK9v2zxykn8PmfEDIfxUGcXA9+4JQMYdj2DadkNOMJBpyS//+/weke4Ud/ccoxqP5Fk2RBYj/b+RS3TgwAAEumPT1N4lSbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C064550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAT0lEQVR4nMWSQQqAQAwDB///5/GyyMJqAitiDj00pGlDAQFAUEZZIA7CqfcEE1lw718MG92Um9u+uTMpjz3Zb/hip5xtSqgPjtRcvH5DOAF+hCTgWh4jZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE038CAF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shape_summary(data):\n",
    "    for name, v in data.items():\n",
    "        print(name, \"shape\", v.shape)\n",
    "        print(name, \"dtype\", v.dtype)\n",
    "\n",
    "def el_summary(data):\n",
    "    print(\"index\", data[\"index\"].numpy(), \"which is a\", data[\"label\"].numpy())\n",
    "    if 'pix_index' in data:\n",
    "        print(\"pix_idx:\", data[\"pix_index\"].numpy())\n",
    "    display_uint8_image(data[\"image\"])\n",
    "    display_mask(data[\"mask\"])\n",
    "    display_uint8_image(tf.reshape(data[\"image\"], [28, 28]) * tf.cast(mask_to_image_mask(data[\"mask\"]), tf.uint8))\n",
    "\n",
    "def train_summary(d):\n",
    "    data, target = next(iter(d))\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(iter(d))\n",
    "    el_summary(data)\n",
    "\n",
    "def test_summary(d):\n",
    "    data, target = next(d)\n",
    "    shape_summary(data)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    for i in range(780):\n",
    "        next(d)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "    data, target = next(d)\n",
    "    el_summary(data)\n",
    "\n",
    "train_summary(dataset_train)\n",
    "\n",
    "\n",
    "# TODO: TEST DATASET GENERATOR DOES NOT WORK HOW I EXPECT.\n",
    "#       IT SHOULD PRODUCE 784 EXAMPLES with the SAME image and mask, then change\n",
    "#       to a different image and mask.\n",
    "\n",
    "test_summary(dataset_test)\n",
    "\n",
    "# reset datasets after summary, because it consumes elements\n",
    "dataset_train, dataset_test = make_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = layers.Dropout(dropout_rate)\n",
    "    dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        attn_output = attn(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = dropout1(attn_output)\n",
    "        if include_residual:\n",
    "            attn_output = inputs + attn_output\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "#         attn_output = attn_output * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        attn_output = layernorm1(attn_output)\n",
    "        dense_output = dense_net_1(attn_output)\n",
    "        dense_output = dense_net_2(dense_output)\n",
    "        dense_output = dropout2(dense_output)\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 32\n",
    "    pointwise_feedforward_dim = 8000\n",
    "\n",
    "    val = keras.Input(shape=[784], name='val', batch_size=batch_size)\n",
    "    row = keras.Input(shape=[784], name='row', batch_size=batch_size)\n",
    "    col = keras.Input(shape=[784], name='col', batch_size=batch_size)\n",
    "    mask = keras.Input(shape=[784], name='mask', batch_size=batch_size, dtype=tf.bool)\n",
    "    \n",
    "    print(val.shape)\n",
    "    print(row.shape)\n",
    "    print(col.shape)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    row_pos_enc = positional_encoding(row, n_embd//2)\n",
    "    col_pos_enc = positional_encoding(col, n_embd//2)\n",
    "    \n",
    "    print(row_pos_enc.shape)\n",
    "    print(col_pos_enc.shape)\n",
    "    \n",
    "    pos_enc = tf.concat([row_pos_enc, col_pos_enc], axis=-1)\n",
    "    print(pos_enc.shape)\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    \n",
    "#     rows = tf.expand_dims(rows, -1)\n",
    "#     cols = tf.expand_dims(cols, -1)\n",
    "    \n",
    "#     m = tf.concat([vals,rows,cols], axis=-1)\n",
    "    \n",
    "    m = layers.Reshape((784, 1))(val)\n",
    "#     m = layers.Concatenate()([m, rows_pos_enc, cols_pos_enc])\n",
    "    m = layers.Dense(pointwise_feedforward_dim, activation='relu')(m)\n",
    "    m = layers.Dense(n_embd, activation=None)(m)\n",
    "    \n",
    "#     print(m.shape)\n",
    "    \n",
    "    m = m + pos_enc\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=8, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=2, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "#     m = transformer_block(n_embed_dim=n_embd, n_heads=2, n_dense_dim=pointwise_feedforward_dim, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    \n",
    "    m = layers.Flatten()(m)\n",
    "    \n",
    "    m = layers.Dense(1, activation='relu')(m)\n",
    "    \n",
    "    val_out = layers.Reshape([], name='val_out')(m)\n",
    "    \n",
    "    model = keras.Model(inputs=[val, row, col, mask], outputs=[val_out])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784)\n",
      "(8, 784, 16)\n",
      "(8, 784, 16)\n",
      "(8, 784, 32)\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "row (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_28 (TFOpLambda)  (8, 784, 1)          0           row[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_29 (TFOpLambda)  (8, 784, 1)          0           col[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "val (InputLayer)                [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_28 (TFOpLambda) (8, 784, 8)          0           tf.expand_dims_28[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_29 (TFOpLambda) (8, 784, 8)          0           tf.expand_dims_29[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (8, 784, 1)          0           val[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_28 (TFOpLambda)     (8, 784, 8)          0           tf.math.truediv_28[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_28 (TFOpLambda)     (8, 784, 8)          0           tf.math.truediv_28[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sin_29 (TFOpLambda)     (8, 784, 8)          0           tf.math.truediv_29[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.cos_29 (TFOpLambda)     (8, 784, 8)          0           tf.math.truediv_29[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (8, 784, 8000)       16000       reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_42 (TFOpLambda)       (8, 784, 16)         0           tf.math.sin_28[0][0]             \n",
      "                                                                 tf.math.cos_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_43 (TFOpLambda)       (8, 784, 16)         0           tf.math.sin_29[0][0]             \n",
      "                                                                 tf.math.cos_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               [(8, 784)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (8, 784, 32)         256032      dense_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_44 (TFOpLambda)       (8, 784, 32)         0           tf.concat_42[0][0]               \n",
      "                                                                 tf.concat_43[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_56 (Sl (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_57 (Sl (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_70 (TFOpLa (8, 784, 32)         0           dense_95[0][0]                   \n",
      "                                                                 tf.concat_44[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_28 (TFOpLam (8, 784, 784)        0           tf.__operators__.getitem_56[0][0]\n",
      "                                                                 tf.__operators__.getitem_57[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_28 (MultiH (8, 784, 32)         33568       tf.__operators__.add_70[0][0]    \n",
      "                                                                 tf.__operators__.add_70[0][0]    \n",
      "                                                                 tf.math.logical_and_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (8, 784, 32)         0           multi_head_attention_28[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_71 (TFOpLa (8, 784, 32)         0           tf.__operators__.add_70[0][0]    \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_56 (LayerNo (8, 784, 32)         64          tf.__operators__.add_71[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (8, 784, 8000)       264000      layer_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (8, 784, 32)         256032      dense_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (8, 784, 32)         0           dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_72 (TFOpLa (8, 784, 32)         0           layer_normalization_56[0][0]     \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_58 (Sl (8, 784, 1)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_59 (Sl (8, 1, 784)          0           mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_57 (LayerNo (8, 784, 32)         64          tf.__operators__.add_72[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_and_29 (TFOpLam (8, 784, 784)        0           tf.__operators__.getitem_58[0][0]\n",
      "                                                                 tf.__operators__.getitem_59[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_29 (MultiH (8, 784, 32)         33568       layer_normalization_57[0][0]     \n",
      "                                                                 layer_normalization_57[0][0]     \n",
      "                                                                 tf.math.logical_and_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (8, 784, 32)         0           multi_head_attention_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_73 (TFOpLa (8, 784, 32)         0           layer_normalization_57[0][0]     \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_58 (LayerNo (8, 784, 32)         64          tf.__operators__.add_73[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (8, 784, 8000)       264000      layer_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (8, 784, 32)         256032      dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (8, 784, 32)         0           dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_74 (TFOpLa (8, 784, 32)         0           layer_normalization_58[0][0]     \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_59 (LayerNo (8, 784, 32)         64          tf.__operators__.add_74[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (8, 25088)           0           layer_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (8, 1)               25089       flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "val_out (Reshape)               (8,)                 0           dense_100[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,404,577\n",
      "Trainable params: 1,404,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss={ \"val_out\": keras.losses.MeanSquaredError() })\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, n=1):\n",
    "    \n",
    "    dataset = dataset.batch(n)\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        outputs = txformer(batch)\n",
    "        for i in range(n):\n",
    "            print(\"index\", batch[\"index\"][i], \"which is a\", batch[\"label\"][i])\n",
    "            display_uint8_image(batch[\"image\"][i])\n",
    "            display_mask(batch[\"mask\"][i])\n",
    "            display_uint8_image(tf.reshape(batch[\"image\"][i], [28, 28]) * tf.cast(mask_to_image_mask(batch[\"mask\"][i]), tf.uint8))\n",
    "            display_float32_image(outputs[i])\n",
    "\n",
    "def gen_image_many_pixels(dataset):\n",
    "    \n",
    "    # assume dataset is a 'many single pixel dataset'\n",
    "    # so it has runs of 784 examples, one for each pixel in an mnist digit\n",
    "#     dataset = dataset.take(784)\n",
    "#     batch_size = 32\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     reconstructed_image = np.zeros([28, 28])\n",
    "#     for batch, batch_targ in dataset:\n",
    "#         inputs = [batch['val'], batch['row'], batch['col'], batch['mask']]\n",
    "#         out_vals = txformer(inputs)\n",
    "        \n",
    "#         # np can do this yay\n",
    "#         reconstructed_image[batch['target_row'], batch['target_col']] = out_vals\n",
    "\n",
    "    reconstructed_image = np.ones([28, 28]) * 230\n",
    "#     for row in range(28):\n",
    "#         for col in range(28):\n",
    "#             data, targ = next(dataset)\n",
    "#             inputs = [data['val'], data['row'], data['col'], data['mask']]\n",
    "#             inputs = [tf.expand_dims(x, 0) for x in inputs]\n",
    "            \n",
    "#             out_vals = txformer(inputs)\n",
    "#             reconstructed_image[data['target_row'], data['target_col']] = out_vals\n",
    "\n",
    "    image = data['image']\n",
    "    mask = data['mask']\n",
    "    \n",
    "    print(\"index\", data[\"index\"], \"which is a\", data[\"label\"])\n",
    "    display_uint8_image(image)\n",
    "    display_mask(mask)\n",
    "    display_uint8_image(tf.reshape(image, [28, 28]) * tf.cast(mask_to_image_mask(mask), tf.uint8))\n",
    "    display_float32_image(reconstructed_image)\n",
    "        \n",
    "        \n",
    "def image_performance_test(n=5):\n",
    "    for i in range(n):\n",
    "        gen_image_many_pixels(dataset_test)\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10000)\n",
    "    txformer.fit(dataset, epochs=1, steps_per_epoch=4000, batch_size=batch_size, callbacks=[WandbCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(112, shape=(), dtype=int64) which is a tf.Tensor(3, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvklEQVR4nGNgGGSAEc5qrma4eYbhzYaD2CTdqtgYGFSEP7idxWVQ2N+/ExA8JlTJXwwMP3FpVHzy97EADjmeJ//fReOQk1n29501DjnPe3/PW6GIIDnIUZ7h5jFcrhFY9PfvHHZcsgz55/+/8sYpy972860bTlmGwr9fVLA5iIGBgYHhDANnNi5J6SoGhsu4TE3+9++TFA45j+9/v4ZjlQlTCfr872slVrmony///r0YiN1I0Q9/vzXicgtVAQBFqD22YrR9RwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7150698910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbElEQVR4nJWSSxbAIAgD4/0PPV3U+lADWhbKxwSMSpLQZ92DJRGOGPPFLFtRMdb1FGkQE519b9EkWjHlvC3IUYoMLx0HaTztJu8vQxIUUKfffb9t2FybCUX1tKZ2FiHvnH7L/OqmcYyILhbwAIWEPsehExpPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CDB20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFUlEQVR4nGNgGAWjYJCDqIF2AAQAADKiAFvVEAKoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70007A1160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C4C47C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(365, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8klEQVR4nGNgGNSAOefF/6eZQtglG/7+/fv371lJbHJan/7UKChU/FyPRU7u2e8KBgYGhs2PpTDkWBb8PcXAwMDA0P5XGyLChJC0imUIQ1WOJOnHcPoFDl8wyLx56wBhtT+XQZds+3sWytqzB8NYc4ZdEIa9HYapAle/ikJY2/9moEvq/O2FMNgurWFFN9aD4SeEEaB9+je65BmGCAgjCItPdP5+ZmBgYGDQ/XWSEyYG1/n1AwMDAwODyVHm7d8xtU78rsHAEPX17wIOLOYG/X1mFfvn7zwmLHIMEmf//v37rp8VmxwDg9jEJwflsEtRFwAAnhBJ6d+fmNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7150698250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlklEQVR4nHWS0RLEIAgD1/v/f957UBQ4zpm20kgIERARBBWQWJ7A8/FBIBpQfuAAKS5p83bTVeTRBWFIaaQJUoIpqRmWI+IYtc61tJdVvYLXtHqw0Jbcwayi5R/Q5dr1PHwNGQtZAJ8OrvsalNkFzV7aduTbed6YnalDGPf/W6zOyybb5J+j20u23F3aWdQ3r/e39goCX4jmjICV9vJwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7150061D30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAVUlEQVR4nGNgGPTgaSZ5+moIKdj8mDyDGRgY/kJpJixy7SSa1UaUKvtfJBpLFAj7C2dieiUIj0ZdhpOc5Fn5dwE2UYhdf7FJMTAwnMVv5BPyXEIqAAAnrw3p0hAyVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CDD00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70007A1160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(204, shape=(), dtype=int64) which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGATA8IUinM2ELikrZoFbkoHhDk5TOe9eYcSpM0Lx/3+cksEMu3Gaavj6nRBOyQn/NiLx0I39sxinRoF395C5qDpzBTYy4AJsJz844JRU+XcOhY9ibAjDZtySMQzvcEsyMDLgkfyPW9JGmuEZdpcyMDBM/X+VHadk3HN3nHJ0AwA+rB1Vi+8kXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F7150698940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nFWSQRLEMAjD5E7//2XtAZN0e2gaBgxyAVERRaGnCq9AAInMIxAR5jUlbh023mR7p8p/WjBiYRvBs+HMPRAyGT6NiCQzddqsOTGfMasqD4GYGTQTHo7MaINyvg7bS9gRiRBkmsBbzGAzDp48aiYfE+Kn9Bm04ahDHaslacdcmmW9pq7dc5dYQ+Pyz2TryiZbk1yld0tH99jaeW2zT88K+vjxjssfCO9cqzdG3gX47gbdJN3Y359w4l4Oztp5UIRlz3r4sYLd53N4B1rU7u0iV15/j3rgnVM+pRQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CDD00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAZUlEQVR4nGNgGBRAkVyN/0xxSnEyXGGEc5jQJL8z/P+PU+eWf7gtNHyNxzUTUDSi28kUglOjwDs8pjIw9OOUYWP4gGoHMkfu3z28xuIBeEKA4QpDPj5J3MDm/b9Q8pwTR5426gIAd9kUTpOVppIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE03B3130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70C41F0C70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clarkemaxw/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:588: UserWarning: Input dict contained keys ['index', 'image', 'label', 'target_row', 'target_col', 'target_val'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/4000 [..............................] - ETA: 3:59 - loss: 3284.9072WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0087s vs `on_train_batch_end` time: 0.0469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0087s vs `on_train_batch_end` time: 0.0469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 228s 57ms/step - loss: 7220.6069\n"
     ]
    }
   ],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(869, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nNXRMU/CQBjG8RciDpCGODiQwMLCJKMbYcBVjSYuDnwBvoAdTXR1YOE7uJMQYGJgoMHN0ZC41KgDxcQW87/IUEpK6RFWnu3u9+buuZzIXubEzIkYptlRfhohK9vMplMHFAA8H4rIwRIrx5IOH+T9hTCU9xeRrzvZwNeP7kC+J9Hx/BjA1LS9/AV+6hq9GAPurUbzjx5wk9DwtQXK0KBkLfXf1KFkR8q9Wq2S6+j0JHWuw/UEWKiVg61cJjrUwG6dHhUrbwpKUSx+AsMJKPqbj3nw/I9UzlPM7aM5AO5ZbLd7gHZ1W/2dswBIIW8rrMkjqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70742FEDF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAT0lEQVR4nMWSQQqAQAwDB///5/GyyMJqAitiDj00pGlDAQFAUEZZIA7CqfcEE1lw718MG92Um9u+uTMpjz3Zb/hip5xtSqgPjtRcvH5DOAF+hCTgWh4jZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70742FEDF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70742FE490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70742FEDF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(387, shape=(), dtype=int64) which is a tf.Tensor(2, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAyklEQVR4nGNgGGSAEcFUb2Bn2LD0LzZVCnO/fn706F81Njmr11caFBgY2h7yYMo1fJzOx8DAwMCwexKGXMaPCpgJH4TR5K7es4UxJT6FIcRZGBgYGOz/fIDxX6yTweYkKFB7imAzoUtKPscj6fMQt6nOXyfikuLI/fRUDKsMm0XPg3+/HbDKyZ76+/fvv1f1etgkzf79+/fv/79//05mC0KFEFEmaP/8GgODXoCeoci3FSvPvcXuKr6oPZ//XsflZgYGxWxb3JIDCAD5O0F3nT8EbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70742FEBE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkklEQVR4nKWSQQ7EMAgDh/z/z9MDgaSV0l3t5oJqsI3TIIgIQJcG0IQ1+6I556TODwujkRYpcdvCYm4sAIZAEBDcrJfKFJ/cNgkhEo19PhZvLdthOtgjPz2+rsdt/6zTL2u7WqvcI6+LVWuJ8tkymtH3EN8e/+p7bMenfY7McRat3/HjOQuPN9nxEmU9qk2hHg0XS2l+m8lk5GYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F715036F250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAOUlEQVR4nGNgGMRg+ToGBmasMgpzvzIwMPzDKmf1+goeM6cTY/EHYTSBqwimBIZqEQFizBwFQx0AADPeCK3qtq7dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CDBB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F6FE06F41F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(750, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBElEQVR4nGNgGGAgXDDn5r/78VjlBB/9/buufem7jRxYJMv+/p3FwMBg+Xo7D4Zc6K+/BawMDAwMW/5FY0j++1vGysDAwMBg868dTYpr9f+PclD2/1dokvp/f6bD2Bdeokku+5sEZ895L4Ei5/n7L4Iz568fAwMDAwMTlM/GdAtZrSqKZCLjEYTUK0YbFMn//9chJPf+/48iycDwESEZCaURklHIdp5FcW3X34Nwttrzt0Iokv7//olDmVwX/3egyDHw7Ph7XoqBgYGBQefq39sqqJIMMq//3QxhYBAOe/r3gBwDOjC9//f7q1fv/37IZYUJMSJk+SIrfjAc+TH1BoY+egIAhrdW1bRZonwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CDC10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAt0lEQVR4nGWSQRLEIAgEe7by/y/3HkTEJAcTQUamCYgqIGtRYL3s/UpWamXPZj4imqpIxSDG2q5iP3UixIDpKGkhU+ccRS22e/WkRp9PthJDGghm6cxMed7WhXnnCr/gnMtG7ENJdrcD9QkfMa9+jjsXoSkZcJt7coFvlvWhw6jeY7QXbhqQjevEClzMHnI6mX75u9l28S6YaC8O1eJERA94z3scvNhy27Ksfcy9p/5gMPWnxYvmH7WnyXGqoEVaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F715036F250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAZUlEQVR4nGNgGNTg7991DEvfbeTAo+T1dqzCBRDqXzQ2SVYGBgYGBhtKXIYHzHmPV9qPbIPXMzAwMDAwYZNyZviPWzISSrNgNfUvM5nO+Y9XVgW7sHDYUzya/sLiBhNcJ8pNtAQABPoSFomhDLUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C386640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F710C186940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(388, shape=(), dtype=int64) which is a tf.Tensor(1, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGPQg5d9S3JKf/8IlmdDlgjlx62O79PdnOi5J578/s3HqnPn3AE65lu9/XXBKXv67lgWXXOn/Mzy45IJf/+vCJSf16N98nBZW/f2rhEuO9eDfhTg1lv29KoJT8tCXKJxyc36uwikn/XAf7vjw/uuIU07w+Sw2nJK8++VxylEbAADYoSuGxnjblAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F70742FE0D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbklEQVR4nJWSSQ7AIAwDx1X//+XpgW6XuGrEAWG8kBDGkowIZJuZTRP7hcnzg1hARUraFNCmW1zrO0UcPYWWtlVvbfH87l9Da9oK7v/TAOJaazzn7hqVuAVzaRgCEc+peBs8FFDBff16gZeG6+gAMGpD5V8rlOMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CD9A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C386730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F710C186940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(813, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAUlEQVR4nNXQsS9DURTH8Z9HowZJiSiThKEWookYTUarjcFubiJdpDVIbPUv0NFgwGSowWSgS5GSWJpbUoI0XqLfGl6HW+++sPpNJ+dzcnLulf5Teqx6ckmz7XLxwzEWz98BcJBw4BZwtHENh2F7a/Ew16tY/+OLMp/HQ7YlfUxakrSwVmrSSNm4h5kPqtQNULBtxqfSKStQ3x+QJPUFHS/WodGVCel8teuc5AXP23Fp8xaa2eEfxw76UDemBaYYfkr+iSDLjj/QSC5XBUoJF0pjl7Bum2fVi2l91V4jUNLJqXuppqrcT0eYdiHb3fHcg6GMv1O7OovSAm12/rjpl3wDLgdqkF/bgrAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F71506CD7F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAeUlEQVR4nMWSQQrAIAwEZ/3/n6eHRNtLLRRKI5KFdaPCoIgqVFMQEQYRA6AJEBQgEuldi2tNk0A1Tz1ooSSkIjrVbfngb5Jj527NP+785kEf1f6f78eeDF3omnqQIrMhgxSsSdPW4SUiMRSrcTHZAwNmRfpIXxeDcACwsEoD1D6RZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F710C186940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAI0lEQVR4nGNgGAWjYNCDf/+qcUt+Znh+cR8uyUn//v+jkiMAqQAJbjTgChcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F715036F250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAABPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F702C3566A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_image(dataset_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)\n",
    "fit_one_epoch(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(dataset_test, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
