{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9vGgfSkcpL"
   },
   "source": [
    "\n",
    "# Conditional autoregressive transformer\n",
    "\n",
    "Train a transformer to predict missing pixel from mnist \n",
    "\n",
    "### plan\n",
    "\n",
    "* note to try padded mnist (relative encoding might require black padding???)\n",
    "* probably don't need positional encoding?\n",
    "* create transformer model\n",
    "* masking \n",
    "* randomised masking\n",
    "* relative position encoding (x - current_x, y - current_y, val)\n",
    "* train to predict when current pixel missing\n",
    "* train to predict when 10% are missing\n",
    "* train to predict when 90% are missing\n",
    "* train to predict when 99% are missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"txformer-bigger-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxeonyx\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crimson-pine-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/maxeonyx/conditional-mnist/runs/3me8ypk4\" target=\"_blank\">https://wandb.ai/maxeonyx/conditional-mnist/runs/3me8ypk4</a><br/>\n",
       "                Run data is saved locally in <code>/am/monterey/home1/clarkemaxw/conditional-mnist/wandb/run-20210930_041701-3me8ypk4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init weights and biases project\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "wandb.init(project='conditional-mnist', entity='maxeonyx')\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve GPU 0 only (for VUW machines)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 04:17:06.557238: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 04:17:07.121051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "tf.constant([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "def display_uint8_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display(Image.fromarray(image, \"L\"))\n",
    "\n",
    "def display_float32_image(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, 0]\n",
    "    if tf.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "    display_uint8_image(image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
      "   0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00]\n",
      " [ 7.0710683e-01  7.0710677e-01  1.3921213e-01  9.9026257e-01\n",
      "   2.4833918e-02  9.9969161e-01  4.4166050e-03  9.9999022e-01]\n",
      " [ 1.0000000e+00 -4.3711388e-08  2.7571312e-01  9.6123999e-01\n",
      "   4.9652517e-02  9.9876654e-01  8.8331243e-03  9.9996096e-01]\n",
      " [ 7.0710683e-01 -7.0710677e-01  4.0684462e-01  9.1349739e-01\n",
      "   7.4440487e-02  9.9722546e-01  1.3249470e-02  9.9991220e-01]\n",
      " [-8.7422777e-08 -1.0000000e+00  5.3005296e-01  8.4796453e-01\n",
      "   9.9182546e-02  9.9506927e-01  1.7665559e-02  9.9984396e-01]], shape=(5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def idxs_to_onehots(idxs, depth=784):\n",
    "    onehots = tf.one_hot(idxs, depth, dtype=tf.bool, on_value=False, off_value=True)\n",
    "    return onehots\n",
    "\n",
    "# takes 2D tensor (batch and index list)\n",
    "def idxs_to_multihot(idxs, depth=784):\n",
    "    onehots = idxs_to_onehots(idxs, depth)\n",
    "    multihot = tf.math.reduce_all(onehots, axis=len(onehots.shape)-2)\n",
    "    return multihot\n",
    "\n",
    "def idxs_to_attention_mask(idxs):\n",
    "    multihot = idxs_to_multihot(idxs)\n",
    "    attn_mask = tf.logical_and(multihot[:, :, None], multihot[:, None, :])\n",
    "    return attn_mask\n",
    "\n",
    "def mask_to_image_mask(mask):\n",
    "    image_mask = tf.reshape(mask, [28, 28])\n",
    "    return image_mask\n",
    "\n",
    "# scale is the max-min of vals\n",
    "# for mnist it's 28 because thats the width and height of the images\n",
    "def positional_encoding(vals, dims, scale=1000):\n",
    "\n",
    "    i = tf.range(dims//2, dtype=tf.float32)\n",
    "    i = tf.expand_dims(i, -2)\n",
    "    \n",
    "    vals = tf.expand_dims(vals, -1)\n",
    "    \n",
    "    # the bit inside the sin / cos\n",
    "    rate = vals / tf.pow(scale, 2.*i/dims)\n",
    "    \n",
    "    sin = tf.sin(rate)\n",
    "    cos = tf.cos(rate)\n",
    "    \n",
    "    # expand dims to allow alternating concat\n",
    "    sin = tf.expand_dims(sin, -1)\n",
    "    cos = tf.expand_dims(cos, -1)\n",
    "    \n",
    "    encoding = tf.concat([sin, cos], axis=-1)\n",
    "    encoding = tf.reshape(encoding, [-1, dims])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(positional_encoding(tf.constant([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]), 8))\n",
    "\n",
    "def img_to_tuples(img):\n",
    "    \n",
    "    height, width, chan = img.shape\n",
    "    length = height * width\n",
    "    vals = tf.reshape(img, [length, chan])\n",
    "    vals = tf.cast(vals, tf.float32)\n",
    "    rows = tf.range(height, dtype=tf.float32)\n",
    "    cols = tf.range(width, dtype=tf.float32)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    rows = tf.reshape(rows, [-1])\n",
    "    cols = tf.reshape(cols, [-1])\n",
    "    \n",
    "    # permute the order, to ensure the network uses the positional encoding and not the implicit locaiton\n",
    "    idxs = tf.range(length)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    \n",
    "    rows = tf.gather(rows, idxs)\n",
    "    cols = tf.gather(cols, idxs)\n",
    "    vals = tf.gather(vals, idxs)\n",
    "    \n",
    "    return vals, rows, cols\n",
    "\n",
    "def random_mask():\n",
    "    idxs = tf.range(784)\n",
    "    idxs = tf.random.shuffle(idxs)\n",
    "    n = tf.random.uniform(shape=[], maxval=784, dtype=tf.int32)\n",
    "    idxs = idxs[:n]\n",
    "    return idxs_to_multihot(idxs)\n",
    "\n",
    "def random_square_mask(maxsize=28):\n",
    "    height = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    width = tf.random.uniform(shape=[], minval=1, maxval=maxsize, dtype=tf.int32)\n",
    "    start_row = tf.random.uniform(shape=[], minval=0, maxval=maxsize-height, dtype=tf.int32)\n",
    "    start_col = tf.random.uniform(shape=[], minval=0, maxval=maxsize-width, dtype=tf.int32)\n",
    "    rows = tf.range(start_row, start_row + height)\n",
    "    cols = tf.range(start_col, start_col + width)\n",
    "    cols, rows = tf.meshgrid(rows, cols)\n",
    "    idxs = rows*maxsize+cols\n",
    "    idxs = tf.reshape(idxs, [-1])\n",
    "    return idxs_to_multihot(idxs, depth=maxsize*maxsize)\n",
    "\n",
    "def random_offset():\n",
    "    return tf.random.uniform(shape=[2], maxval=28, dtype=tf.int32)\n",
    "    \n",
    "def display_mask(mask):\n",
    "    image_mask = np.array(mask_to_image_mask(mask), np.uint8)\n",
    "    image_mask = image_mask * 255\n",
    "    display_uint8_image(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "positions = tf.range(28, dtype=tf.float32)\n",
    "encodings = positional_encoding(positions, 16, scale=28)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "im = ax.imshow(encodings)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow.data data generator\n",
    "\n",
    "from tensorflow import data as td\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def make_dataset_generator(split='train'):\n",
    "    \n",
    "    def split_enumerate(i, data):\n",
    "        data[\"index\"] = i\n",
    "        return data\n",
    "    \n",
    "    dataset = tfds.load('mnist', split=split, shuffle_files=True)\n",
    "    \n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.map(split_enumerate)\n",
    "    # shuffle the digits\n",
    "    dataset = dataset.shuffle(60000)\n",
    "    # repeat the dataset infinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "#     def map_just(component, func):\n",
    "#         def do(data):\n",
    "#             data[component] = func(data[component])\n",
    "#             return data\n",
    "#         return do\n",
    "    \n",
    "    def map_add(component, func):\n",
    "        def do(data):\n",
    "            data[component] = func()\n",
    "            return data\n",
    "        return do\n",
    "    \n",
    "    def add_tuples(data):\n",
    "        data['vals'], data['rows'], data['cols'] = img_to_tuples(data['image'])\n",
    "        return data\n",
    "    \n",
    "#     def offset_tuples(data):\n",
    "#         offset = tf.cast(data['offset'], tf.float32)\n",
    "#         data['tuples'] = tf.stack([\n",
    "#             data['tuples'][:, 0],\n",
    "#             data['tuples'][:, 1] + offset[0],\n",
    "#             data['tuples'][:, 1] + offset[1],\n",
    "#         ], axis=-1)\n",
    "#         data['tuples'] = tf.random.shuffle(data['tuples'])\n",
    "#         return data\n",
    "    \n",
    "    \n",
    "#     def add_target_px_val(data):\n",
    "#         offset = data['offset']\n",
    "#         data['target_px'] = tf.cast(data['image'][offset[0],offset[1]], tf.float32)\n",
    "#         return data\n",
    "    \n",
    "    def add_square_mask(data):\n",
    "        mask = data['mask']\n",
    "        square_mask = random_square_mask()\n",
    "        mask = tf.logical_and(mask, square_mask)\n",
    "        data['mask'] = mask\n",
    "        return data\n",
    "    \n",
    "    dataset = dataset.map(add_tuples)\n",
    "    # data['image'][:, 0] are vals\n",
    "    # data['image'][:, 1] are rows\n",
    "    # data['image'][:, 2] are cols\n",
    "    \n",
    "    dataset = dataset.map(map_add('mask', random_mask))\n",
    "    dataset = dataset.map(add_square_mask)\n",
    "#     dataset = dataset.map(map_add('offset', random_offset))\n",
    "#     dataset = dataset.map(offset_tuples)\n",
    "#     dataset = dataset.map(add_target_px_val)\n",
    "    \n",
    "#     if batch_size is not None:\n",
    "#         dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset_train = make_dataset_generator()\n",
    "dataset_test = make_dataset_generator(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape (28, 28, 1)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'int64'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "vals shape (784, 1)\n",
      "vals dtype <dtype: 'float32'>\n",
      "rows shape (784,)\n",
      "rows dtype <dtype: 'float32'>\n",
      "cols shape (784,)\n",
      "cols dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "index tf.Tensor(31512, shape=(), dtype=int64) which is a tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nGNgoAKIP49HcuFPazQRJgSTk6UCt847f5/hlLP5/281TmMV//+/h9tOhtcL8Ug+u4ZHcim6M5jQBZABC4zBHAhlCDo53d9yA1WVxN9/xQwMDAyhr/79/bsAzQj2M/+KGRgYzN/++/fv3z8pVEnmff+OMTAw3Pv7d4r8wr99qA76+4lBTo5BQfhfc84zBgYlNHNj//09y5DxdxGD1Oy/f2PRHZ7x79+df/8mLLz7798TdJ0M7Ml///77+/fvv78/HDD9zNn47t/fv3//3Y/BlGNgYJCM+/r3doc8VrlBDwDKsFiBGI7kcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5185779D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2ElEQVR4nHWSS3IFQQjDJO5/Z2eB6bwkld5MTX9sIxAIxkA/QsCIBGQPMRJuOSjJXoEeuT9DUBH2VUQwSAYCIUjQFQwQddN0kzxXcxHujOAFerL/rDH8PneTh8kL/op4NmPNn2tks8ukQj90KzxAPi2XU0HFhw6OxWZ0/iQNHuppbz7zpCLXx7P7ha8IdqtdbY97YSfgcyumEGqnx7JjM7xWmIT4mnqE2s8NUIALal8av6tvLZOOBKt3Ndb6BvWj1HLNqR3pLayYr5Xfo5NCiBnfJFwMrwC/ACTWjClwnM8YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5206AE160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjUlEQVR4nNWOMQrCUBQEB0EkpaVpYukdvIqx9gi26azEQg/wPUEuIh5AsUptJwiyS0rh+2OVQrd67LC8gT5y/sIC86gZvM9M614E/iRjDsw6qdW9tDF5Et3Evghso7o2MOUFw6A6gqV1YqUjOahMfLvCLmA3H2wEGMl6JmSy6m4hWCRdJ8uHLpsiyX4+LZ8VLhSHip6KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51839A3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(39285, shape=(), dtype=int64) which is a tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABC0lEQVR4nM3Qvy9DURjG8W8urpQmSrsxScRm82uQNC4xmiQkFnvDUEk3dotIF+0fwNguJgaRJiKmyjUZxOqSJm4aN/UUE07l3M3g3d7zyfue5xz4k3K9/ffquN26ilJbJTteSEfHChI2m3mJtp2UrwWLjT228kBVGxZM3iwCyWv12dYOOsCoDOz+wQaAR61ljws8mE9xfuEI9djBnY+rdJxNhc1Zo/0ONLm08nY4kTi5tMxshpKksAeA5ft1wwbudL7aUJAFIPOkioGeaulc9HVU0fO8gXnt5iLJTwH9B9HrnHnldFOSH6gAQ1tSuTNQXcGaW5bOTm+lktuJw3tZyBTVlsJCb8w3/O/6BNpcaceTbfv0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD518577E80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2ElEQVR4nFWSQRKEQAgDO1P7/y/3HgijaqnlEAIkRAjCfCIEjMScHuaGUTAYD0gyjxQzVwLYA0tIzJwqiEJh0r8buDEVZe7fELWIBCS37KAuwM1zBtouMVt4zk75VoYRodgfMU/VSmQEJ3NpMZAopqJNA7e7HUvv+3W5A3jKU/SkGRESfJSuyGYUpgo/g77kGj/fsYxVQTiTf6s+AyekTJsq8dr5QW/La2gWvQJtT3kZ787pLohwtrO6kq5DXuZ+tVu+Q1K1eDZrtrT0NebjgV3Th3IGTCf/A6bdw8ZN7YnLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51871E070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlElEQVR4nGNgoApgc2b4x6COXY6ZgYHhL8NMXFr/MDAwvMYh94OBQeDKH7w2J+KWOv2HC59O3JKpeG38g9MrDBDf4AL/8RqLE6RMYtiI274/n1kZGBgYGPwZYpCE+SEUNFxF3vxZj6kTzniLLFz8ByIpwMDAwM3A8N0WWdIcSWs+Hr9e/cPAwIZFXASiuZwdp87BDABoqiOBKdYt1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51871E8E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(51144, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4UlEQVR4nGNgoD9gRDC9IhkYGKR13c9hUWb649+/f//+/buMEGKBs36yMaQeZGBI58Nqw79/KmgiTPgchCS5DJ/kf3ySDAx+aJII1zLcYnD+I/+QYcUrbNbrf//379//f2dEsDrO1Lr66euj/85glWSQ6dnGK3HtTwU2OY5Nm7kZGIz+bcAmWf9OnoGBgePyP2ySl3YyMDAwMJzAJmnwo4GBgYFB9T88NJD8mcv2n4GBgcf//3QsOqf92yjKwD3l33P0yGFgYGAQOPTv2Ykb/27JY3MPg8Dif//+TcWmj44AAD6IRmc5d8BsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD518620730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAeUlEQVR4nH2SSxLAMAhCnd7/zq+LOvUDiZs2Cgk4RFQRu+A0Q04fFjMjO8gDs0NjKtbI66h1k2UjE9b3P+FlpNFU6p9pKzCAEJNjx4bI7thbs57j9Ma6IDQCbTu1JqZkF5eyx/IJQnM5CrcXY2Cqk+zNJLgE9b8GewHQf2WdNfhTUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51839A340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAARklEQVR4nGNgGFDgRZl2lW7s4v8oMxaPhbQymIGBgUEEu7A1AwMDA8MZWtpMEvhBWEkJrggQZWBgwB2KJ27gMZNmcUo0AACMEQh4hC34XwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51839AA30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(41627, shape=(), dtype=int64) which is a tf.Tensor(7, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA60lEQVR4nGNgGJZA9MCEBAYGBgYOj6+v1dDkYq79/fvXWVTa99DfXxUiKFKmNV9+rv7799/9x3///v2ug6qv9e/fQ2Kznr94/vzz37/1qHI6l/6+ioUwz/69zoMqWfb7sA0DAwMDg0TL71f1rChyTr/+hDAwMDAwsNf8/dOL5tJ5f59DGBP+/i1B96LQXEcGBgYGhiWP/k5UQJeEgtKf/yaz4ZDT/f/vrRIOOY59f/9OwCHHkPH370JhHHLxP/6eFsEhl/v67wlxXBYe+ftVDJeFG//+Csclx/D570Sccgyfn8pjiDHhVk4AAAAXBltWVKqh4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51839A3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAaklEQVR4nJ2SQQpDQQhDX0rvf+XXjX/XpFDBQRBjEicI5HmuiETC9/Cyhfga7TEm1MmAjdEPOrM/dgr+KwXbzs1H7DqFcRUZxge7t3RcIfDucxU2A/VMWIS6FBnfBLNgTdd5xR1OQdGn5QckVDf3U94CmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD518620910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFElEQVR4nGNgGPZAbaAdMAqGBwAAWgIAJ6Pl6gIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51867CCD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(9095, shape=(), dtype=int64) which is a tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/ElEQVR4nGNgoD9gROao/rvLkG1yZtFnbJJHlacxxKgwPNv14Na+V+jGfPsHB28tGBgYWJDkwjimqf3+J/R+IwMDw7NTaJJ6DN+Dv/5DEkCWNGZg+MyADJgQTH4tPL6S/PfPGlUESaczA8MHnDqv/Pu3uEgUh+TNf//+/bvoilVO6Mm/f//+/fs1gQ2LpNW/f0+b/Vd++9fMiSmp9P5fGgMDg9mPf7FYtC5ayczAwMDQ++8KFklmdgYGBo7w7/++ossUMTEwMDAwqyz/9+/fOZggLD7v3X3JcEdF05CBgeF2wXY0ncGwuPy73xLTRr3KM//+/fs30RpTiq4AAKvuZr8uC4vfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5186A4640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAKUlEQVR4nGNgIAn8J005NU0mpOA/BmPwAmKdSE2vUBzAAwiwO43eDgYAas8K9g9Qs64AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD518620760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAGElEQVR4nGNgGAWjYLCDMwPtAAYGBgYGAG30AM2KbzJcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD518620760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape (28, 28, 1)\n",
      "image dtype <dtype: 'uint8'>\n",
      "label shape ()\n",
      "label dtype <dtype: 'int64'>\n",
      "index shape ()\n",
      "index dtype <dtype: 'int64'>\n",
      "vals shape (784, 1)\n",
      "vals dtype <dtype: 'float32'>\n",
      "rows shape (784,)\n",
      "rows dtype <dtype: 'float32'>\n",
      "cols shape (784,)\n",
      "cols dtype <dtype: 'float32'>\n",
      "mask shape (784,)\n",
      "mask dtype <dtype: 'bool'>\n",
      "index tf.Tensor(2500, shape=(), dtype=int64) which is a tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArUlEQVR4nGNgoCJY9xq3nMLfk7glZ/y2xCln+H0vbo37//HhlEv6U8CIS47v0XNunBpn/TPBKafxcTU7LjmOuz+UcGps+NeIU47pGB4bm/79+3Rk3jxpbHLMd39mJi199/NXABbJrH9TGBgYGMT+nMIiOeufMwMDg+zOXzaYctz3fzEwcER8/OiNRaPt/18a9Q/+bZfD5h62t/++/7vnzYzdJ7l/jmWwYZcaEgAAg947sFGIJfcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5184C7D60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nFWSUY7FIAwDx+jd/8qzHw6hW0EFIRg7DugMBAVmarqOkQhA7JYGwoQnkzk+QJRkUrKw4bzN4kIAL/qdDfeeFHaeJKTUAbsrAXxk559zdTV0sxs7LEtZrYEQzqMIRjOEFhvfY7c6hIc0dCctdHxgXmH3gu/o/+f1SUVcA2ULlw+9JZVPPda+ewd+l8XHyiV+wvbHGjaFzjGkiVlNErGwI5bY9Rgfp01ss1R1ymA74dXo6+oeZZvo6Rqzg7oNlFlVSm4xa3KrRk5lWfObGKbL/gCM7635YsdRmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5184C7430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkklEQVR4nGNgoCJYh0dO4e9J3JIzfluSaeU/PpxSSX8YGHFKPmLgxmOqCW6pj6txSnHc/aGEU7IBj3VMx/7hsREfuMuQybD0HcOvACxyWf+mMDAwMIj9OQUTYUFIGjA5MzAwMLxiscGi8z4DAwMDw8ePWK38/0ujnoFhuxw2OTaGf9//3WNgxuXcYxlsuKSGAAAAahIi5LTziRsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5184C75B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(216, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABDUlEQVR4nNXQvyuEARzH8c8VT0TKogxK3XnKgNSZnViu3GAyGE12hmfyoyzuOVwZpfwLFpNsZDJYbpTuuTsRJXVc728Md3HPwzMYfZZvfV99v/X9Sv84vdv7Znac7Poh3RvFugHATV/Eps8Aq5ZKmz6Mhi1zDwSFlCS3wl5oZaEGge9KUvaKIN2OIwAZSZL7AMuSpI6mTa5JtcNLSZry+lXubB8sAvOSpLlnuB1XBPOOJHkvcDcWvr0OOWnFe3wDcuEznHOznYmqmX2YlZORB6RoxXhfj5hmn77w4rubaNX08NDSEfmeRGPxJDrZzCk0Vn8nzbzCdYwN+sBCDGaBgxjTFjAQh26FXScO/5hPQmuLy6RZ4+4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD520597550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0ElEQVR4nG2TQW7EMAwDZ4T8/8vsgXK2rVcJkMCiZJKWDRjASHh/JGbQIOwSQoKBYOznV7G7ngfaDZPWhkAMPO2fQkyBG1M+CkBEBNt/sG1KJkDCoseAHB0tJCTK8JZUB5GjxGMCrxlg+jZxKn9H2p/wV0KTZFjBX7JOQd5ZzXwcuOP5cL03HaLL6mL7vDIuRuHpad4yXz37/F/n2bzfXJh4nL7CWQ+/7ZhB6OGcWUlqtmfS3q47gZU2PRQ5Q8CObmeoeCwmC7WEPGnWpZi9BT9fD4weVux9ZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51862A0A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGMKAp5Xh79+/C7HIcDJM/M7AwPDnz58/DHzoknsZGP4wMNxgaOxh+KOJKuXwkuHPn6e9DAwMDAzP/qAa2fsCt0tU8bjScNkfLKKMUPoPAwMLhiQTnNWPy1Qehj/YjIXo/LWfkbELj5MYsGmFAOd3uOUYGEwYCvEZSyvg+YeBYQqKCFKYWTL8YZDCqfUZQx8blVwBAPehJrgD0cmhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51862ACD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(190, shape=(), dtype=int64) which is a tf.Tensor(3, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABE0lEQVR4nGNgoD9gROHZhjMwMLxfwfD1AabK+X///Pnz5++fP5f4GBgYGBiYkCW/vf7N8LBp8v63Wq3YbEm+u4iBgSHmzzasbhDlY2BguP5nMqaxDAwMrz8xSE6R+/4Sh/tVTv75GoRdiilm89+vNVilHByW//lzA4u+gOLLl//++fOnUghTTujm379////96owkxgJjvFumyeDO9589+fQn7K4RVFv46c9+LMZCQeqfP5dxSrJ0/XkqDeMwMTAwhE+RQ8jqMr5/iqx6+5+7i40YGBgYGPhD+/786YFLMDIwMKR08TH8+LrhGQODow3Djbp1qBbJxJ79A43nQ/ZI4tBkwi0vpK3DwPBu5a0/ON1KHwAAVS5mKKwV29UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD52006EEB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAz0lEQVR4nGWTW47FIAxDj9Hd/5bPfOQB6lRUSgiJXZsGY5B5YibJxAEjASsDOJXE2lPDOwgUdLcmErmrXrWLp9pCD/OB5CwpL1Evnv+B+8wviOmGvG1wNGaRqj6YwUBrtNpUefWpIEicVk9IK5L0+RfX0qKY+ip3PsrlzU4khJgZuLQ8NlIzznwOkJNWP5TRuXyMkEuwR44Kjk/Uev0Mzw0qVLNaly2uQ+P5x6lVYm/CV3Fim3GLcyFdo3h7ypCsis3aJQIvJP7q1PNLuO7yB2G0v8G0qcGSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51839AA30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2ElEQVR4nL2SMUtCYRSGn8pNELpOkdDSZlO7k38ggjabzF/QGrhLOOkPuAl5+xWGTq1eIRICHRxcapCLy/tBw1Xw5ndW3+k733Pecw6cA4fXUSaqjKDzG5HMPKkOSU6M0zC3y7ona+ZhUF4WvV3qvADIGKIAfFoQzjqsLHb5oeTWj45ruOTJ8PUlPL6bR2KQf5Yvh8NR9VeMIpBeC0bDU0I0CAwKDYhNmGtpcW7SPem7d52+7triOcMetmtovovJf2fpHoEQGu7+b84kfxGUr+DnbWpv6zD6A4ZQQw0lqCs6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD52006E6D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(5082, shape=(), dtype=int64) which is a tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nGNgGEqAEV3ATe7MBawqWcTLD//88wirHLP7gz9///z52mvBiiEn3//nz5+/f/4sv/dnEqoMq+jB998/XgiZcUODUW3t73gUXev+/NlswsDA0N/PwMAQ+ee1BkKO+/ifbxl8DAwMDCaLGRgYxE//cUJINv65qARl8jEwMDB4/TnMB5f8/zcGxQU8f//qwzl//+igenjHn30MDAxMUK42imSMCcMyJJ1n2aBMLuGAOZ9+PSthYoCF7dx4hrVbbzEwGFzIU1XiZ3i7uATJGI5Zf6Dg759bS2JVoMLQWGGzdpIIY2C4v/nw+R+fGYYhAAD6eFxqXFYwpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD52006EEB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqklEQVR4nH2SWQ7EMAhDTTX3v/KbD5aYTjRR1bI4YExDEpJCeZCCEDHu+5wQgDSPpz+EJBQnHl3T8WQNL0wl2pDwno3Aefywy1vOvi6u2Js19v03Odtkx7gCiycHXqbp15QbQwZmktK2pM1QEDeytFAcHbyfD7V7bynqPJPJdyw9jpPdjK0T39DmNyRYw/eyZmetdHpPc45IPucHnopc2toKNA0Mz5rdVwpflPC3Z4vBJ3wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD52006EEB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAf0lEQVR4nGNgGK6gnOHnH0JqWLEL/2X4w3APU/ggjMHIsBZD8g+DCRIbKzBZzMDAII4uq4TM8UKW/P83Bt1dDAwMDCwMDAwMDP+YUeVYdqNww7E7AQLOIrHnMDA8Q5b7A3FBBsMyLBpn/WH48wfuPxVMBR8/MlxoZhDFZ/nQBQCS/h7zyKnWKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD52006E640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index tf.Tensor(6152, shape=(), dtype=int64) which is a tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzUlEQVR4nGNgGI6AUbH05Mf1u7x5sUlW/vty/sKNf/9uKWLKpfwql2BgYDMp/TcfQ876dTTU9P4/2mhyHHfnwphWX8yhLCYoba64H8pSk3l3CU2SgcEDQiUeY3j4A81YgX9vU4S5uNQnfntXmY/uHubJ/969uXfv379/80IxfcKSferfv38H777kwZSDArN3tTjlGPb/k8ct+W8FC065+v8WyFwmFEn1R1dxapT7UojCR9HpxXUXt6QNTjMZGJy+XePDqVP7QfknPHrRAAA0hELTKxpWgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51862A940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAaUlEQVR4nKWSQQ4AIQgD+/9Pzx7MmhVq3UQOalKmgigkSWO1QdMRzNMUWQDr0nykYQX6eIZ6CNqvwNAkz3Qbh2q39R5bCLaU3VBY1LVuLHpW5q7GsafjUBpeeqPz5dmA5dtWWWhk8KZLD6nVTLStjJOpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51862A400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAE0lEQVR4nGNgGAWjYMgDP3pYAgAh8gBP3hlmugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD51862A8B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataset_summary(d):\n",
    "    for el in d.take(1):\n",
    "        for name, v in el.items():\n",
    "            print(name, \"shape\", v.shape)\n",
    "            print(name, \"dtype\", v.dtype)\n",
    "\n",
    "    for el in d.take(5):\n",
    "        print(\"index\", el[\"index\"], \"which is a\", el[\"label\"])\n",
    "        display_uint8_image(el[\"image\"])\n",
    "        display_mask(el[\"mask\"])\n",
    "        display_uint8_image(tf.reshape(el[\"image\"], [28, 28]) * tf.cast(mask_to_image_mask(el[\"mask\"]), tf.uint8))\n",
    "\n",
    "dataset_summary(dataset_train)\n",
    "dataset_summary(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f_masks = np.array([29, 31, 754, 752])\n",
    "# print(f_masks)\n",
    "# f_masks = idxs_to_multihot(f_masks)\n",
    "# print(f_masks)\n",
    "# display_mask(f_masks)\n",
    "\n",
    "# t_masks = np.indices([784])\n",
    "# print(t_masks)\n",
    "# t_masks = idxs_to_multihot(t_masks)\n",
    "# print(t_masks)\n",
    "# display_mask(t_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNNZdCnizvLU",
    "outputId": "8472bdb9-66be-4f7a-8b8a-66ac282bb160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784, 3) (10000, 2) (10000,)\n",
      "tf.Tensor(\n",
      "[[ 11.   1.   0.]\n",
      " [ -5. -19.   0.]\n",
      " [ -8. -17.   0.]\n",
      " ...\n",
      " [  1. -26.   0.]\n",
      " [  2. -25.   0.]\n",
      " [  5. -14.   0.]], shape=(784, 3), dtype=float32)\n",
      "tf.Tensor([10 26], shape=(2,), dtype=int64)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_dataset[\"tuples\"].shape, test_dataset[\"offsets\"].shape, test_dataset[\"target_pxs\"].shape)\n",
    "print(test_dataset[\"tuples\"][0])\n",
    "print(test_dataset[\"offsets\"][0])\n",
    "print(test_dataset[\"target_pxs\"][0])\n",
    "\n",
    "# actual training dataset is 784 x,y,v tuples, but with the spatial location changed so the the pixel to predict is in the center\n",
    "# and the pixel value to predict removed (via masking)\n",
    "\n",
    "# then, can mask more and more stuff and compare performance\n",
    "\n",
    "# we can also use the model in an \"iterative\" mode to generate images, in an arbitrary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W3nKMxnPe6cc"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5. 3.]\n",
      " [1. 4.]\n",
      " [1. 6.]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.11920292 0.         0.880797  ]\n",
      " [0.18242551 0.         0.81757444]\n",
      " [0.5        0.         0.5       ]], shape=(3, 3), dtype=float32)\n",
      "(1, 3, 2)\n",
      "(3, 3, 1)\n",
      "(3, 3, 2)\n",
      "tf.Tensor(\n",
      "[[[0.5960146  0.35760877]\n",
      "  [0.         0.        ]\n",
      "  [0.880797   5.2847824 ]]\n",
      "\n",
      " [[0.91212755 0.54727656]\n",
      "  [0.         0.        ]\n",
      "  [0.81757444 4.9054465 ]]\n",
      "\n",
      " [[2.5        1.5       ]\n",
      "  [0.         0.        ]\n",
      "  [0.5        3.        ]]], shape=(3, 3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.5960146  0.35760877]\n",
      "  [0.         0.        ]\n",
      "  [0.880797   5.2847824 ]]\n",
      "\n",
      " [[0.91212755 0.54727656]\n",
      "  [0.         0.        ]\n",
      "  [0.81757444 4.9054465 ]]\n",
      "\n",
      " [[2.5        1.5       ]\n",
      "  [0.         0.        ]\n",
      "  [0.5        3.        ]]], shape=(3, 3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.4768116 5.642391 ]\n",
      " [1.729702  5.452723 ]\n",
      " [3.        4.5      ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# attention manual calculation example\n",
    "\n",
    "m = tf.constant([[5, 3], [1, 2]], dtype=tf.float32)\n",
    "val = tf.constant([[5, 3], [1, 4], [1, 6]], dtype=tf.float32)\n",
    "print(val)\n",
    "mask = tf.constant([1, 0, 1], dtype=tf.float32)\n",
    "wei = tf.constant([[1, 2, 3], [0.5, 2, 2], [1, 3, 1]], dtype=tf.float32)\n",
    "wei = keras.layers.Softmax()(wei, mask)\n",
    "print(wei)\n",
    "val = tf.expand_dims(val, -3)\n",
    "print(val.shape)\n",
    "wei = tf.expand_dims(wei, -1)\n",
    "print(wei.shape)\n",
    "x = val * wei\n",
    "print(x.shape)\n",
    "print(x)\n",
    "mask = tf.expand_dims(mask, -1)\n",
    "# x = x * mask\n",
    "print(x)\n",
    "x = tf.reduce_sum(x, axis=-2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Maths\n",
    "\n",
    "Dimensions $N$, $D$, $E$ and $B$.\n",
    "\n",
    "- $N = 784$ is the number of inputs.\n",
    "- $D$ is the width of the _key_ $K$ and _query_ $Q$ vectors.\n",
    "- $E$ is the width of the _value_ vectors $V$.\n",
    "- There is also a (or multiple) batch dimension(s) $B$.\n",
    "\n",
    "$K$ is $B \\times N \\times D$ dimensional.\n",
    "$Q$ is $B \\times N \\times D$ dimensional.\n",
    "$V$ is $B \\times N \\times E$ dimensional.\n",
    "Because it is self-attention, $K$ and $Q$ have the same length $N$, and the attention matrix is square.\n",
    "The attention matrix is $A = Q \\cdot K^T$, and is $B \\times N \\times N$ dimensional. Formally:\n",
    "$$\n",
    "A_{b,i,j} = \\sum_d Q_{b,i,d} K_{b,j,d}\n",
    "$$\n",
    "\n",
    "We do softmax normalization along the columns $j$ of the attention matrix (such that each _row_ $i$ sums to 1). The result is the attention weights. Formally:\n",
    "$$\n",
    "\\bar{A}_{b,i,j} = \\frac{e^{A_{b,i,j}}}{\\sum_{j'} e^{A_{b,i,j'}}}\n",
    "$$\n",
    "\n",
    "The output $O$ of the attention layer is $B \\times N \\times E$ dimensional. It is obtained by the attention weights multiplied by the value vectors $V$. $A$ is $B \\times N \\times N$ dimensional and $V$ is $B \\times N \\times E$ dimensional.\n",
    "$$\n",
    "    O_{b,i,e} = \\sum_j A_{b,i,j} V_{b,j,e}\n",
    "$$\n",
    "\n",
    "Often the dimensions $E = D$ because this allows multiple attention layers in sequence, but this need not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5noipvB9oe8v"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def multi_head_attention(n_heads, n_kq_dim, n_val_dim):\n",
    "    \n",
    "    k_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    q_dense = layers.Dense(n_kq_dim, activation='linear')\n",
    "    \n",
    "    \n",
    "    \n",
    "    softmax = layers.Softmax(axis=-1)\n",
    "    \n",
    "    val_dense = layers.Dense(n_val_dim, activation='relu')\n",
    "    \n",
    "    def call(inputs, mask):\n",
    "        \n",
    "        k = k_dense(inputs)\n",
    "        q = q_dense(inputs)\n",
    "        \n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        weights = softmax(scores, mask)\n",
    "        \n",
    "        vals = val_dense(inputs)\n",
    "        \n",
    "        vals = tf.expand_dims(-1)\n",
    "        weights = tf.expand_dims(-2)\n",
    "        \n",
    "        outputs = tf.reduce_sum(vals * weights)\n",
    "        \n",
    "        \n",
    "        vals *= mask\n",
    "        \n",
    "\n",
    "def transformer_block(n_embed_dim, n_heads, n_dense_dim, dropout_rate):\n",
    "    attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_embed_dim)\n",
    "    dense_net_1 = layers.Dense(n_dense_dim, activation='relu')\n",
    "    dense_net_2 = layers.Dense(n_embed_dim)\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = layers.Dropout(dropout_rate)\n",
    "    dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, masks, include_residual):\n",
    "        mask = tf.logical_and(masks[:, :, None], masks[:, None, :])\n",
    "        attn_output = attn(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = dropout1(attn_output)\n",
    "        if include_residual:\n",
    "            attn_output = inputs + attn_output\n",
    "        # mask outputs. important! without, model learns magic powers (can detect and use verrrrrrry small numbers which are not literally 0)\n",
    "        attn_output = attn_output * tf.expand_dims(tf.cast(masks, tf.float32), -1)\n",
    "        attn_output = layernorm1(attn_output)\n",
    "        dense_output = dense_net_1(attn_output)\n",
    "        dense_output = dense_net_2(dense_output)\n",
    "        dense_output = dropout2(dense_output)\n",
    "        return layernorm2(attn_output + dense_output)\n",
    "    \n",
    "    return call\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-Xi5wBCwEVHp"
   },
   "outputs": [],
   "source": [
    "def model(batch_size):\n",
    "\n",
    "    # no batch size to start makes it simpler\n",
    "    n_embd = 20\n",
    "\n",
    "    vals = keras.Input(shape=[784], name='vals', batch_size=batch_size)\n",
    "    rows = keras.Input(shape=[784], name='rows', batch_size=batch_size)\n",
    "    cols = keras.Input(shape=[784], name='cols', batch_size=batch_size)\n",
    "    mask = keras.Input(shape=[784], name='mask', batch_size=batch_size, dtype=tf.bool)\n",
    "    \n",
    "    rows_pos_enc = positional_encoding(rows, n_embd//2)\n",
    "    cols_pos_enc = positional_encoding(cols, n_embd//2)\n",
    "    \n",
    "    pos_enc = tf.concat([rows_pos_enc, cols_pos_enc], axis=-2)\n",
    "    \n",
    "    m = vals\n",
    "    \n",
    "    # produce images of the attention/relevance/contribution for each output.\n",
    "\n",
    "    # make it smaller\n",
    "    # - less heads\n",
    "    # - less dense layers\n",
    "    # - smaller layer sizes'\n",
    "    \n",
    "    # look at standard transformer structure again.\n",
    "    # what is the expected training time?\n",
    "    \n",
    "    # simple setup -> build up.\n",
    "    \n",
    "    # literature / other task at the same time\n",
    "    # have enough to get help from supervisors in discussion\n",
    "    # start writing\n",
    "    \n",
    "    # make n_embd-dimensional input embeddings per pixel from [x, y, v]\n",
    "    # embedding\n",
    "    m = layers.Dense(n_embd, activation='relu')(m)\n",
    "    \n",
    "    m = m + pos_enc\n",
    "    \n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=12, n_dense_dim=200, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    m = transformer_block(n_embed_dim=n_embd, n_heads=12, n_dense_dim=200, dropout_rate=0.1)(m, masks=mask, include_residual=True)\n",
    "    \n",
    "    m = layers.Reshape((28, 28, n_embd))(m)\n",
    "    m = layers.Dense(1, activation=None)(m)\n",
    "\n",
    "    model = keras.Model(inputs=[vals, rows, cols, mask], outputs=[m])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOqsXnxifpG",
    "outputId": "e1fee0a6-197b-4ca4-92a0-1d23c1906133"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 16 and 25088 for '{{node tf.__operators__.add/AddV2}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [16,20], [25088,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mr_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__r%s__\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1398\u001b[0m       \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmaybe_promote_tensors\u001b[0;34m(force_same_dtype, *tensors)\u001b[0m\n\u001b[1;32m   1334\u001b[0m       promoted_tensors.append(\n\u001b[0;32m-> 1335\u001b[0;31m           ops.convert_to_tensor(tensor, dtype, name=\"x\"))\n\u001b[0m\u001b[1;32m   1336\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpromoted_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    272\u001b[0m                         allow_broadcast=True)\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;34m'Cannot convert a symbolic Keras input/output to a numpy array. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_912551/2310362533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlr_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lr_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtxformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtxformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MeanAbsoluteError'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_912551/1861768790.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_embed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dense_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_residual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(op, args, kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdispatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GLOBAL_DISPATCHERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         for x in nest.flatten([args, kwargs])):\n\u001b[0;32m-> 1495\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mTFOpLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    997\u001b[0m                                                 input_list)\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    905\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_call_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;31m# Decorate the function to produce this layer's call method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_call_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_call_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m       \u001b[0;31m# multiple ops w/ the same name when the layer is reused)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreated_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    462\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \"AddV2\", x=x, y=y, name=name)\n\u001b[1;32m    466\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    746\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    749\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         compute_device)\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3559\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3561\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3562\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2039\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   2043\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conditional-mnist/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1881\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 16 and 25088 for '{{node tf.__operators__.add/AddV2}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [16,20], [25088,10]."
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "txformer = model(batch_size)\n",
    "txformer.compile(optimizer=optimizer, loss='mse', metrics=['MeanAbsoluteError', lr_metric])\n",
    "\n",
    "def fit_one_epoch(dataset):\n",
    "    txformer.fit(x=[dataset[\"tuples\"], dataset[\"masks\"]], y=dataset[\"images\"], epochs=1, batch_size=batch_size, callbacks=[WandbCallback()])\n",
    "\n",
    "load_saved_model = False\n",
    "if load_saved_model:\n",
    "    txformer.load_weights(f\"./models/{model_name}\")\n",
    "\n",
    "txformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "fzuSaIstGU0A",
    "outputId": "765dc0e1-e241-4363-8f90-c06fe21ea4e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# display:\n",
    "# - before mask\n",
    "# - mask\n",
    "# - after mask\n",
    "# - prediction\n",
    "def gen_image(dataset, idx):\n",
    "    img = np.zeros([28, 28])\n",
    "    mask = dataset[\"masks\"][idx:idx+1] # slice to keep batch dim\n",
    "    erow, ecol = dataset[\"offsets\"][idx]\n",
    "\n",
    "    image_mask = np.array(mask_to_image_mask(mask[np.newaxis, :]), np.uint8)[0]\n",
    "    masked_image = np.copy(dataset[\"images\"][idx])\n",
    "    \n",
    "    print(\"MNIST idx\", idx, \"which is a\", dataset['digits'][idx])\n",
    "    \n",
    "    display_uint8_image(masked_image) # before mask\n",
    "    display_uint8_image(image_mask * 255) # mask\n",
    "    masked_image = masked_image * image_mask\n",
    "    display_uint8_image(masked_image) # after mask\n",
    "    \n",
    "    img_tups = dataset[\"tuples\"][idx:idx+1] # slice to keep batch dim\n",
    "    img = txformer([img_tups, mask])\n",
    "    img = tf.reshape(img, [28, 28])\n",
    "    display_float32_image(img)\n",
    "\n",
    "def image_performance_test():\n",
    "    gen_image(test_dataset, 0)\n",
    "    gen_image(test_dataset, 1)\n",
    "    gen_image(test_dataset, 2)\n",
    "    \n",
    "    # full masks dataset as sanity test\n",
    "    gen_image(full_masks_dataset, 0)\n",
    "    gen_image(full_masks_dataset, 1)\n",
    "    gen_image(full_masks_dataset, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Qc-55LXO8Dtl",
    "outputId": "47b797e1-67ca-440b-c252-7e961a14c6ee"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6NnXshwaCj"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSfq8VcPOEW"
   },
   "outputs": [],
   "source": [
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2oRg5MMrmK"
   },
   "outputs": [],
   "source": [
    "image_performance_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNhU_P0QPWPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_epoch(train_dataset)\n",
    "fit_one_epoch(train_dataset)\n",
    "fit_one_epoch(train_dataset)\n",
    "fit_one_epoch(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    gen_image(test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not load_saved_model:\n",
    "    txformer.save_weights(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST conditional prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
